{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbcIPWYG4Qj6"
   },
   "source": [
    "# RNN기반 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1694,
     "status": "ok",
     "timestamp": 1750140631111,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "6U_oVjNc4c3I",
    "outputId": "339f9151-f4ae-4015-8be5-784639f4fc96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comp.graphics', 'rec.sport.baseball', 'sci.space']\n",
      "From: kjenks@gothamcity.jsc.nasa.gov\n",
      "Subject: Life on Mars???\n",
      "Organization: NASA/JSC/GM2, Space Shuttle Program Office \n",
      "X-Newsreader: TIN [version 1.1 PL8]\n",
      "Lines: 12\n",
      "\n",
      "I know it's only wishful thinking, with our current President,\n",
      "but this is from last fall:\n",
      "\n",
      "     \"Is there life on Mars?  Maybe not now.  But there will be.\"\n",
      "        -- Daniel S. Goldin, NASA Administrator, 24 August 1992\n",
      "\n",
      "-- Ken Jenks, NASA/JSC/GM2, Space Shuttle Program Office\n",
      "      kjenks@gothamcity.jsc.nasa.gov  (713) 483-4368\n",
      "\n",
      "     \"The man who makes no mistakes does not usually make\n",
      "      anything.\"\n",
      "        -- Edward John Phelps, American Diplomat/Lawyer (1825-1895)\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로딩\n",
    "from sklearn.datasets import fetch_20newsgroups     # 20 Newsgroups 텍스트 분류 데이터셋 로드\n",
    "\n",
    "categories = ['comp.graphics', 'sci.space', 'rec.sport.baseball']  # 사용할 뉴스그룹 카테고리 선택\n",
    "newsgroups = fetch_20newsgroups(                                  # 선택한 카테고리만 데이터 로드\n",
    "    subset='all',\n",
    "    categories=categories\n",
    ")\n",
    "\n",
    "X = newsgroups.data                           # 뉴스 문서 텍스트 리스트\n",
    "y = newsgroups.target                         # 각 문서의 클래스 인덱스 라벨\n",
    "\n",
    "print(newsgroups.target_names)                # 클래스 인덱스 → 실제 뉴스그룹 이름\n",
    "print(X[0])                                   # 첫 번째 뉴스 문서 원문 출력\n",
    "print(y[0])                                   # 첫 번째 문서의 클래스 인덱스 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6431,
     "status": "ok",
     "timestamp": 1750140647214,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "yXBtkCIp4H51",
    "outputId": "e593d05d-06d7-4379-efa1-6ddb83765ef1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2954, 200)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer        # 텍스트를 정수 시퀀스로 변환\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences # 시퀀스 길이 맞추기용 패딩 함수\n",
    "\n",
    "vocab_size = 10000                                               # 사용할 최대 단어 사전 크기\n",
    "max_len = 200                                                    # 모든 문장의 최대 길이\n",
    "\n",
    "tokenizer = Tokenizer(                                          # Tokenizer 객체 생성\n",
    "    num_words=vocab_size,                                       # 상위 vocab_size 단어만 사용\n",
    "    oov_token='<OOV>'                                           # 미등록 단어 토큰 설정\n",
    ")\n",
    "tokenizer.fit_on_texts(X)                                       # 전체 텍스트 기준으로 단어 사전 생성\n",
    "X_encoded = tokenizer.texts_to_sequences(X)                     # 문장을 정수 인덱스 시퀀스로 변환\n",
    "X_padded = pad_sequences(                                       # 모든 시퀀스를 동일한 길이로 패딩\n",
    "    X_encoded,\n",
    "    maxlen=max_len\n",
    ")\n",
    "\n",
    "print(X_padded.shape)                                           # (문서 수, max_len) 형태 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0suBZZ3V6ZuL"
   },
   "outputs": [],
   "source": [
    "# 데이터 분리 / 텐서 변환\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split     # 데이터 분할 함수\n",
    "from torch.utils.data import TensorDataset, DataLoader   # PyTorch Dataset / DataLoader\n",
    "\n",
    "# train / test 분리 (80% / 20%)\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(\n",
    "        torch.tensor(X_padded, dtype=torch.long),        # 패딩된 입력 데이터를 Long 텐서로 변환\n",
    "        torch.tensor(y, dtype=torch.long),               # 라벨 데이터를 텐서로 변환\n",
    "        test_size=0.2,                                   # 테스트 데이터 비율\n",
    "        random_state=42                                  # 재현성 확보\n",
    "    )\n",
    "\n",
    "# train / validation 분리 (train의 20%를 validation으로 사용)\n",
    "X_train, X_val, y_train, y_val = \\\n",
    "    train_test_split(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        test_size=0.2,                                   # 검증 데이터 비율\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "# Dataset 생성 (입력, 라벨 묶기)\n",
    "train_dataset = TensorDataset(X_train, y_train)          # 학습 데이터셋\n",
    "val_dataset = TensorDataset(X_val, y_val)                # 검증 데이터셋\n",
    "test_dataset = TensorDataset(X_test, y_test)             # 테스트 데이터셋\n",
    "\n",
    "# DataLoader 설정\n",
    "batch_size = 64                                          # 배치 크기\n",
    "train_loader = DataLoader(                               # 학습용 DataLoader\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True                                         # 학습 데이터는 셔플\n",
    ")\n",
    "val_loader = DataLoader(                                 # 검증용 DataLoader\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "test_loader = DataLoader(                                # 테스트용 DataLoader\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "y7BjT5FY8Tnt"
   },
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "import torch.nn as nn                              # PyTorch 신경망 모듈\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super().__init__()                         # nn.Module 초기화\n",
    "        # embedding → lstm → dense 구조\n",
    "        self.embedding = nn.Embedding(             # 단어 인덱스를 임베딩 벡터로 변환\n",
    "            vocab_size,                            # 단어 사전 크기\n",
    "            embedding_dim,                         # 임베딩 벡터 차원\n",
    "            padding_idx=0                          # PAD 토큰은 학습에 영향 없음\n",
    "        )\n",
    "        self.lstm = nn.LSTM(                       # 문맥 정보를 학습하는 LSTM\n",
    "            embedding_dim,                         # 입력 차원 (임베딩 크기)\n",
    "            hidden_size,                           # 은닉 상태 차원\n",
    "            batch_first=True                       # (batch, seq, feature) 형태 사용\n",
    "        )\n",
    "        self.fc = nn.Linear(                       # 최종 분류용 선형 레이어\n",
    "            hidden_size,                           # LSTM 은닉 상태 차원\n",
    "            3                                      # 클래스 개수 (3개 뉴스그룹)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)                      # (batch, seq) → (batch, seq, embed)\n",
    "        _, (h, c) = self.lstm(x)                   # LSTM 통과 (h: 마지막 은닉 상태)\n",
    "        out = self.fc(h[-1])                       # 마지막 타임스텝 은닉 상태로 분류\n",
    "        return out                                 # (batch, 3) 로짓 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11441,
     "status": "ok",
     "timestamp": 1750139218556,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "-vO1L30O8Vf5",
    "outputId": "3485c91a-9816-4db2-9410-6c819ee4562a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Train Loss 1.0504, Train Acc 0.4646, Val Loss 0.9947, Val Acc 0.4757, \n",
      "Epoch 2/50: Train Loss 0.8857, Train Acc 0.6037, Val Loss 0.8581, Val Acc 0.6110, \n",
      "Epoch 3/50: Train Loss 0.6921, Train Acc 0.7180, Val Loss 0.7799, Val Acc 0.6765, \n",
      "Epoch 4/50: Train Loss 0.4979, Train Acc 0.8032, Val Loss 0.6915, Val Acc 0.7104, \n",
      "Epoch 5/50: Train Loss 0.3602, Train Acc 0.8757, Val Loss 0.6784, Val Acc 0.7378, \n",
      "Epoch 6/50: Train Loss 0.2770, Train Acc 0.9063, Val Loss 0.6655, Val Acc 0.7674, \n",
      "Epoch 7/50: Train Loss 0.1838, Train Acc 0.9397, Val Loss 0.6729, Val Acc 0.7674, \n",
      "Epoch 8/50: Train Loss 0.1643, Train Acc 0.9519, Val Loss 0.7093, Val Acc 0.8034, \n",
      "Epoch 9/50: Train Loss 0.0866, Train Acc 0.9762, Val Loss 0.6380, Val Acc 0.8161, \n",
      "Epoch 10/50: Train Loss 0.0486, Train Acc 0.9873, Val Loss 0.7134, Val Acc 0.8266, \n",
      "Epoch 11/50: Train Loss 0.0338, Train Acc 0.9926, Val Loss 0.6709, Val Acc 0.7992, \n",
      "Epoch 12/50: Train Loss 0.0905, Train Acc 0.9751, Val Loss 0.6259, Val Acc 0.7907, \n",
      "Epoch 13/50: Train Loss 0.0565, Train Acc 0.9873, Val Loss 0.7357, Val Acc 0.7949, \n",
      "Epoch 14/50: Train Loss 0.0320, Train Acc 0.9937, Val Loss 0.7030, Val Acc 0.8266, \n",
      "Epoch 15/50: Train Loss 0.0183, Train Acc 0.9952, Val Loss 0.6908, Val Acc 0.8266, \n",
      "Epoch 16/50: Train Loss 0.0134, Train Acc 0.9979, Val Loss 0.7022, Val Acc 0.8351, \n",
      "Epoch 17/50: Train Loss 0.0095, Train Acc 0.9995, Val Loss 0.7893, Val Acc 0.8330, \n",
      "Epoch 18/50: Train Loss 0.0221, Train Acc 0.9952, Val Loss 0.8035, Val Acc 0.7992, \n",
      "Epoch 19/50: Train Loss 0.0342, Train Acc 0.9894, Val Loss 0.7580, Val Acc 0.7992, \n",
      "Epoch 20/50: Train Loss 0.0448, Train Acc 0.9868, Val Loss 0.7845, Val Acc 0.8076, \n",
      "Epoch 21/50: Train Loss 0.0123, Train Acc 0.9974, Val Loss 0.8394, Val Acc 0.8224, \n",
      "Epoch 22/50: Train Loss 0.0063, Train Acc 0.9995, Val Loss 0.8819, Val Acc 0.8245, \n",
      "Epoch 23/50: Train Loss 0.0038, Train Acc 0.9989, Val Loss 0.8729, Val Acc 0.8309, \n",
      "Epoch 24/50: Train Loss 0.0034, Train Acc 0.9989, Val Loss 0.9026, Val Acc 0.8393, \n",
      "Epoch 25/50: Train Loss 0.0026, Train Acc 0.9995, Val Loss 0.9357, Val Acc 0.8414, \n",
      "Epoch 26/50: Train Loss 0.0023, Train Acc 0.9995, Val Loss 0.8760, Val Acc 0.8393, \n",
      "Epoch 27/50: Train Loss 0.0018, Train Acc 0.9995, Val Loss 0.9197, Val Acc 0.8436, \n",
      "Epoch 28/50: Train Loss 0.0020, Train Acc 0.9995, Val Loss 0.9389, Val Acc 0.8414, \n",
      "Epoch 29/50: Train Loss 0.0016, Train Acc 0.9989, Val Loss 0.8662, Val Acc 0.8436, \n",
      "Epoch 30/50: Train Loss 0.0018, Train Acc 0.9995, Val Loss 0.9757, Val Acc 0.8393, \n",
      "Epoch 31/50: Train Loss 0.0017, Train Acc 0.9989, Val Loss 0.9300, Val Acc 0.8436, \n",
      "Epoch 32/50: Train Loss 0.0014, Train Acc 0.9989, Val Loss 0.9116, Val Acc 0.8436, \n",
      "Epoch 33/50: Train Loss 0.0012, Train Acc 0.9995, Val Loss 0.9085, Val Acc 0.8414, \n",
      "Epoch 34/50: Train Loss 0.0017, Train Acc 0.9995, Val Loss 0.8838, Val Acc 0.8478, \n",
      "Epoch 35/50: Train Loss 0.0016, Train Acc 0.9995, Val Loss 1.0184, Val Acc 0.8351, \n",
      "Epoch 36/50: Train Loss 0.0014, Train Acc 0.9995, Val Loss 0.9487, Val Acc 0.8372, \n",
      "Epoch 37/50: Train Loss 0.0013, Train Acc 0.9989, Val Loss 0.9090, Val Acc 0.8372, \n",
      "Epoch 38/50: Train Loss 0.0011, Train Acc 0.9995, Val Loss 0.9234, Val Acc 0.8436, \n",
      "Epoch 39/50: Train Loss 0.0010, Train Acc 0.9995, Val Loss 1.0361, Val Acc 0.8351, \n",
      "Epoch 40/50: Train Loss 0.0015, Train Acc 0.9995, Val Loss 0.8734, Val Acc 0.8520, \n",
      "Epoch 41/50: Train Loss 0.0013, Train Acc 0.9995, Val Loss 0.9577, Val Acc 0.8436, \n",
      "Epoch 42/50: Train Loss 0.0012, Train Acc 0.9995, Val Loss 1.0522, Val Acc 0.8330, \n",
      "Epoch 43/50: Train Loss 0.0012, Train Acc 0.9995, Val Loss 0.9496, Val Acc 0.8414, \n",
      "Epoch 44/50: Train Loss 0.0009, Train Acc 0.9995, Val Loss 0.9378, Val Acc 0.8457, \n",
      "Epoch 45/50: Train Loss 0.0010, Train Acc 0.9995, Val Loss 0.9941, Val Acc 0.8457, \n",
      "Epoch 46/50: Train Loss 0.0005, Train Acc 1.0000, Val Loss 0.9915, Val Acc 0.8499, \n",
      "Epoch 47/50: Train Loss 0.0289, Train Acc 0.9937, Val Loss 1.1387, Val Acc 0.7104, \n",
      "Epoch 48/50: Train Loss 0.1159, Train Acc 0.9667, Val Loss 0.9400, Val Acc 0.7632, \n",
      "Epoch 49/50: Train Loss 0.0539, Train Acc 0.9847, Val Loss 0.7796, Val Acc 0.8266, \n",
      "Epoch 50/50: Train Loss 0.0192, Train Acc 0.9952, Val Loss 0.8236, Val Acc 0.8245, \n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "import torch.optim as optim                                  # 옵티마이저 모듈\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # cuda 가능하면 GPU 사용\n",
    "embedding_dim = 100                                          # 임베딩 벡터 차원\n",
    "hidden_size = 128                                            # LSTM 은닉 상태 차원\n",
    "\n",
    "model = LSTMClassifier(vocab_size, embedding_dim, hidden_size).to(device)  # 모델 생성 후 GPU 이동\n",
    "criterion = nn.CrossEntropyLoss()                            # 다중 클래스 분류용 손실 함수\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)         # Adam 옵티마이저 설정\n",
    "\n",
    "# 학습 루프 기록용\n",
    "train_losses, train_accs = [], []                            # 학습 손실/정확도 기록\n",
    "val_losses, val_accs = [], []                                # 검증 손실/정확도 기록\n",
    "\n",
    "epochs = 50                                                  # 전체 학습 에폭 수\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # 학습\n",
    "    model.train()                                            # 학습 모드 전환\n",
    "    train_loss, train_correct, train_total = 0, 0, 0          # 에폭 누적 변수 초기화\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # 배치를 GPU로 이동\n",
    "        optimizer.zero_grad()                                # 이전 gradient 초기화\n",
    "        output = model(X_batch)                              # 순전파 (로짓 출력)\n",
    "        loss = criterion(output, y_batch)                    # 손실 계산\n",
    "        loss.backward()                                      # 역전파\n",
    "        optimizer.step()                                     # 파라미터 업데이트\n",
    "\n",
    "        train_loss += loss.detach().cpu().item()             # 배치 손실 누적\n",
    "        pred = output.argmax(dim=1)                          # 가장 큰 로짓 인덱스 = 예측 클래스\n",
    "        train_correct += (pred == y_batch).sum().detach().cpu().item()  # 정답 개수 누적\n",
    "        train_total += len(y_batch)                          # 전체 샘플 수 누적\n",
    "\n",
    "    train_loss /= len(train_loader)                          # 에폭 평균 학습 손실\n",
    "    train_acc = train_correct / train_total                  # 에폭 학습 정확도\n",
    "    train_losses.append(train_loss)                          # 학습 손실 기록\n",
    "    train_accs.append(train_acc)                             # 학습 정확도 기록\n",
    "\n",
    "    # 검증\n",
    "    model.eval()                                             # 평가 모드 전환\n",
    "    val_loss, val_correct, val_total = 0, 0, 0                # 검증 누적 변수 초기화\n",
    "\n",
    "    with torch.no_grad():                                    # gradient 계산 비활성화\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # 배치를 GPU로 이동\n",
    "            output = model(X_batch)                           # 검증 순전파\n",
    "            loss = criterion(output, y_batch)                 # 검증 손실 계산\n",
    "\n",
    "            val_loss += loss.detach().cpu().item()            # 검증 손실 누적\n",
    "            pred = output.argmax(dim=1)                       # 예측 클래스\n",
    "            val_correct += (pred == y_batch).sum().detach().cpu().item()  # 정답 누적\n",
    "            val_total += len(y_batch)                         # 샘플 수 누적\n",
    "\n",
    "        val_loss /= len(val_loader)                           # 에폭 평균 검증 손실\n",
    "        val_acc = val_correct / val_total                     # 에폭 검증 정확도\n",
    "        val_losses.append(val_loss)                           # 검증 손실 기록\n",
    "        val_accs.append(val_acc)                              # 검증 정확도 기록\n",
    "\n",
    "    # 출력 (train_loss, val_loss)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}: '                     # 현재 에폭 정보 출력\n",
    "          f'Train Loss {train_loss:.4f}, '\n",
    "          f'Train Acc {train_acc:.4f}, '\n",
    "          f'Val Loss {val_loss:.4f}, '\n",
    "          f'Val Acc {val_acc:.4f}, ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1750139872430,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "clNCNefO9nBv",
    "outputId": "4505f72f-01c6-4123-81bc-30dc2c86a93c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     comp.graphics       0.81      0.72      0.76       202\n",
      "rec.sport.baseball       0.87      0.86      0.87       202\n",
      "         sci.space       0.73      0.83      0.78       187\n",
      "\n",
      "          accuracy                           0.80       591\n",
      "         macro avg       0.81      0.80      0.80       591\n",
      "      weighted avg       0.81      0.80      0.80       591\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "# - 정답 라벨과 모델 예측값을 사용해 classification_report 생성\n",
    "from sklearn.metrics import classification_report    # 분류 성능 리포트 함수\n",
    "\n",
    "model.eval()                                         # 모델을 평가 모드로 전환\n",
    "all_preds, all_labels = [], []                       # 전체 예측값/정답 저장용 리스트\n",
    "\n",
    "with torch.no_grad():                                # gradient 계산 비활성화\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # 배치를 GPU로 이동\n",
    "        output = model(X_batch)                      # 모델 예측값(로짓) 계산\n",
    "        loss = criterion(output, y_batch)            # 테스트 손실 계산(로그용)\n",
    "        pred = output.argmax(dim=1)                  # 가장 큰 로짓을 갖는 클래스 선택\n",
    "\n",
    "        all_preds.extend(pred.detach().cpu().numpy())    # 예측 결과를 CPU numpy로 저장\n",
    "        all_labels.extend(y_batch.detach().cpu().numpy())# 실제 라벨 저장\n",
    "\n",
    "print(\n",
    "    classification_report(                           # 클래스별 성능 지표 출력\n",
    "        all_labels,                                  # 실제 라벨\n",
    "        all_preds,                                   # 예측 라벨\n",
    "        target_names=newsgroups.target_names          # 클래스 이름 매핑\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRcaUXF_gGi-"
   },
   "source": [
    "## 사전학습된 임베딩 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22000,
     "status": "ok",
     "timestamp": 1750140554021,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "a9UBkjQlgQdh",
    "outputId": "2f5e4a97-1014-4754-f96f-9a54a3da4027"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1440,
     "status": "ok",
     "timestamp": 1750140871457,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "w8s1Ezq6hQlW",
    "outputId": "da2ef527-897d-4b87-ae81-c7ba2bb5980d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText                 # FastText 단어 임베딩 모델 클래스\n",
    "\n",
    "fasttext_model = FastText.load('ted_en_fasttext.model')  # 사전 학습된 FastText 모델 로드\n",
    "print(fasttext_model.vector_size)                  # 각 단어를 표현하는 임베딩 벡터 차원 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 104,
     "status": "ok",
     "timestamp": 1750141482707,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "bv-YDGQJhyTf",
    "outputId": "aa62b304-44f1-45b2-c9ac-3a1094b38e5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np                                      # 수치 계산용 numpy\n",
    "\n",
    "embedding_dim = fasttext_model.vector_size              # FastText 임베딩 벡터 차원\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))# 단어 사전 크기 × 임베딩 차원 행렬 생성\n",
    "\n",
    "word_index = tokenizer.word_index                       # 전체 단어 → 인덱스 사전 (예: 38,000개)\n",
    "word_index = {word: index                               # vocab_size 이내 단어만 필터링\n",
    "              for word, index in word_index.items()\n",
    "              if index < vocab_size}\n",
    "print(len(word_index))                                  # 실제 사용할 단어 수 확인 (예: 10,000)\n",
    "\n",
    "for word, index in word_index.items():                  # 단어 사전 순회\n",
    "    if word in fasttext_model.wv:                       # FastText 모델에 단어가 있으면\n",
    "        embedding_matrix[index] = fasttext_model.wv[word]  # 해당 단어 임베딩 벡터 복사\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "-wiQxp4Ag9xz"
   },
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "import torch.nn as nn                               # PyTorch 신경망 모듈\n",
    "\n",
    "class LSTMClassifier2(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, embedding_matrix, hidden_size):\n",
    "        super().__init__()                          # nn.Module 초기화\n",
    "        # embedding → lstm → dense 구조\n",
    "\n",
    "        self.embedding = nn.Embedding(              # 단어 인덱스를 임베딩 벡터로 변환\n",
    "            vocab_size,                             # 단어 사전 크기\n",
    "            embedding_dim,                          # 임베딩 벡터 차원\n",
    "            padding_idx=0                           # PAD 토큰은 학습 영향 없음\n",
    "        )\n",
    "        self.embedding.weight.data.copy_(            # 사전학습된 FastText 임베딩 가중치 복사\n",
    "            torch.from_numpy(embedding_matrix)\n",
    "        )\n",
    "        self.embedding.weight.requires_grad = True   # 임베딩을 미세조정(fine-tuning) 허용\n",
    "\n",
    "        self.lstm = nn.LSTM(                        # 문맥 정보를 학습하는 LSTM\n",
    "            embedding_dim,                          # 입력 차원 (임베딩 크기)\n",
    "            hidden_size,                            # 은닉 상태 차원\n",
    "            batch_first=True                        # (batch, seq, feature) 형태 사용\n",
    "        )\n",
    "        self.fc = nn.Linear(                        # 최종 분류용 선형 레이어\n",
    "            hidden_size,                            # LSTM 은닉 상태 차원\n",
    "            3                                       # 클래스 개수 (3개 뉴스그룹)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)                       # (batch, seq) → (batch, seq, embed)\n",
    "        _, (h, c) = self.lstm(x)                    # LSTM 통과 (h: 마지막 은닉 상태)\n",
    "        out = self.fc(h[-1])                        # 마지막 타임스텝 은닉 상태로 분류\n",
    "        return out                                  # (batch, 3) 로짓 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23895,
     "status": "ok",
     "timestamp": 1750142124548,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "47_k6ozcg9x1",
    "outputId": "c2d0c0d9-98bb-40f9-bf06-bf56a79d8627"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: Train Loss 1.1023, Train Acc 0.3307, Val Loss 1.1029, Val Acc 0.3108, \n",
      "Epoch 2/100: Train Loss 1.0951, Train Acc 0.3767, Val Loss 1.0948, Val Acc 0.4059, \n",
      "Epoch 3/100: Train Loss 1.0902, Train Acc 0.4222, Val Loss 1.0886, Val Acc 0.4440, \n",
      "Epoch 4/100: Train Loss 1.0856, Train Acc 0.4466, Val Loss 1.0817, Val Acc 0.4482, \n",
      "Epoch 5/100: Train Loss 1.0793, Train Acc 0.4698, Val Loss 1.0737, Val Acc 0.4630, \n",
      "Epoch 6/100: Train Loss 1.0684, Train Acc 0.5016, Val Loss 1.0590, Val Acc 0.5011, \n",
      "Epoch 7/100: Train Loss 1.0366, Train Acc 0.5063, Val Loss 0.9641, Val Acc 0.5793, \n",
      "Epoch 8/100: Train Loss 0.9392, Train Acc 0.5894, Val Loss 0.8911, Val Acc 0.6195, \n",
      "Epoch 9/100: Train Loss 0.8924, Train Acc 0.6243, Val Loss 0.8476, Val Acc 0.6617, \n",
      "Epoch 10/100: Train Loss 0.8219, Train Acc 0.6937, Val Loss 0.9418, Val Acc 0.5708, \n",
      "Epoch 11/100: Train Loss 0.8128, Train Acc 0.6799, Val Loss 0.7500, Val Acc 0.7357, \n",
      "Epoch 12/100: Train Loss 0.6931, Train Acc 0.7476, Val Loss 0.6837, Val Acc 0.7294, \n",
      "Epoch 13/100: Train Loss 0.6577, Train Acc 0.7397, Val Loss 0.6637, Val Acc 0.7653, \n",
      "Epoch 14/100: Train Loss 0.5620, Train Acc 0.8085, Val Loss 0.7029, Val Acc 0.7738, \n",
      "Epoch 15/100: Train Loss 0.7753, Train Acc 0.6910, Val Loss 1.0093, Val Acc 0.5856, \n",
      "Epoch 16/100: Train Loss 0.7431, Train Acc 0.6857, Val Loss 0.6477, Val Acc 0.7780, \n",
      "Epoch 17/100: Train Loss 0.6065, Train Acc 0.8101, Val Loss 0.6151, Val Acc 0.7696, \n",
      "Epoch 18/100: Train Loss 0.5935, Train Acc 0.8069, Val Loss 0.5742, Val Acc 0.7907, \n",
      "Epoch 19/100: Train Loss 0.5285, Train Acc 0.8307, Val Loss 0.5206, Val Acc 0.8097, \n",
      "Epoch 20/100: Train Loss 0.4342, Train Acc 0.8772, Val Loss 0.5245, Val Acc 0.8118, \n",
      "Epoch 21/100: Train Loss 0.5802, Train Acc 0.7894, Val Loss 0.5979, Val Acc 0.7780, \n",
      "Epoch 22/100: Train Loss 0.5259, Train Acc 0.8238, Val Loss 0.9535, Val Acc 0.6089, \n",
      "Epoch 23/100: Train Loss 0.9871, Train Acc 0.6175, Val Loss 0.9530, Val Acc 0.6195, \n",
      "Epoch 24/100: Train Loss 0.8685, Train Acc 0.6238, Val Loss 0.8594, Val Acc 0.6195, \n",
      "Epoch 25/100: Train Loss 0.7868, Train Acc 0.6280, Val Loss 0.7989, Val Acc 0.6258, \n",
      "Epoch 26/100: Train Loss 0.7351, Train Acc 0.6317, Val Loss 0.7550, Val Acc 0.6216, \n",
      "Epoch 27/100: Train Loss 0.6974, Train Acc 0.6540, Val Loss 0.7268, Val Acc 0.6427, \n",
      "Epoch 28/100: Train Loss 0.6691, Train Acc 0.6571, Val Loss 0.7055, Val Acc 0.6385, \n",
      "Epoch 29/100: Train Loss 0.6425, Train Acc 0.6714, Val Loss 0.6837, Val Acc 0.6660, \n",
      "Epoch 30/100: Train Loss 0.6329, Train Acc 0.6725, Val Loss 0.6768, Val Acc 0.6850, \n",
      "Epoch 31/100: Train Loss 0.6151, Train Acc 0.6915, Val Loss 0.6633, Val Acc 0.6871, \n",
      "Epoch 32/100: Train Loss 0.5828, Train Acc 0.6968, Val Loss 0.6397, Val Acc 0.6913, \n",
      "Epoch 33/100: Train Loss 0.5635, Train Acc 0.7233, Val Loss 0.7081, Val Acc 0.6321, \n",
      "Epoch 34/100: Train Loss 0.6357, Train Acc 0.7058, Val Loss 0.6272, Val Acc 0.7188, \n",
      "Epoch 35/100: Train Loss 0.5490, Train Acc 0.7497, Val Loss 0.6332, Val Acc 0.7336, \n",
      "Epoch 36/100: Train Loss 0.5191, Train Acc 0.7746, Val Loss 0.5889, Val Acc 0.7357, \n",
      "Epoch 37/100: Train Loss 0.5133, Train Acc 0.7868, Val Loss 0.5788, Val Acc 0.7421, \n",
      "Epoch 38/100: Train Loss 0.5195, Train Acc 0.8005, Val Loss 0.5757, Val Acc 0.7696, \n",
      "Epoch 39/100: Train Loss 0.4862, Train Acc 0.8106, Val Loss 0.5735, Val Acc 0.7738, \n",
      "Epoch 40/100: Train Loss 0.4651, Train Acc 0.8302, Val Loss 0.5855, Val Acc 0.7886, \n",
      "Epoch 41/100: Train Loss 0.4986, Train Acc 0.8302, Val Loss 0.5784, Val Acc 0.7886, \n",
      "Epoch 42/100: Train Loss 0.4785, Train Acc 0.8402, Val Loss 0.6235, Val Acc 0.7949, \n",
      "Epoch 43/100: Train Loss 0.4805, Train Acc 0.8566, Val Loss 0.5897, Val Acc 0.7992, \n",
      "Epoch 44/100: Train Loss 0.4299, Train Acc 0.8741, Val Loss 0.5458, Val Acc 0.8055, \n",
      "Epoch 45/100: Train Loss 0.4166, Train Acc 0.8772, Val Loss 0.5428, Val Acc 0.8076, \n",
      "Epoch 46/100: Train Loss 0.4016, Train Acc 0.8831, Val Loss 0.5474, Val Acc 0.8097, \n",
      "Epoch 47/100: Train Loss 0.6017, Train Acc 0.8164, Val Loss 1.4902, Val Acc 0.5074, \n",
      "Epoch 48/100: Train Loss 0.9448, Train Acc 0.6714, Val Loss 0.5554, Val Acc 0.7886, \n",
      "Epoch 49/100: Train Loss 0.4510, Train Acc 0.8725, Val Loss 0.5310, Val Acc 0.7865, \n",
      "Epoch 50/100: Train Loss 0.4036, Train Acc 0.8968, Val Loss 0.5595, Val Acc 0.8097, \n",
      "Epoch 51/100: Train Loss 0.3822, Train Acc 0.9021, Val Loss 0.5037, Val Acc 0.8182, \n",
      "Epoch 52/100: Train Loss 0.3644, Train Acc 0.9037, Val Loss 0.4790, Val Acc 0.8203, \n",
      "Epoch 53/100: Train Loss 0.3336, Train Acc 0.9079, Val Loss 0.4787, Val Acc 0.8309, \n",
      "Epoch 54/100: Train Loss 0.3075, Train Acc 0.9164, Val Loss 0.4886, Val Acc 0.8266, \n",
      "Epoch 55/100: Train Loss 0.2753, Train Acc 0.9291, Val Loss 0.5538, Val Acc 0.8393, \n",
      "Epoch 56/100: Train Loss 0.2769, Train Acc 0.9254, Val Loss 0.4978, Val Acc 0.8626, \n",
      "Epoch 57/100: Train Loss 0.3140, Train Acc 0.9180, Val Loss 0.3854, Val Acc 0.8647, \n",
      "Epoch 58/100: Train Loss 0.2380, Train Acc 0.9339, Val Loss 0.4207, Val Acc 0.8858, \n",
      "Epoch 59/100: Train Loss 0.2108, Train Acc 0.9376, Val Loss 0.4103, Val Acc 0.8414, \n",
      "Epoch 60/100: Train Loss 0.2168, Train Acc 0.9354, Val Loss 0.4369, Val Acc 0.8689, \n",
      "Epoch 61/100: Train Loss 0.2386, Train Acc 0.9307, Val Loss 0.3736, Val Acc 0.8732, \n",
      "Epoch 62/100: Train Loss 0.1968, Train Acc 0.9450, Val Loss 0.3602, Val Acc 0.8943, \n",
      "Epoch 63/100: Train Loss 0.1648, Train Acc 0.9524, Val Loss 0.3787, Val Acc 0.8795, \n",
      "Epoch 64/100: Train Loss 0.1526, Train Acc 0.9582, Val Loss 0.3526, Val Acc 0.8858, \n",
      "Epoch 65/100: Train Loss 0.1489, Train Acc 0.9577, Val Loss 0.3888, Val Acc 0.8901, \n",
      "Epoch 66/100: Train Loss 0.2445, Train Acc 0.9328, Val Loss 0.4330, Val Acc 0.8858, \n",
      "Epoch 67/100: Train Loss 0.3686, Train Acc 0.8989, Val Loss 0.4226, Val Acc 0.8710, \n",
      "Epoch 68/100: Train Loss 0.1609, Train Acc 0.9608, Val Loss 0.3305, Val Acc 0.8858, \n",
      "Epoch 69/100: Train Loss 0.1529, Train Acc 0.9529, Val Loss 0.4710, Val Acc 0.8351, \n",
      "Epoch 70/100: Train Loss 0.1768, Train Acc 0.9455, Val Loss 0.3598, Val Acc 0.9049, \n",
      "Epoch 71/100: Train Loss 0.1278, Train Acc 0.9693, Val Loss 0.3693, Val Acc 0.8943, \n",
      "Epoch 72/100: Train Loss 0.1182, Train Acc 0.9693, Val Loss 0.3727, Val Acc 0.8943, \n",
      "Epoch 73/100: Train Loss 0.1058, Train Acc 0.9720, Val Loss 0.3647, Val Acc 0.9049, \n",
      "Epoch 74/100: Train Loss 0.1021, Train Acc 0.9741, Val Loss 0.3656, Val Acc 0.9006, \n",
      "Epoch 75/100: Train Loss 0.0953, Train Acc 0.9783, Val Loss 0.3743, Val Acc 0.8901, \n",
      "Epoch 76/100: Train Loss 0.0920, Train Acc 0.9757, Val Loss 0.3661, Val Acc 0.9091, \n",
      "Epoch 77/100: Train Loss 0.0881, Train Acc 0.9783, Val Loss 0.3626, Val Acc 0.9091, \n",
      "Epoch 78/100: Train Loss 0.0859, Train Acc 0.9788, Val Loss 0.3328, Val Acc 0.9049, \n",
      "Epoch 79/100: Train Loss 0.0943, Train Acc 0.9735, Val Loss 0.3426, Val Acc 0.9112, \n",
      "Epoch 80/100: Train Loss 0.1790, Train Acc 0.9508, Val Loss 0.3758, Val Acc 0.8879, \n",
      "Epoch 81/100: Train Loss 0.0894, Train Acc 0.9772, Val Loss 0.3481, Val Acc 0.9006, \n",
      "Epoch 82/100: Train Loss 0.0799, Train Acc 0.9804, Val Loss 0.3339, Val Acc 0.9154, \n",
      "Epoch 83/100: Train Loss 0.0760, Train Acc 0.9810, Val Loss 0.3728, Val Acc 0.9091, \n",
      "Epoch 84/100: Train Loss 0.0741, Train Acc 0.9815, Val Loss 0.3092, Val Acc 0.9239, \n",
      "Epoch 85/100: Train Loss 0.0749, Train Acc 0.9815, Val Loss 0.3593, Val Acc 0.9049, \n",
      "Epoch 86/100: Train Loss 0.2434, Train Acc 0.9418, Val Loss 0.4240, Val Acc 0.8858, \n",
      "Epoch 87/100: Train Loss 0.1064, Train Acc 0.9746, Val Loss 0.2858, Val Acc 0.9070, \n",
      "Epoch 88/100: Train Loss 0.0737, Train Acc 0.9815, Val Loss 0.2939, Val Acc 0.9175, \n",
      "Epoch 89/100: Train Loss 0.0644, Train Acc 0.9841, Val Loss 0.3327, Val Acc 0.9154, \n",
      "Epoch 90/100: Train Loss 0.0662, Train Acc 0.9836, Val Loss 0.3420, Val Acc 0.9133, \n",
      "Epoch 91/100: Train Loss 0.0644, Train Acc 0.9820, Val Loss 0.3363, Val Acc 0.9175, \n",
      "Epoch 92/100: Train Loss 0.0742, Train Acc 0.9820, Val Loss 0.3720, Val Acc 0.8985, \n",
      "Epoch 93/100: Train Loss 0.0856, Train Acc 0.9783, Val Loss 0.3148, Val Acc 0.9133, \n",
      "Epoch 94/100: Train Loss 0.0632, Train Acc 0.9873, Val Loss 0.3367, Val Acc 0.9154, \n",
      "Epoch 95/100: Train Loss 0.1484, Train Acc 0.9497, Val Loss 0.4709, Val Acc 0.8288, \n",
      "Epoch 96/100: Train Loss 0.2181, Train Acc 0.9381, Val Loss 0.3280, Val Acc 0.8901, \n",
      "Epoch 97/100: Train Loss 0.1395, Train Acc 0.9661, Val Loss 0.2760, Val Acc 0.9027, \n",
      "Epoch 98/100: Train Loss 0.0815, Train Acc 0.9831, Val Loss 0.2604, Val Acc 0.9197, \n",
      "Epoch 99/100: Train Loss 0.0650, Train Acc 0.9831, Val Loss 0.2790, Val Acc 0.9239, \n",
      "Epoch 100/100: Train Loss 0.0548, Train Acc 0.9884, Val Loss 0.2950, Val Acc 0.9281, \n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "import torch.optim as optim                                  # 옵티마이저 모듈\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # cuda 가능하면 GPU 사용\n",
    "embedding_dim = 100                                          # 임베딩 벡터 차원\n",
    "hidden_size = 128                                            # LSTM 은닉 상태 차원\n",
    "\n",
    "model = LSTMClassifier2(vocab_size, embedding_dim, embedding_matrix, hidden_size).to(device)  # 모델 생성 후 GPU 이동\n",
    "criterion = nn.CrossEntropyLoss()                            # 다중 클래스 분류용 손실 함수\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)        # Adam 옵티마이저 설정 (학습률 1e-4)\n",
    "\n",
    "# 학습 루프 기록용\n",
    "train_losses, train_accs = [], []                            # 학습 손실/정확도 기록\n",
    "val_losses, val_accs = [], []                                # 검증 손실/정확도 기록\n",
    "\n",
    "epochs = 100                                                 # 전체 학습 에폭 수\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # 학습\n",
    "    model.train()                                            # 학습 모드 전환\n",
    "    train_loss, train_correct, train_total = 0, 0, 0          # 에폭 누적 변수 초기화\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # 배치를 GPU로 이동\n",
    "        optimizer.zero_grad()                                # 이전 gradient 초기화\n",
    "        output = model(X_batch)                              # 순전파 (클래스 로짓 출력)\n",
    "        loss = criterion(output, y_batch)                    # 손실 계산\n",
    "        loss.backward()                                      # 역전파\n",
    "        optimizer.step()                                     # 파라미터 업데이트\n",
    "\n",
    "        train_loss += loss.detach().cpu().item()             # 배치 손실 누적\n",
    "        pred = output.argmax(dim=1)                          # 예측 클래스(최대 로짓 인덱스)\n",
    "        train_correct += (pred == y_batch).sum().detach().cpu().item()  # 정답 개수 누적\n",
    "        train_total += len(y_batch)                          # 전체 샘플 수 누적\n",
    "\n",
    "    train_loss /= len(train_loader)                          # 에폭 평균 학습 손실\n",
    "    train_acc = train_correct / train_total                  # 에폭 학습 정확도\n",
    "    train_losses.append(train_loss)                          # 학습 손실 기록\n",
    "    train_accs.append(train_acc)                             # 학습 정확도 기록\n",
    "\n",
    "    # 검증\n",
    "    model.eval()                                             # 평가 모드 전환\n",
    "    val_loss, val_correct, val_total = 0, 0, 0                # 검증 누적 변수 초기화\n",
    "\n",
    "    with torch.no_grad():                                    # gradient 계산 비활성화\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # 배치를 GPU로 이동\n",
    "            output = model(X_batch)                           # 검증 순전파\n",
    "            loss = criterion(output, y_batch)                 # 검증 손실 계산\n",
    "\n",
    "            val_loss += loss.detach().cpu().item()            # 검증 손실 누적\n",
    "            pred = output.argmax(dim=1)                       # 예측 클래스\n",
    "            val_correct += (pred == y_batch).sum().detach().cpu().item()  # 정답 누적\n",
    "            val_total += len(y_batch)                         # 샘플 수 누적\n",
    "\n",
    "        val_loss /= len(val_loader)                           # 에폭 평균 검증 손실\n",
    "        val_acc = val_correct / val_total                     # 에폭 검증 정확도\n",
    "        val_losses.append(val_loss)                           # 검증 손실 기록\n",
    "        val_accs.append(val_acc)                              # 검증 정확도 기록\n",
    "\n",
    "    # 출력 (train_loss, val_loss)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}: '                     # 현재 에폭 정보 출력\n",
    "          f'Train Loss {train_loss:.4f}, '\n",
    "          f'Train Acc {train_acc:.4f}, '\n",
    "          f'Val Loss {val_loss:.4f}, '\n",
    "          f'Val Acc {val_acc:.4f}, ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1750142126818,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "-JkT4PSYg9x3",
    "outputId": "95531598-aa4b-4ba6-daa1-f95780ffa7a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     comp.graphics       0.84      0.80      0.82       202\n",
      "rec.sport.baseball       0.83      0.91      0.87       202\n",
      "         sci.space       0.76      0.72      0.74       187\n",
      "\n",
      "          accuracy                           0.81       591\n",
      "         macro avg       0.81      0.81      0.81       591\n",
      "      weighted avg       0.81      0.81      0.81       591\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "# - 정답 라벨과 모델 예측값을 사용해 classification_report 생성\n",
    "from sklearn.metrics import classification_report    # 분류 성능 리포트 함수\n",
    "\n",
    "model.eval()                                         # 모델을 평가 모드로 전환\n",
    "all_preds, all_labels = [], []                       # 전체 예측값/정답 저장용 리스트\n",
    "\n",
    "with torch.no_grad():                                # gradient 계산 비활성화\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # 배치를 GPU로 이동\n",
    "        output = model(X_batch)                      # 모델 예측값(로짓) 계산\n",
    "        loss = criterion(output, y_batch)            # 테스트 손실 계산(로그/확인용)\n",
    "        pred = output.argmax(dim=1)                  # 가장 큰 로짓을 갖는 클래스 선택\n",
    "\n",
    "        all_preds.extend(pred.detach().cpu().numpy())    # 예측 라벨을 CPU numpy로 저장\n",
    "        all_labels.extend(y_batch.detach().cpu().numpy())# 실제 라벨을 CPU numpy로 저장\n",
    "\n",
    "print(\n",
    "    classification_report(                           # 클래스별 precision/recall/f1 출력\n",
    "        all_labels,                                  # 실제 라벨\n",
    "        all_preds,                                   # 예측 라벨\n",
    "        target_names=newsgroups.target_names          # 클래스 이름 매핑\n",
    "    )\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNEca1JB5mJdMj6HFe4GY4d",
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "nlp_venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
