{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d61f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! This is a test ë¬¸ì¥ì…ë‹ˆë‹¤. NLP ì „ì²˜ë¦¬: ì •ê·œí™”/í† í°í™”/ì¸ì½”ë”© ğŸ˜Š\n",
      "ì •ê·œí™”: hello! this is a test ë¬¸ì¥ì…ë‹ˆë‹¤. nlp ì „ì²˜ë¦¬ ì •ê·œí™” í† í°í™” ì¸ì½”ë”©\n",
      "ë¬¸ì¥: ['hello!', 'this is a test ë¬¸ì¥ì…ë‹ˆë‹¤.', 'nlp ì „ì²˜ë¦¬ ì •ê·œí™” í† í°í™” ì¸ì½”ë”©']\n",
      "ì›Œë“œ: [['hello'], ['test', 'ë¬¸ì¥ì…ë‹ˆë‹¤'], ['nlp', 'ì „ì²˜ë¦¬', 'ì •ê·œí™”', 'í† í°í™”', 'ì¸ì½”ë”©']]\n",
      "ì–´ê°„: [['hello'], ['test', 'ë¬¸ì¥ì…ë‹ˆë‹¤'], ['nlp', 'ì „ì²˜ë¦¬', 'ì •ê·œí™”', 'í† í°í™”', 'ì¸ì½”ë”©']]\n",
      "í‘œì œì–´: [['hello'], ['test', 'ë¬¸ì¥ì…ë‹ˆë‹¤'], ['nlp', 'ì „ì²˜ë¦¬', 'ì •ê·œí™”', 'í† í°í™”', 'ì¸ì½”ë”©']]\n",
      "ì„œë¸Œì›Œë“œ(ë°ëª¨): [['hel', '##lo'], ['test', 'ë¬¸ì¥ì…', '##ë‹ˆë‹¤'], ['nlp', 'ì „ì²˜ë¦¬', 'ì •ê·œí™”', 'í† í°í™”', 'ì¸ì½”ë”©']]\n",
      "vocab size: 12\n",
      "vocab: {'[PAD]': 0, '[UNK]': 1, 'hel': 2, '##lo': 3, 'test': 4, 'ë¬¸ì¥ì…': 5, '##ë‹ˆë‹¤': 6, 'nlp': 7, 'ì „ì²˜ë¦¬': 8, 'ì •ê·œí™”': 9, 'í† í°í™”': 10, 'ì¸ì½”ë”©': 11}\n",
      "ì¸ì½”ë”©: [[2, 3], [4, 5, 6], [7, 8, 9, 10, 11]]\n",
      "íŒ¨ë”©/ë§ˆìŠ¤í¬: [([2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), ([4, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]), ([7, 8, 9, 10, 11, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "import re  # ì •ê·œí‘œí˜„ì‹ ì‚¬ìš©                                     \n",
    "from collections import Counter  # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°ìš©                      \n",
    "\n",
    "TEXT = \"Hello! This is a test ë¬¸ì¥ì…ë‹ˆë‹¤. NLP ì „ì²˜ë¦¬: ì •ê·œí™”/í† í°í™”/ì¸ì½”ë”© ğŸ˜Š\"  # ì˜ˆì‹œ í…ìŠ¤íŠ¸      \n",
    "print(TEXT)\n",
    "def normalize(text: str) -> str:  # ì •ê·œí™” í•¨ìˆ˜(ì†Œë¬¸ìí™”/íŠ¹ìˆ˜ë¬¸ì ì œê±°)      \n",
    "    text = text.lower()  # ì†Œë¬¸ìí™”                                       \n",
    "    text = re.sub(r\"[^\\w\\s\\.!?ê°€-í£]\", \" \", text)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°(ì¼ë¶€ ë¬¸ì¥ë¶€í˜¸ëŠ” ìœ ì§€) \n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # ê³µë°± ì •ë¦¬                    \n",
    "    return text  # ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ ë°˜í™˜                                    \n",
    "\n",
    "def split_sentences(text: str) -> list[str]:  # ë¬¸ì¥ ë¶„ë¦¬ í•¨ìˆ˜               \n",
    "    parts = re.split(r\"(?<=[\\.\\!\\?])\\s+\", text)  # .!? ë’¤ì—ì„œ ë¶„ë¦¬           \n",
    "    parts = [s.strip() for s in parts if s.strip()]  # ë¹ˆ ë¬¸ì¥ ì œê±°          \n",
    "    return parts  # ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜                                        \n",
    "\n",
    "def tokenize_word(sentence: str) -> list[str]:  # ì›Œë“œ í† í°í™”                \n",
    "    return re.findall(r\"[A-Za-z0-9_]+|[ê°€-í£]+\", sentence)  # ì˜ë¬¸/ìˆ«ì/í•œê¸€ ë‹¨ì–´ ì¶”ì¶œ\n",
    "\n",
    "def tokenize_char(sentence: str) -> list[str]:  # ìºë¦­í„° í† í°í™”              \n",
    "    return list(sentence)  # ê¸€ì ë‹¨ìœ„ë¡œ ìª¼ê°¬                               \n",
    "\n",
    "def tokenize_subword_simple(words: list[str]) -> list[str]:  # â€œê°„ë‹¨â€ ì„œë¸Œì›Œë“œ í† í°í™”(ë°ëª¨) \n",
    "    pieces = []  # ê²°ê³¼ ì¡°ê° ë¦¬ìŠ¤íŠ¸                                         \n",
    "    for w in words:  # ê° ë‹¨ì–´ì— ëŒ€í•´                                      \n",
    "        if len(w) <= 4:  # ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©                                \n",
    "            pieces.append(w)  # ê·¸ëŒ€ë¡œ ì¶”ê°€                                 \n",
    "        else:  # ê¸¸ë©´ ì•/ë’¤ë¡œ ëŒ€ì¶© ë¶„í•´(ì§„ì§œ BPE/WordPiece ëŒ€ì²´ìš© ë°ëª¨)       \n",
    "            pieces.append(w[:3])  # ì• 3ê¸€ì                                 \n",
    "            pieces.append(\"##\" + w[3:])  # ë‚˜ë¨¸ì§€(ì ‘ë‘ í‘œê¸° í‰ë‚´)             \n",
    "    return pieces  # ì„œë¸Œì›Œë“œ ì¡°ê° ë°˜í™˜                                     \n",
    "\n",
    "STOPWORDS = {  # ë¶ˆìš©ì–´(ë°ëª¨ìš©, í•„ìš”ì— ë§ê²Œ í™•ì¥)                            \n",
    "    \"is\", \"a\", \"the\", \"this\", \"to\", \"of\", \"and\", \"in\", \"on\", \"for\",  # ì˜ì–´ ë¶ˆìš©ì–´ \n",
    "    \"ì€\", \"ëŠ”\", \"ì´\", \"ê°€\", \"ì„\", \"ë¥¼\", \"ì—\", \"ì˜\", \"ì™€\", \"ê³¼\", \"ë„\"  # í•œê¸€ ì¡°ì‚¬ ì¼ë¶€ \n",
    "}  # ë¶ˆìš©ì–´ ì§‘í•© ë                                                        \n",
    "\n",
    "def remove_stopwords(tokens: list[str]) -> list[str]:  # ë¶ˆìš©ì–´ ì œê±° í•¨ìˆ˜     \n",
    "    return [t for t in tokens if t not in STOPWORDS]  # ë¶ˆìš©ì–´ ì œì™¸          \n",
    "\n",
    "def stem_simple(token: str) -> str:  # ì•„ì£¼ ë‹¨ìˆœí•œ ì–´ê°„ ì¶”ì¶œ(ì˜ì–´ ìœ„ì£¼ ë°ëª¨)  \n",
    "    t = token  # ì‘ì—…ìš© ë³µì‚¬                                                \n",
    "    for suf in (\"ing\", \"ed\", \"es\", \"s\"):  # í”í•œ ì ‘ë¯¸ì‚¬ ëª©ë¡                  \n",
    "        if len(t) > 4 and t.endswith(suf):  # ë„ˆë¬´ ì§§ì€ ë‹¨ì–´ëŠ” ë³´í˜¸            \n",
    "            t = t[: -len(suf)]  # ì ‘ë¯¸ì‚¬ ì œê±°                                \n",
    "            break  # í•˜ë‚˜ë§Œ ì œê±°í•˜ê³  ì¢…ë£Œ                                   \n",
    "    return t  # ìŠ¤í…œ ê²°ê³¼ ë°˜í™˜                                              \n",
    "\n",
    "LEMMA_MAP = {  # í‘œì œì–´ ì¶”ì¶œ(ê°„ë‹¨ ë§¤í•‘ ë°ëª¨: ì‹¤ì œë¡  ì‚¬ì „/ëª¨ë¸ í•„ìš”)           \n",
    "    \"mice\": \"mouse\",  # ì˜ˆì‹œ ë§¤í•‘                                          \n",
    "    \"children\": \"child\",  # ì˜ˆì‹œ ë§¤í•‘                                      \n",
    "    \"went\": \"go\",  # ì˜ˆì‹œ ë§¤í•‘                                              \n",
    "}  # ë§¤í•‘ ë                                                                \n",
    "\n",
    "def lemmatize_simple(token: str) -> str:  # ì•„ì£¼ ë‹¨ìˆœí•œ í‘œì œì–´ ì¶”ì¶œ(ë°ëª¨)     \n",
    "    return LEMMA_MAP.get(token, token)  # ë§¤í•‘ ìˆìœ¼ë©´ ì¹˜í™˜, ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ     \n",
    "\n",
    "def build_vocab(token_sequences: list[list[str]], min_freq: int = 1) -> dict[str, int]:  # ì–´íœ˜ì§‘í•© ìƒì„±\n",
    "    counts = Counter(t for seq in token_sequences for t in seq)  # ì „ì²´ í† í° ë¹ˆë„ ì§‘ê³„ \n",
    "    vocab = {\"[PAD]\": 0, \"[UNK]\": 1}  # íŠ¹ìˆ˜ í† í°(íŒ¨ë”©/ë¯¸ë“±ë¡) ê¸°ë³¸ ë“±ë¡         \n",
    "    for tok, c in counts.most_common():  # ë¹ˆë„ ë†’ì€ ìˆœìœ¼ë¡œ ìˆœíšŒ               \n",
    "        if c >= min_freq and tok not in vocab:  # ìµœì†Œ ë¹ˆë„ ì´ìƒë§Œ              \n",
    "            vocab[tok] = len(vocab)  # ìƒˆ ID ë¶€ì—¬                            \n",
    "    return vocab  # vocab(í† í°â†’id) ë°˜í™˜                                     \n",
    "\n",
    "def integer_encode(tokens: list[str], vocab: dict[str, int]) -> list[int]:  # ì •ìˆ˜ ì¸ì½”ë”©\n",
    "    return [vocab.get(t, vocab[\"[UNK]\"]) for t in tokens]  # ëª¨ë¥´ë©´ UNK       \n",
    "\n",
    "def pad_and_mask(ids: list[int], max_len: int, pad_id: int = 0) -> tuple[list[int], list[int]]:  # íŒ¨ë”©/ë§ˆìŠ¤í‚¹\n",
    "    ids = ids[:max_len]  # ê¸¸ë©´ ìë¥´ê¸°                                      \n",
    "    mask = [1] * len(ids)  # ì‹¤ì œ í† í°ì€ 1                                   \n",
    "    while len(ids) < max_len:  # ì§§ìœ¼ë©´ íŒ¨ë”© ì¶”ê°€                             \n",
    "        ids.append(pad_id)  # PAD ì¶”ê°€                                       \n",
    "        mask.append(0)  # PAD ìœ„ì¹˜ëŠ” 0                                       \n",
    "    return ids, mask  # (íŒ¨ë”©ëœ ids, ë§ˆìŠ¤í¬) ë°˜í™˜                            \n",
    "\n",
    "# --------- â€œí†µìœ¼ë¡œâ€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì˜ˆì‹œ ---------  # êµ¬ë¶„ì„                  \n",
    "norm = normalize(TEXT)  # 1) ì •ê·œí™”                                         \n",
    "sents = split_sentences(norm)  # 2) ë¬¸ì¥ ë¶„ë¦¬                               \n",
    "\n",
    "word_tokens = [tokenize_word(s) for s in sents]  # 3) ì›Œë“œ í† í°í™”(ë¬¸ì¥ë³„)      \n",
    "word_tokens = [remove_stopwords(ws) for ws in word_tokens]  # 4) ë¶ˆìš©ì–´ ì œê±°(ë¬¸ì¥ë³„)\n",
    "\n",
    "stemmed = [[stem_simple(t) for t in ws] for ws in word_tokens]  # 5) ì–´ê°„ ì¶”ì¶œ(ë¬¸ì¥ë³„)\n",
    "lemmatized = [[lemmatize_simple(t) for t in ws] for ws in word_tokens]  # 6) í‘œì œì–´(ë¬¸ì¥ë³„)\n",
    "\n",
    "subword_tokens = [tokenize_subword_simple(ws) for ws in word_tokens]  # 7) ì„œë¸Œì›Œë“œ í† í°í™”(ë°ëª¨)\n",
    "char_tokens = [tokenize_char(s) for s in sents]  # 8) ìºë¦­í„° í† í°í™”(ë¬¸ì¥ë³„)    \n",
    "\n",
    "vocab = build_vocab(subword_tokens, min_freq=1)  # 9) ì–´íœ˜ì§‘í•©(vocab) ìƒì„±(ì—¬ê¸°ì„  ì„œë¸Œì›Œë“œ ê¸°ì¤€)\n",
    "encoded = [integer_encode(seq, vocab) for seq in subword_tokens]  # 10) ì •ìˆ˜ ì¸ì½”ë”©(ë¬¸ì¥ë³„)\n",
    "\n",
    "max_len = 12  # 11) íŒ¨ë”© ê¸°ì¤€ ê¸¸ì´                                          \n",
    "padded_and_masked = [pad_and_mask(seq, max_len, pad_id=vocab[\"[PAD]\"]) for seq in encoded]  # 12) íŒ¨ë”©/ë§ˆìŠ¤í‚¹\n",
    "\n",
    "print(\"ì •ê·œí™”:\", norm)  # ê²°ê³¼ ì¶œë ¥                                         \n",
    "print(\"ë¬¸ì¥:\", sents)  # ê²°ê³¼ ì¶œë ¥                                          \n",
    "print(\"ì›Œë“œ:\", word_tokens)  # ê²°ê³¼ ì¶œë ¥                                    \n",
    "print(\"ì–´ê°„:\", stemmed)  # ê²°ê³¼ ì¶œë ¥                                        \n",
    "print(\"í‘œì œì–´:\", lemmatized)  # ê²°ê³¼ ì¶œë ¥                                   \n",
    "print(\"ì„œë¸Œì›Œë“œ(ë°ëª¨):\", subword_tokens)  # ê²°ê³¼ ì¶œë ¥                        \n",
    "print(\"vocab size:\", len(vocab))  # ê²°ê³¼ ì¶œë ¥                                \n",
    "print(\"vocab:\", vocab)  # ê²°ê³¼ ì¶œë ¥                                \n",
    "print(\"ì¸ì½”ë”©:\", encoded)  # ê²°ê³¼ ì¶œë ¥                                      \n",
    "print(\"íŒ¨ë”©/ë§ˆìŠ¤í¬:\", padded_and_masked)  # ê²°ê³¼ ì¶œë ¥                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64e1b5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1) í˜•íƒœì†Œ ë¶„ì„(ë°ëª¨) ===\n",
      "ë¨¹ì—ˆìŠµë‹ˆë‹¤ -> ë¨¹/VV/VA? + ì—ˆìŠµë‹ˆë‹¤/EF\n",
      "ë¨¹ëŠ” -> ë¨¹/NNG/VV? + ëŠ”/JX\n",
      "í•™ìƒì„ -> í•™ìƒ/NNG/VV? + ì„/JKO\n",
      "ìì—°ì–´ì²˜ë¦¬ëŠ” -> ìì—°ì–´ì²˜ë¦¬/NNG/VV? + ëŠ”/JX\n",
      "\n",
      "=== 2) BPE í•™ìŠµ + WordPiece(##) ìŠ¤íƒ€ì¼ ===\n",
      "í•™ìŠµ ë‹¨ì–´ë“¤: ['i', 'love', 'tokenization', 'and', 'nlp', 'tokenization', 'makes', 'models', 'robust']\n",
      "ë³‘í•© ì¼ë¶€: [('k', 'e'), ('t', 'o'), ('to', 'ke'), ('toke', 'n'), ('token', 'i'), ('tokeni', 'z'), ('tokeniz', 'a'), ('tokeniza', 't'), ('tokenizat', 'i'), ('tokenizati', 'o')]\n",
      "tokenization -> BPE: ['tokenization</w>'] -> WordPiece-style: ['tokenization</w>']\n",
      "models -> BPE: ['mode', 'l', 's</w>'] -> WordPiece-style: ['mode', '##l', '##s</w>']\n",
      "robust -> BPE: ['r', 'o', 'b', 'u', 's', 't'] -> WordPiece-style: ['r', '##o', '##b', '##u', '##s', '##t']\n",
      "\n",
      "=== 3) ì„ë² ë”©(ê³µê°„) ===\n",
      "tokens: ['tokenization</w>']\n",
      "ids   : [2]\n",
      "vecs shape: (1, 8)\n",
      "\n",
      "=== 4) í¬ì§€ì…˜ + ë§ˆìŠ¤í¬(Self-Attention) ===\n",
      "ids_padded: [2, 0, 0, 0, 0, 0]\n",
      "mask      : [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "attention row0(sum): 0.9999999989999999\n",
      "attention row0 to PAD idx: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "no-mask row0 to PAD idx: [3.81400250275637e-08, 1.032163907460866e-06, 3.29044389342248e-06, 4.7612960365780886e-07, 2.1288682403033365e-08]\n"
     ]
    }
   ],
   "source": [
    "import re  # ì •ê·œí‘œí˜„ì‹                                                   \n",
    "import math  # ìˆ˜í•™ í•¨ìˆ˜                                                 \n",
    "import numpy as np  # ë²¡í„°/í–‰ë ¬ ê³„ì‚°                                     \n",
    "from collections import Counter, defaultdict  # ë¹ˆë„/ë§µí•‘                 \n",
    "\n",
    "# =========================\n",
    "# 0) ì˜ˆì‹œ í…ìŠ¤íŠ¸\n",
    "# =========================\n",
    "TEXTS = [  # ê°„ë‹¨ ì½”í¼ìŠ¤                                                 \n",
    "    \"ë¨¹ì—ˆìŠµë‹ˆë‹¤\",  # í˜•íƒœì†Œ ì˜ˆì‹œ                                         \n",
    "    \"ë¨¹ëŠ” ì¤‘ì…ë‹ˆë‹¤\",  # í˜•íƒœì†Œ ì˜ˆì‹œ                                     \n",
    "    \"ìì—°ì–´ì²˜ë¦¬ëŠ” ì¬ë¯¸ìˆìŠµë‹ˆë‹¤\",  # í˜•íƒœì†Œ ì˜ˆì‹œ                          \n",
    "    \"I love tokenization and NLP\",  # ì„œë¸Œì›Œë“œ/ì„ë² ë”© ì˜ˆì‹œ               \n",
    "    \"tokenization makes models robust\",  # ì„œë¸Œì›Œë“œ ì˜ˆì‹œ                 \n",
    "]  # ì½”í¼ìŠ¤ ë                                                           \n",
    "\n",
    "# =========================\n",
    "# 1) í˜•íƒœì†Œ ë¶„ì„(ë°ëª¨): ì¡°ì‚¬/ì–´ë¯¸ ë¶„ë¦¬ + POS í‰ë‚´\n",
    "#    â€» ì‹¤ì œë¡  Mecab/Komoran/OKT ë“± í˜•íƒœì†Œ ë¶„ì„ê¸° ì‚¬ìš©\n",
    "# =========================\n",
    "KOREAN_ENDINGS = [  # ì•„ì£¼ ì¼ë¶€ ì–´ë¯¸/ì¢…ê²°(ë°ëª¨)                           \n",
    "    (\"ì—ˆìŠµë‹ˆë‹¤\", \"EF\"),  # ì¢…ê²° ì–´ë¯¸ í‰ë‚´                                \n",
    "    (\"ìŠµë‹ˆë‹¤\", \"EF\"),  # ì¢…ê²° ì–´ë¯¸ í‰ë‚´                                \n",
    "    (\"ì…ë‹ˆë‹¤\", \"EF\"),  # ì¢…ê²° ì–´ë¯¸ í‰ë‚´                                \n",
    "    (\"ëŠ”\", \"ETM\"),  # ê´€í˜•ì‚¬í˜• ì–´ë¯¸ í‰ë‚´                                \n",
    "    (\"ì—ˆ\", \"EP\"),  # ì„ ì–´ë§ ì–´ë¯¸ í‰ë‚´                                   \n",
    "]  # ì–´ë¯¸ ëª©ë¡ ë                                                        \n",
    "\n",
    "KOREAN_PARTICLES = [  # ì•„ì£¼ ì¼ë¶€ ì¡°ì‚¬(ë°ëª¨)                              \n",
    "    (\"ì„\", \"JKO\"), (\"ë¥¼\", \"JKO\"),  # ëª©ì ê²© ì¡°ì‚¬                          \n",
    "    (\"ì€\", \"JX\"), (\"ëŠ”\", \"JX\"),  # ë³´ì¡°ì‚¬                                \n",
    "    (\"ì´\", \"JKS\"), (\"ê°€\", \"JKS\"),  # ì£¼ê²© ì¡°ì‚¬                            \n",
    "    (\"ì—\", \"JKB\"), (\"ì˜\", \"JKG\"),  # ë¶€ì‚¬ê²©/ê´€í˜•ê²© ì¡°ì‚¬                   \n",
    "]  # ì¡°ì‚¬ ëª©ë¡ ë                                                         \n",
    "\n",
    "def morph_analyze_demo(token: str) -> list[tuple[str, str]]:  # í˜•íƒœì†Œ ë¶„ì„(ë°ëª¨)  \n",
    "    t = token.strip()  # ê³µë°± ì œê±°                                       \n",
    "    out = []  # ê²°ê³¼                                                     \n",
    "    for suf, pos in sorted(KOREAN_PARTICLES, key=lambda x: -len(x[0])):  # ê¸´ ì¡°ì‚¬ë¶€í„°\n",
    "        if t.endswith(suf) and len(t) > len(suf):  # ì¡°ì‚¬ë¡œ ëë‚˜ë©´        \n",
    "            stem = t[:-len(suf)]  # ì•ë¶€ë¶„                               \n",
    "            out.append((stem, \"NNG/VV?\"))  # ì–´ê°„(ëª…ì‚¬/ë™ì‚¬?) ë°ëª¨         \n",
    "            out.append((suf, pos))  # ì¡°ì‚¬                               \n",
    "            return out  # ì¢…ë£Œ                                            \n",
    "    for suf, pos in sorted(KOREAN_ENDINGS, key=lambda x: -len(x[0])):  # ê¸´ ì–´ë¯¸ë¶€í„°\n",
    "        if t.endswith(suf) and len(t) > len(suf):  # ì–´ë¯¸ë¡œ ëë‚˜ë©´        \n",
    "            stem = t[:-len(suf)]  # ì–´ê°„                                 \n",
    "            out.append((stem, \"VV/VA?\"))  # ì–´ê°„(ë™ì‚¬/í˜•ìš©ì‚¬?) ë°ëª¨        \n",
    "            out.append((suf, pos))  # ì–´ë¯¸                               \n",
    "            return out  # ì¢…ë£Œ                                            \n",
    "    return [(t, \"NNG/VV?\")]  # ëª» ìë¥´ë©´ í†µì§¸ë¡œ                          \n",
    "\n",
    "print(\"=== 1) í˜•íƒœì†Œ ë¶„ì„(ë°ëª¨) ===\")  # ì„¹ì…˜ ì¶œë ¥                         \n",
    "for s in [\"ë¨¹ì—ˆìŠµë‹ˆë‹¤\", \"ë¨¹ëŠ”\", \"í•™ìƒì„\", \"ìì—°ì–´ì²˜ë¦¬ëŠ”\"]:  # ì˜ˆì‹œ            \n",
    "    print(s, \"->\", \" + \".join([f\"{m}/{p}\" for m, p in morph_analyze_demo(s)]))  # ì¶œë ¥\n",
    "\n",
    "# =========================\n",
    "# 2) ì§„ì§œ ì„œë¸Œì›Œë“œ: BPE(í•™ìŠµ) + WordPiece ìŠ¤íƒ€ì¼(##) í† í°í™”\n",
    "# =========================\n",
    "\n",
    "def basic_normalize_en(text: str) -> list[str]:  # ì˜ì–´ í† í°í™”(ë°ëª¨)      \n",
    "    return re.findall(r\"[a-zA-Z]+\", text.lower())  # ë‹¨ì–´ë§Œ ì¶”ì¶œ           \n",
    "\n",
    "CORPUS_WORDS = []  # BPE í•™ìŠµìš© ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸                               \n",
    "for t in TEXTS:  # ì½”í¼ìŠ¤ ìˆœíšŒ                                            \n",
    "    CORPUS_WORDS += basic_normalize_en(t)  # ì˜ì–´ ë‹¨ì–´ ëˆ„ì                 \n",
    "\n",
    "def build_bpe_vocab(words: list[str]) -> dict[tuple[str, ...], int]:  # BPEìš© vocab(ë‹¨ì–´->ë¹ˆë„)\n",
    "    counts = Counter(words)  # ë‹¨ì–´ ë¹ˆë„                                  \n",
    "    vocab = {}  # {(ë¬¸ì ì‹œí€€ìŠ¤): ë¹ˆë„}                                  \n",
    "    for w, c in counts.items():  # ê° ë‹¨ì–´                                \n",
    "        seq = tuple(list(w) + [\"</w>\"])  # ë¬¸ì ë‹¨ìœ„ + ì¢…ë£Œ í‘œì‹œ           \n",
    "        vocab[seq] = c  # ë¹ˆë„ ì €ì¥                                       \n",
    "    return vocab  # ë°˜í™˜                                                  \n",
    "\n",
    "def get_pair_stats(vocab: dict[tuple[str, ...], int]) -> Counter:  # bigram í†µê³„\n",
    "    stats = Counter()  # (a,b) ë¹ˆë„                                       \n",
    "    for seq, c in vocab.items():  # ê° ë‹¨ì–´ ì‹œí€€ìŠ¤                         \n",
    "        for i in range(len(seq) - 1):  # ì¸ì ‘ìŒ                            \n",
    "            stats[(seq[i], seq[i+1])] += c  # ê°€ì¤‘ ë¹ˆë„                    \n",
    "    return stats  # ë°˜í™˜                                                  \n",
    "\n",
    "def merge_pair(vocab: dict[tuple[str, ...], int], pair: tuple[str, str]) -> dict[tuple[str, ...], int]:  # ë³‘í•©\n",
    "    a, b = pair  # ë³‘í•© ëŒ€ìƒ                                               \n",
    "    new_vocab = {}  # ìƒˆ vocab                                             \n",
    "    ab = a + b  # í•©ì¹œ í† í°                                                 \n",
    "    for seq, c in vocab.items():  # ê° ì‹œí€€ìŠ¤                              \n",
    "        new_seq = []  # ë³‘í•© ê²°ê³¼                                          \n",
    "        i = 0  # ì¸ë±ìŠ¤                                                    \n",
    "        while i < len(seq):  # ìˆœíšŒ                                        \n",
    "            if i < len(seq) - 1 and seq[i] == a and seq[i+1] == b:  # ìŒì´ë©´\n",
    "                new_seq.append(ab)  # ë³‘í•© í† í° ì¶”ê°€                       \n",
    "                i += 2  # 2ì¹¸ ì´ë™                                        \n",
    "            else:  # ì•„ë‹ˆë©´                                                \n",
    "                new_seq.append(seq[i])  # ê·¸ëŒ€ë¡œ ì¶”ê°€                      \n",
    "                i += 1  # 1ì¹¸ ì´ë™                                        \n",
    "        new_vocab[tuple(new_seq)] = new_vocab.get(tuple(new_seq), 0) + c  # ëˆ„ì \n",
    "    return new_vocab  # ë°˜í™˜                                               \n",
    "\n",
    "def train_bpe(words: list[str], num_merges: int = 20) -> tuple[list[tuple[str, str]], set[str]]:  # BPE í•™ìŠµ\n",
    "    vocab = build_bpe_vocab(words)  # ì´ˆê¸° ë¬¸ì vocab                      \n",
    "    merges = []  # ë³‘í•© ê¸°ë¡                                               \n",
    "    for _ in range(num_merges):  # ë³‘í•© ë°˜ë³µ                               \n",
    "        stats = get_pair_stats(vocab)  # ìŒ ë¹ˆë„                           \n",
    "        if not stats:  # ë” ì´ìƒ ì—†ìœ¼ë©´                                    \n",
    "            break  # ì¢…ë£Œ                                                  \n",
    "        best = stats.most_common(1)[0][0]  # ìµœë¹ˆ ìŒ                        \n",
    "        merges.append(best)  # ê¸°ë¡                                        \n",
    "        vocab = merge_pair(vocab, best)  # ë³‘í•© ì ìš©                        \n",
    "    token_set = set()  # ìµœì¢… í† í° ì§‘í•©                                    \n",
    "    for seq in vocab.keys():  # vocabì˜ ì‹œí€€ìŠ¤ë“¤                            \n",
    "        for tok in seq:  # ê° í† í°                                         \n",
    "            token_set.add(tok)  # ì¶”ê°€                                     \n",
    "    return merges, token_set  # ë°˜í™˜                                       \n",
    "\n",
    "def bpe_encode(word: str, merges: list[tuple[str, str]]) -> list[str]:  # BPE ì¸ì½”ë”©(ë‹¨ì–´ ë‹¨ìœ„)\n",
    "    seq = list(word) + [\"</w>\"]  # ë¬¸ì + ì¢…ë£Œ                             \n",
    "    for a, b in merges:  # ë³‘í•© ìˆœì„œëŒ€ë¡œ                                   \n",
    "        i = 0  # ì¸ë±ìŠ¤                                                    \n",
    "        new_seq = []  # ë³‘í•© ê²°ê³¼                                          \n",
    "        while i < len(seq):  # ìˆœíšŒ                                        \n",
    "            if i < len(seq) - 1 and seq[i] == a and seq[i+1] == b:  # ìŒì´ë©´\n",
    "                new_seq.append(a + b)  # í•©ì¹œ í† í°                          \n",
    "                i += 2  # ì´ë™                                            \n",
    "            else:  # ì•„ë‹ˆë©´                                                \n",
    "                new_seq.append(seq[i])  # ê·¸ëŒ€ë¡œ                           \n",
    "                i += 1  # ì´ë™                                            \n",
    "        seq = new_seq  # ê°±ì‹                                               \n",
    "    if seq and seq[-1] == \"</w>\":  # ì¢…ë£Œ í† í° ì œê±°                         \n",
    "        seq = seq[:-1]  # ì œê±°                                            \n",
    "    return seq  # ë°˜í™˜                                                     \n",
    "\n",
    "# WordPiece ìŠ¤íƒ€ì¼ í† í°í™”(##): \"ë‹¨ì–´ ë‚´ë¶€ ì¡°ê°\"ì„ ##ë¡œ í‘œì‹œ\n",
    "def wordpiece_style(pieces: list[str]) -> list[str]:  # ## ìŠ¤íƒ€ì¼ ë³€í™˜     \n",
    "    if not pieces:  # ë¹ˆ ì…ë ¥                                              \n",
    "        return []  # ë°˜í™˜                                                  \n",
    "    out = [pieces[0]]  # ì²« ì¡°ê°ì€ ê·¸ëŒ€ë¡œ                                  \n",
    "    for p in pieces[1:]:  # ë‚˜ë¨¸ì§€ ì¡°ê°                                    \n",
    "        out.append(\"##\" + p)  # ë’¤ ì¡°ê°ì€ ## í‘œì‹œ                          \n",
    "    return out  # ë°˜í™˜                                                     \n",
    "\n",
    "print(\"\\n=== 2) BPE í•™ìŠµ + WordPiece(##) ìŠ¤íƒ€ì¼ ===\")  # ì„¹ì…˜ ì¶œë ¥           \n",
    "MERGES, TOKSET = train_bpe(CORPUS_WORDS, num_merges=30)  # BPE í•™ìŠµ        \n",
    "print(\"í•™ìŠµ ë‹¨ì–´ë“¤:\", CORPUS_WORDS)  # ì½”í¼ìŠ¤ ë‹¨ì–´                         \n",
    "print(\"ë³‘í•© ì¼ë¶€:\", MERGES[:10])  # ë³‘í•© ì•ë¶€ë¶„                            \n",
    "for w in [\"tokenization\", \"models\", \"robust\"]:  # í…ŒìŠ¤íŠ¸ ë‹¨ì–´              \n",
    "    bpe = bpe_encode(w, MERGES)  # BPE ì¡°ê°                                \n",
    "    wp = wordpiece_style(bpe)  # ## ìŠ¤íƒ€ì¼                                 \n",
    "    print(w, \"-> BPE:\", bpe, \"-> WordPiece-style:\", wp)  # ì¶œë ¥            \n",
    "\n",
    "# =========================\n",
    "# 3) ì„ë² ë”©: token id -> vector (ê³µê°„)\n",
    "# =========================\n",
    "def build_vocab(tokens: list[str]) -> dict[str, int]:  # vocab ìƒì„±        \n",
    "    vocab = {\"[PAD]\": 0, \"[UNK]\": 1}  # íŠ¹ìˆ˜ í† í°                          \n",
    "    for t in tokens:  # í† í° ìˆœíšŒ                                          \n",
    "        if t not in vocab:  # ì—†ìœ¼ë©´                                       \n",
    "            vocab[t] = len(vocab)  # id ë¶€ì—¬                               \n",
    "    return vocab  # ë°˜í™˜                                                   \n",
    "\n",
    "def encode(tokens: list[str], vocab: dict[str, int]) -> list[int]:  # id ì¸ì½”ë”©\n",
    "    return [vocab.get(t, vocab[\"[UNK]\"]) for t in tokens]  # UNK ì²˜ë¦¬       \n",
    "\n",
    "def cosine(a: np.ndarray, b: np.ndarray) -> float:  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„        \n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-9))  # ì•ˆì •í™”\n",
    "\n",
    "print(\"\\n=== 3) ì„ë² ë”©(ê³µê°„) ===\")  # ì„¹ì…˜ ì¶œë ¥                              \n",
    "sample_tokens = wordpiece_style(bpe_encode(\"tokenization\", MERGES))  # ì˜ˆì‹œ í† í°\n",
    "vocab = build_vocab(sample_tokens)  # vocab ìƒì„±                           \n",
    "ids = encode(sample_tokens, vocab)  # id ë³€í™˜                              \n",
    "d = 8  # ì„ë² ë”© ì°¨ì›(ë°ëª¨)                                                 \n",
    "E = np.random.randn(len(vocab), d)  # ì„ë² ë”© í–‰ë ¬(í† í° ìˆ˜ x ì°¨ì›)           \n",
    "vecs = E[ids]  # (ê¸¸ì´ x d)                                                \n",
    "print(\"tokens:\", sample_tokens)  # í† í° ì¶œë ¥                               \n",
    "print(\"ids   :\", ids)  # id ì¶œë ¥                                          \n",
    "print(\"vecs shape:\", vecs.shape)  # ì„ë² ë”© í…ì„œ í¬ê¸°                       \n",
    "\n",
    "if len(vecs) >= 2:  # ë¹„êµ ê°€ëŠ¥í•˜ë©´                                       \n",
    "    print(\"cosine(token[0], token[1]) =\", cosine(vecs[0], vecs[1]))  # ê³µê°„ ìœ ì‚¬ë„\n",
    "\n",
    "# =========================\n",
    "# 4) í¬ì§€ì…˜ ì„ë² ë”© + attention mask ì˜ë¯¸(íŒ¨ë”© ì˜í–¥ ì°¨ë‹¨)\n",
    "#    - toy self-attention(1-head) êµ¬í˜„\n",
    "# =========================\n",
    "def softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:  # ì†Œí”„íŠ¸ë§¥ìŠ¤    \n",
    "    x = x - np.max(x, axis=axis, keepdims=True)  # ì•ˆì •í™”                  \n",
    "    e = np.exp(x)  # ì§€ìˆ˜                                                  \n",
    "    return e / (np.sum(e, axis=axis, keepdims=True) + 1e-9)  # ì •ê·œí™”       \n",
    "\n",
    "def add_positional_embedding(X: np.ndarray) -> np.ndarray:  # í¬ì§€ì…˜ ì„ë² ë”©(ì‚¬ì¸/ì½”ì‚¬ì¸)\n",
    "    L, D = X.shape  # ê¸¸ì´/ì°¨ì›                                            \n",
    "    P = np.zeros((L, D))  # í¬ì§€ì…˜ í–‰ë ¬                                    \n",
    "    for pos in range(L):  # ìœ„ì¹˜                                           \n",
    "        for i in range(0, D, 2):  # ì§/í™€                                   \n",
    "            denom = 10000 ** (i / D)  # ë¶„ëª¨                               \n",
    "            P[pos, i] = math.sin(pos / denom)  # sin                        \n",
    "            if i + 1 < D:  # ë²”ìœ„ ì²´í¬                                     \n",
    "                P[pos, i+1] = math.cos(pos / denom)  # cos                 \n",
    "    return X + P  # ë”í•´ì„œ ë°˜í™˜                                            \n",
    "\n",
    "def self_attention_toy(X: np.ndarray, mask: np.ndarray) -> tuple[np.ndarray, np.ndarray]:  # ì–´í…ì…˜(ë°ëª¨)\n",
    "    D = X.shape[1]  # ì°¨ì›                                                 \n",
    "    Wq = np.random.randn(D, D)  # Q ê°€ì¤‘ì¹˜                                 \n",
    "    Wk = np.random.randn(D, D)  # K ê°€ì¤‘ì¹˜                                 \n",
    "    Wv = np.random.randn(D, D)  # V ê°€ì¤‘ì¹˜                                 \n",
    "    Q = X @ Wq  # (L,D)                                                    \n",
    "    K = X @ Wk  # (L,D)                                                    \n",
    "    V = X @ Wv  # (L,D)                                                    \n",
    "    scores = (Q @ K.T) / math.sqrt(D)  # (L,L) ì ìˆ˜                         \n",
    "    # mask: 1=ìœ íš¨, 0=PAD -> PAD ìœ„ì¹˜ë¡œ ê°€ëŠ” attentionì„ ë§¤ìš° ì‘ì€ ê°’ìœ¼ë¡œ ë§‰ìŒ\n",
    "    pad_to = (1 - mask)[None, :]  # (1,L) PAD ìœ„ì¹˜                          \n",
    "    scores = scores + pad_to * (-1e9)  # PADë¡œ ê°€ëŠ” ê²½ë¡œ ì°¨ë‹¨               \n",
    "    A = softmax(scores, axis=-1)  # attention weights                       \n",
    "    Y = A @ V  # ì»¨í…ìŠ¤íŠ¸ ê²°ê³¼ (L,D)                                       \n",
    "    return Y, A  # ì¶œë ¥/ê°€ì¤‘ì¹˜ ë°˜í™˜                                         \n",
    "\n",
    "print(\"\\n=== 4) í¬ì§€ì…˜ + ë§ˆìŠ¤í¬(Self-Attention) ===\")  # ì„¹ì…˜ ì¶œë ¥           \n",
    "# ì˜ˆì‹œ: ê¸¸ì´ 6ìœ¼ë¡œ íŒ¨ë”©(ë’¤ 2ê°œ PAD)                                        \n",
    "max_len = 6  # ìµœëŒ€ ê¸¸ì´                                                   \n",
    "pad_id = vocab[\"[PAD]\"]  # PAD id                                          \n",
    "ids_padded = ids[:max_len]  # ìë¥´ê¸°                                       \n",
    "mask = np.ones(len(ids_padded), dtype=np.float32)  # ë§ˆìŠ¤í¬(ìœ íš¨=1)         \n",
    "while len(ids_padded) < max_len:  # íŒ¨ë”© ì¶”ê°€                              \n",
    "    ids_padded.append(pad_id)  # PAD ì¶”ê°€                                  \n",
    "    mask = np.append(mask, 0.0)  # PAD ë§ˆìŠ¤í¬=0                             \n",
    "\n",
    "X = E[ids_padded]  # ì„ë² ë”© ì¡°íšŒ (L,D)                                     \n",
    "X_pos = add_positional_embedding(X)  # í¬ì§€ì…˜ ì¶”ê°€                          \n",
    "Y, A = self_attention_toy(X_pos, mask)  # ë§ˆìŠ¤í¬ ì ìš© ì–´í…ì…˜                \n",
    "\n",
    "print(\"ids_padded:\", ids_padded)  # íŒ¨ë”© id                                \n",
    "print(\"mask      :\", mask.tolist())  # ë§ˆìŠ¤í¬                              \n",
    "print(\"attention row0(sum):\", float(np.sum(A[0])))  # í™•ë¥ í•©=1 í™•ì¸          \n",
    "print(\"attention row0 to PAD idx:\", [float(A[0, i]) for i in range(max_len) if mask[i] == 0.0])  # PADë¡œ ê±°ì˜ 0\n",
    "\n",
    "# (ë¹„êµ) ë§ˆìŠ¤í¬ ì—†ì´ ëŒë¦¬ë©´ PADì—ë„ attentionì´ ìƒˆì–´ ë‚˜ê°ˆ ìˆ˜ ìˆìŒ           \n",
    "Y2, A2 = self_attention_toy(X_pos, np.ones_like(mask))  # ë§ˆìŠ¤í¬ ì—†ëŠ” ì²™    \n",
    "print(\"no-mask row0 to PAD idx:\", [float(A2[0, i]) for i in range(max_len) if mask[i] == 0.0])  # PADë¡œ ìƒˆëŠ” ê°’\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
