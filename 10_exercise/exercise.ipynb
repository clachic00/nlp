{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48d61f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì •ê·œí™”: hello! this is a test ë¬¸ì¥ì…ë‹ˆë‹¤. nlp ì „ì²˜ë¦¬ ì •ê·œí™” í† í°í™” ì¸ì½”ë”©\n",
      "ë¬¸ì¥: ['hello!', 'this is a test ë¬¸ì¥ì…ë‹ˆë‹¤.', 'nlp ì „ì²˜ë¦¬ ì •ê·œí™” í† í°í™” ì¸ì½”ë”©']\n",
      "ì›Œë“œ: [['hello'], ['test', 'ë¬¸ì¥ì…ë‹ˆë‹¤'], ['nlp', 'ì „ì²˜ë¦¬', 'ì •ê·œí™”', 'í† í°í™”', 'ì¸ì½”ë”©']]\n",
      "ì–´ê°„: [['hello'], ['test', 'ë¬¸ì¥ì…ë‹ˆë‹¤'], ['nlp', 'ì „ì²˜ë¦¬', 'ì •ê·œí™”', 'í† í°í™”', 'ì¸ì½”ë”©']]\n",
      "í‘œì œì–´: [['hello'], ['test', 'ë¬¸ì¥ì…ë‹ˆë‹¤'], ['nlp', 'ì „ì²˜ë¦¬', 'ì •ê·œí™”', 'í† í°í™”', 'ì¸ì½”ë”©']]\n",
      "ì„œë¸Œì›Œë“œ(ë°ëª¨): [['hel', '##lo'], ['test', 'ë¬¸ì¥ì…', '##ë‹ˆë‹¤'], ['nlp', 'ì „ì²˜ë¦¬', 'ì •ê·œí™”', 'í† í°í™”', 'ì¸ì½”ë”©']]\n",
      "vocab size: 12\n",
      "ì¸ì½”ë”©: [[2, 3], [4, 5, 6], [7, 8, 9, 10, 11]]\n",
      "íŒ¨ë”©/ë§ˆìŠ¤í¬: [([2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), ([4, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]), ([7, 8, 9, 10, 11, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "import re  # ì •ê·œí‘œí˜„ì‹ ì‚¬ìš©                                     \n",
    "from collections import Counter  # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°ìš©                      \n",
    "\n",
    "TEXT = \"Hello! This is a test ë¬¸ì¥ì…ë‹ˆë‹¤. NLP ì „ì²˜ë¦¬: ì •ê·œí™”/í† í°í™”/ì¸ì½”ë”© ğŸ˜Š\"  # ì˜ˆì‹œ í…ìŠ¤íŠ¸      \n",
    "\n",
    "def normalize(text: str) -> str:  # ì •ê·œí™” í•¨ìˆ˜(ì†Œë¬¸ìí™”/íŠ¹ìˆ˜ë¬¸ì ì œê±°)      \n",
    "    text = text.lower()  # ì†Œë¬¸ìí™”                                       \n",
    "    text = re.sub(r\"[^\\w\\s\\.!?ê°€-í£]\", \" \", text)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°(ì¼ë¶€ ë¬¸ì¥ë¶€í˜¸ëŠ” ìœ ì§€) \n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # ê³µë°± ì •ë¦¬                    \n",
    "    return text  # ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ ë°˜í™˜                                    \n",
    "\n",
    "def split_sentences(text: str) -> list[str]:  # ë¬¸ì¥ ë¶„ë¦¬ í•¨ìˆ˜               \n",
    "    parts = re.split(r\"(?<=[\\.\\!\\?])\\s+\", text)  # .!? ë’¤ì—ì„œ ë¶„ë¦¬           \n",
    "    parts = [s.strip() for s in parts if s.strip()]  # ë¹ˆ ë¬¸ì¥ ì œê±°          \n",
    "    return parts  # ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜                                        \n",
    "\n",
    "def tokenize_word(sentence: str) -> list[str]:  # ì›Œë“œ í† í°í™”                \n",
    "    return re.findall(r\"[A-Za-z0-9_]+|[ê°€-í£]+\", sentence)  # ì˜ë¬¸/ìˆ«ì/í•œê¸€ ë‹¨ì–´ ì¶”ì¶œ\n",
    "\n",
    "def tokenize_char(sentence: str) -> list[str]:  # ìºë¦­í„° í† í°í™”              \n",
    "    return list(sentence)  # ê¸€ì ë‹¨ìœ„ë¡œ ìª¼ê°¬                               \n",
    "\n",
    "def tokenize_subword_simple(words: list[str]) -> list[str]:  # â€œê°„ë‹¨â€ ì„œë¸Œì›Œë“œ í† í°í™”(ë°ëª¨) \n",
    "    pieces = []  # ê²°ê³¼ ì¡°ê° ë¦¬ìŠ¤íŠ¸                                         \n",
    "    for w in words:  # ê° ë‹¨ì–´ì— ëŒ€í•´                                      \n",
    "        if len(w) <= 4:  # ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©                                \n",
    "            pieces.append(w)  # ê·¸ëŒ€ë¡œ ì¶”ê°€                                 \n",
    "        else:  # ê¸¸ë©´ ì•/ë’¤ë¡œ ëŒ€ì¶© ë¶„í•´(ì§„ì§œ BPE/WordPiece ëŒ€ì²´ìš© ë°ëª¨)       \n",
    "            pieces.append(w[:3])  # ì• 3ê¸€ì                                 \n",
    "            pieces.append(\"##\" + w[3:])  # ë‚˜ë¨¸ì§€(ì ‘ë‘ í‘œê¸° í‰ë‚´)             \n",
    "    return pieces  # ì„œë¸Œì›Œë“œ ì¡°ê° ë°˜í™˜                                     \n",
    "\n",
    "STOPWORDS = {  # ë¶ˆìš©ì–´(ë°ëª¨ìš©, í•„ìš”ì— ë§ê²Œ í™•ì¥)                            \n",
    "    \"is\", \"a\", \"the\", \"this\", \"to\", \"of\", \"and\", \"in\", \"on\", \"for\",  # ì˜ì–´ ë¶ˆìš©ì–´ \n",
    "    \"ì€\", \"ëŠ”\", \"ì´\", \"ê°€\", \"ì„\", \"ë¥¼\", \"ì—\", \"ì˜\", \"ì™€\", \"ê³¼\", \"ë„\"  # í•œê¸€ ì¡°ì‚¬ ì¼ë¶€ \n",
    "}  # ë¶ˆìš©ì–´ ì§‘í•© ë                                                        \n",
    "\n",
    "def remove_stopwords(tokens: list[str]) -> list[str]:  # ë¶ˆìš©ì–´ ì œê±° í•¨ìˆ˜     \n",
    "    return [t for t in tokens if t not in STOPWORDS]  # ë¶ˆìš©ì–´ ì œì™¸          \n",
    "\n",
    "def stem_simple(token: str) -> str:  # ì•„ì£¼ ë‹¨ìˆœí•œ ì–´ê°„ ì¶”ì¶œ(ì˜ì–´ ìœ„ì£¼ ë°ëª¨)  \n",
    "    t = token  # ì‘ì—…ìš© ë³µì‚¬                                                \n",
    "    for suf in (\"ing\", \"ed\", \"es\", \"s\"):  # í”í•œ ì ‘ë¯¸ì‚¬ ëª©ë¡                  \n",
    "        if len(t) > 4 and t.endswith(suf):  # ë„ˆë¬´ ì§§ì€ ë‹¨ì–´ëŠ” ë³´í˜¸            \n",
    "            t = t[: -len(suf)]  # ì ‘ë¯¸ì‚¬ ì œê±°                                \n",
    "            break  # í•˜ë‚˜ë§Œ ì œê±°í•˜ê³  ì¢…ë£Œ                                   \n",
    "    return t  # ìŠ¤í…œ ê²°ê³¼ ë°˜í™˜                                              \n",
    "\n",
    "LEMMA_MAP = {  # í‘œì œì–´ ì¶”ì¶œ(ê°„ë‹¨ ë§¤í•‘ ë°ëª¨: ì‹¤ì œë¡  ì‚¬ì „/ëª¨ë¸ í•„ìš”)           \n",
    "    \"mice\": \"mouse\",  # ì˜ˆì‹œ ë§¤í•‘                                          \n",
    "    \"children\": \"child\",  # ì˜ˆì‹œ ë§¤í•‘                                      \n",
    "    \"went\": \"go\",  # ì˜ˆì‹œ ë§¤í•‘                                              \n",
    "}  # ë§¤í•‘ ë                                                                \n",
    "\n",
    "def lemmatize_simple(token: str) -> str:  # ì•„ì£¼ ë‹¨ìˆœí•œ í‘œì œì–´ ì¶”ì¶œ(ë°ëª¨)     \n",
    "    return LEMMA_MAP.get(token, token)  # ë§¤í•‘ ìˆìœ¼ë©´ ì¹˜í™˜, ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ     \n",
    "\n",
    "def build_vocab(token_sequences: list[list[str]], min_freq: int = 1) -> dict[str, int]:  # ì–´íœ˜ì§‘í•© ìƒì„±\n",
    "    counts = Counter(t for seq in token_sequences for t in seq)  # ì „ì²´ í† í° ë¹ˆë„ ì§‘ê³„ \n",
    "    vocab = {\"[PAD]\": 0, \"[UNK]\": 1}  # íŠ¹ìˆ˜ í† í°(íŒ¨ë”©/ë¯¸ë“±ë¡) ê¸°ë³¸ ë“±ë¡         \n",
    "    for tok, c in counts.most_common():  # ë¹ˆë„ ë†’ì€ ìˆœìœ¼ë¡œ ìˆœíšŒ               \n",
    "        if c >= min_freq and tok not in vocab:  # ìµœì†Œ ë¹ˆë„ ì´ìƒë§Œ              \n",
    "            vocab[tok] = len(vocab)  # ìƒˆ ID ë¶€ì—¬                            \n",
    "    return vocab  # vocab(í† í°â†’id) ë°˜í™˜                                     \n",
    "\n",
    "def integer_encode(tokens: list[str], vocab: dict[str, int]) -> list[int]:  # ì •ìˆ˜ ì¸ì½”ë”©\n",
    "    return [vocab.get(t, vocab[\"[UNK]\"]) for t in tokens]  # ëª¨ë¥´ë©´ UNK       \n",
    "\n",
    "def pad_and_mask(ids: list[int], max_len: int, pad_id: int = 0) -> tuple[list[int], list[int]]:  # íŒ¨ë”©/ë§ˆìŠ¤í‚¹\n",
    "    ids = ids[:max_len]  # ê¸¸ë©´ ìë¥´ê¸°                                      \n",
    "    mask = [1] * len(ids)  # ì‹¤ì œ í† í°ì€ 1                                   \n",
    "    while len(ids) < max_len:  # ì§§ìœ¼ë©´ íŒ¨ë”© ì¶”ê°€                             \n",
    "        ids.append(pad_id)  # PAD ì¶”ê°€                                       \n",
    "        mask.append(0)  # PAD ìœ„ì¹˜ëŠ” 0                                       \n",
    "    return ids, mask  # (íŒ¨ë”©ëœ ids, ë§ˆìŠ¤í¬) ë°˜í™˜                            \n",
    "\n",
    "# --------- â€œí†µìœ¼ë¡œâ€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì˜ˆì‹œ ---------  # êµ¬ë¶„ì„                  \n",
    "norm = normalize(TEXT)  # 1) ì •ê·œí™”                                         \n",
    "sents = split_sentences(norm)  # 2) ë¬¸ì¥ ë¶„ë¦¬                               \n",
    "\n",
    "word_tokens = [tokenize_word(s) for s in sents]  # 3) ì›Œë“œ í† í°í™”(ë¬¸ì¥ë³„)      \n",
    "word_tokens = [remove_stopwords(ws) for ws in word_tokens]  # 4) ë¶ˆìš©ì–´ ì œê±°(ë¬¸ì¥ë³„)\n",
    "\n",
    "stemmed = [[stem_simple(t) for t in ws] for ws in word_tokens]  # 5) ì–´ê°„ ì¶”ì¶œ(ë¬¸ì¥ë³„)\n",
    "lemmatized = [[lemmatize_simple(t) for t in ws] for ws in word_tokens]  # 6) í‘œì œì–´(ë¬¸ì¥ë³„)\n",
    "\n",
    "subword_tokens = [tokenize_subword_simple(ws) for ws in word_tokens]  # 7) ì„œë¸Œì›Œë“œ í† í°í™”(ë°ëª¨)\n",
    "char_tokens = [tokenize_char(s) for s in sents]  # 8) ìºë¦­í„° í† í°í™”(ë¬¸ì¥ë³„)    \n",
    "\n",
    "vocab = build_vocab(subword_tokens, min_freq=1)  # 9) ì–´íœ˜ì§‘í•©(vocab) ìƒì„±(ì—¬ê¸°ì„  ì„œë¸Œì›Œë“œ ê¸°ì¤€)\n",
    "encoded = [integer_encode(seq, vocab) for seq in subword_tokens]  # 10) ì •ìˆ˜ ì¸ì½”ë”©(ë¬¸ì¥ë³„)\n",
    "\n",
    "max_len = 12  # 11) íŒ¨ë”© ê¸°ì¤€ ê¸¸ì´                                          \n",
    "padded_and_masked = [pad_and_mask(seq, max_len, pad_id=vocab[\"[PAD]\"]) for seq in encoded]  # 12) íŒ¨ë”©/ë§ˆìŠ¤í‚¹\n",
    "\n",
    "print(\"ì •ê·œí™”:\", norm)  # ê²°ê³¼ ì¶œë ¥                                         \n",
    "print(\"ë¬¸ì¥:\", sents)  # ê²°ê³¼ ì¶œë ¥                                          \n",
    "print(\"ì›Œë“œ:\", word_tokens)  # ê²°ê³¼ ì¶œë ¥                                    \n",
    "print(\"ì–´ê°„:\", stemmed)  # ê²°ê³¼ ì¶œë ¥                                        \n",
    "print(\"í‘œì œì–´:\", lemmatized)  # ê²°ê³¼ ì¶œë ¥                                   \n",
    "print(\"ì„œë¸Œì›Œë“œ(ë°ëª¨):\", subword_tokens)  # ê²°ê³¼ ì¶œë ¥                        \n",
    "print(\"vocab size:\", len(vocab))  # ê²°ê³¼ ì¶œë ¥                                \n",
    "print(\"ì¸ì½”ë”©:\", encoded)  # ê²°ê³¼ ì¶œë ¥                                      \n",
    "print(\"íŒ¨ë”©/ë§ˆìŠ¤í¬:\", padded_and_masked)  # ê²°ê³¼ ì¶œë ¥                        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
