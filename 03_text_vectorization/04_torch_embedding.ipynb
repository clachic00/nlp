{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12cd7689",
   "metadata": {},
   "source": [
    "# torch `nn.Embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0ae01602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Playdata/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Playdata/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Playdata/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468c0c85",
   "metadata": {},
   "source": [
    "## 사전학습된 임베딩을 사용하지 않는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8d7c8092",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [          \n",
    "    'nice great best amazing',  # 긍정 문장 예시\n",
    "    'stop lies',                # 부정/비판 문장 예시\n",
    "    'pitiful nerd',             # 부정 문장 예시\n",
    "    'excellent work',           # 긍정 문장 예시\n",
    "    'supreme quality',          # 긍정 문장 예시\n",
    "    'bad',                      # 부정 문장 예시\n",
    "    'highly respectable'        # 긍정 문장 예시\n",
    "]                               # 분류 모델에 넣을 입력 문장 리스트(list[str])\n",
    "labels = [1, 0, 0, 1, 1, 0, 1]  # 각 문장에 대한 이진 라벨(1=긍정, 0=부정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4a4f3684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nice', 'great', 'best', 'amazing'],\n",
       " ['stop', 'lies'],\n",
       " ['pitiful', 'nerd'],\n",
       " ['excellent', 'work'],\n",
       " ['supreme', 'quality'],\n",
       " ['bad'],\n",
       " ['highly', 'respectable']]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰화\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_sentences = [word_tokenize(sent) for sent in sentences]    # 각 문장을 토큰 리스트(list(list[str]))로 변환\n",
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5bc1ce77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'nice': 1, 'great': 1, 'best': 1, 'amazing': 1, 'stop': 1, 'lies': 1, 'pitiful': 1, 'nerd': 1, 'excellent': 1, 'work': 1, 'supreme': 1, 'quality': 1, 'bad': 1, 'highly': 1, 'respectable': 1})\n",
      "{'<PAD>': 0, '<UNK>': 1, 'nice': 2, 'great': 3, 'best': 4, 'amazing': 5, 'stop': 6, 'lies': 7, 'pitiful': 8, 'nerd': 9, 'excellent': 10, 'work': 11, 'supreme': 12, 'quality': 13, 'bad': 14, 'highly': 15, 'respectable': 16}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 사전 생성 + 정수 인코딩\n",
    "from collections import Counter\n",
    "\n",
    "tokens = [token for sent in tokenized_sentences for token in sent]  # 문장 리스트를 평탄화하여 전체 토큰 리스트 생성\n",
    "word_counts = Counter(tokens)  # 전체 토큰 등장 빈도 계산\n",
    "print(word_counts)  # 토큰별 빈도 딕셔너리 형태\n",
    "\n",
    "word_to_index = {word: index + 2 for index, word in enumerate(tokens)}  # 토큰을 순서대로 인덱싱(+2 : 특수토큰용)\n",
    "word_to_index['<PAD>'] = 0    # 패딩 토큰 (길이 맞추기용)\n",
    "word_to_index['<UNK>'] = 1    # OOV 토큰 (처리 불가 단어 대체)\n",
    "word_to_index = dict(sorted(word_to_index.items(), key=lambda x: x[1]))  # 인덱스를 기준으로 정렬\n",
    "print(word_to_index)  # 단어 -> 인덱스 사전\n",
    "\n",
    "vocab_size = len(word_to_index)  # 전체 어휘 수 (특수토큰 포함)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e5e4e32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14], [15, 16]]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정수 인코딩 함수 : 토큰화된 문장 리스트를 단어 -> 인덱스 사전으로 정수 시퀀스 (list[list(int)])로 변환\n",
    "def texts_to_sequences(sentences, word_to_index):\n",
    "    sequences = []\n",
    "\n",
    "    for sent in sentences:  # 문장 단위로 반복\n",
    "        sequence = []\n",
    "\n",
    "        for token in sent:\n",
    "            if token in word_to_index:\n",
    "                sequence.append(word_to_index[token])  # 해당 단어 인덱스 추가\n",
    "            else:\n",
    "                sequence.append(word_to_index['<UNK>'])  # 사전에 없으면 UNK 토큰\n",
    "        \n",
    "        sequences.append(sequence)\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "sequences = texts_to_sequences(tokenized_sentences, word_to_index)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "33b427e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  3,  4,  5],\n",
       "       [ 6,  7,  0,  0],\n",
       "       [ 8,  9,  0,  0],\n",
       "       [10, 11,  0,  0],\n",
       "       [12, 13,  0,  0],\n",
       "       [14,  0,  0,  0],\n",
       "       [15, 16,  0,  0]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 서로 다른 길이의 정수 시퀀스를 0(<PAD>)으로 채우거나 잘라내 (문장수, maxlen) 형태에 맞춰주는 함수\n",
    "def pad_sequences(sequences, maxlen):\n",
    "    padded_sequences = np.zeros((len(sequences), maxlen), dtype=int)  # (문장수 x maxlen) 크기의 0 패딩 배열\n",
    "    \n",
    "    for index, seq in enumerate(sequences):  # 각 문장 시퀀스 순회\n",
    "        padded_sequences[index, :len(seq)] = seq[:maxlen]  # 앞에서부터 시퀀스 채운다. 길면 maxlen까지만 채워 자른다.\n",
    "    \n",
    "    return padded_sequences  # 패딩 작업 완료된 2D 배열\n",
    "\n",
    "padded_sequences = pad_sequences(sequences, maxlen=4)  # 모든 문장 길이 4로 패딩/자르기\n",
    "padded_sequences  # (문장 수, 4) 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a6cd7bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 4)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "93751631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNet(\n",
       "  (embedding): Embedding(17, 100, padding_idx=0)\n",
       "  (rnn): RNN(100, 16, batch_first=True)\n",
       "  (out): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pytorch 텍스트 분류 모델 : Embedding + RNN + Linear로 이진 분류(logit) 출력\n",
    "import torch\n",
    "import torch.nn as nn           # 신경망 레이어\n",
    "import torch.optim as optim     # 옵티마이저(활성화함수)\n",
    "from torch.utils.data import DataLoader, TensorDataset  # 배치 로더 / 데이터셋 유틸\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    # 정수 시퀀스를 임베딩 -> RNN -> 선형층으로 처리해 이진 분류 logit(1개)를 출력\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super().__init__()                    # nn.Module 초기화\n",
    "        self.embedding = nn.Embedding(        # 단어 ID를 밀집 벡터로 변환하는 임베딩 층\n",
    "            num_embeddings = vocab_size,      # 단어 사전 크기 (어휘 수)\n",
    "            embedding_dim = embedding_dim,    # 임베딩 차원\n",
    "            padding_idx = 0                   # PAD(0) 인덱스는 0 그대로 사용\n",
    "        )\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)    # 입력(배치, 길이, 차원) 형태의 RNN\n",
    "        self.out = nn.Linear(hidden_size, 1)     # 마지막 은닉 상태를 1차원 logit으로 변환\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)     # (batch, seq_len) -> (batch, seq_len, embedding_dim)\n",
    "        out, h_n = self.rnn(embedded)    # h_n: (num_layers*directions, batch, hidden_size)\n",
    "        out = self.out(h_n.squeeze(0))   # (batch_size, hidden_size) -> (batch, 1)\n",
    "        return out  # 출력 : 시그모이드 전 logit(확률이 아님)\n",
    "    \n",
    "embedding_dim = 100  # 단어 벡터 차원 설정\n",
    "model = SimpleNet(vocab_size, embedding_dim, hidden_size=16)  # 어휘 크기 / 임베딩 차원/ 은닉크기로 모델 생성\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7f45bf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (1.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b2e27f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "SimpleNet                                --\n",
       "├─Embedding: 1-1                         1,700\n",
       "├─RNN: 1-2                               1,888\n",
       "├─Linear: 1-3                            17\n",
       "=================================================================\n",
       "Total params: 3,605\n",
       "Trainable params: 3,605\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary  # 모델 구조를 표 형태로 요약\n",
    "\n",
    "summary(model)  # model의 레이어 구성 / 파라미터 수를 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3a284e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17, 100])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;PAD&gt;</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;UNK&gt;</th>\n",
       "      <td>0.067155</td>\n",
       "      <td>0.450458</td>\n",
       "      <td>1.197854</td>\n",
       "      <td>-1.593690</td>\n",
       "      <td>0.261582</td>\n",
       "      <td>-0.589503</td>\n",
       "      <td>-0.752261</td>\n",
       "      <td>2.182404</td>\n",
       "      <td>2.608035</td>\n",
       "      <td>1.115606</td>\n",
       "      <td>...</td>\n",
       "      <td>1.127545</td>\n",
       "      <td>-0.862246</td>\n",
       "      <td>-0.218617</td>\n",
       "      <td>1.591112</td>\n",
       "      <td>-1.003133</td>\n",
       "      <td>-0.399965</td>\n",
       "      <td>0.870370</td>\n",
       "      <td>0.658544</td>\n",
       "      <td>-2.109090</td>\n",
       "      <td>-0.630291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nice</th>\n",
       "      <td>-0.961702</td>\n",
       "      <td>0.222218</td>\n",
       "      <td>-0.243321</td>\n",
       "      <td>-0.836248</td>\n",
       "      <td>-1.297869</td>\n",
       "      <td>-0.161578</td>\n",
       "      <td>0.085727</td>\n",
       "      <td>-1.050393</td>\n",
       "      <td>0.948098</td>\n",
       "      <td>-0.696058</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.104610</td>\n",
       "      <td>-0.752611</td>\n",
       "      <td>-0.158800</td>\n",
       "      <td>0.496218</td>\n",
       "      <td>-0.555352</td>\n",
       "      <td>0.857601</td>\n",
       "      <td>-1.245629</td>\n",
       "      <td>-1.056859</td>\n",
       "      <td>1.768178</td>\n",
       "      <td>-1.671158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>-0.164069</td>\n",
       "      <td>0.590218</td>\n",
       "      <td>-0.240765</td>\n",
       "      <td>1.974253</td>\n",
       "      <td>0.461243</td>\n",
       "      <td>-0.856731</td>\n",
       "      <td>-1.351909</td>\n",
       "      <td>-0.218527</td>\n",
       "      <td>0.181933</td>\n",
       "      <td>-0.335685</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.301831</td>\n",
       "      <td>-0.357120</td>\n",
       "      <td>-0.135636</td>\n",
       "      <td>-0.329625</td>\n",
       "      <td>0.019454</td>\n",
       "      <td>-1.991897</td>\n",
       "      <td>1.797811</td>\n",
       "      <td>-2.292777</td>\n",
       "      <td>-1.222405</td>\n",
       "      <td>-1.054984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>0.574102</td>\n",
       "      <td>-1.687522</td>\n",
       "      <td>-0.559748</td>\n",
       "      <td>0.009251</td>\n",
       "      <td>1.022382</td>\n",
       "      <td>1.940633</td>\n",
       "      <td>0.743564</td>\n",
       "      <td>-0.636236</td>\n",
       "      <td>-0.726370</td>\n",
       "      <td>-0.366768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.493886</td>\n",
       "      <td>-0.048740</td>\n",
       "      <td>1.090409</td>\n",
       "      <td>-0.540918</td>\n",
       "      <td>0.007862</td>\n",
       "      <td>-0.658797</td>\n",
       "      <td>-1.218989</td>\n",
       "      <td>1.711116</td>\n",
       "      <td>0.087020</td>\n",
       "      <td>-0.461664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazing</th>\n",
       "      <td>-0.226293</td>\n",
       "      <td>0.105611</td>\n",
       "      <td>0.224042</td>\n",
       "      <td>-0.531338</td>\n",
       "      <td>-0.901778</td>\n",
       "      <td>-2.720743</td>\n",
       "      <td>-1.007496</td>\n",
       "      <td>0.216615</td>\n",
       "      <td>0.145610</td>\n",
       "      <td>0.594590</td>\n",
       "      <td>...</td>\n",
       "      <td>1.547327</td>\n",
       "      <td>-0.000321</td>\n",
       "      <td>0.522299</td>\n",
       "      <td>0.294366</td>\n",
       "      <td>-0.707502</td>\n",
       "      <td>0.336025</td>\n",
       "      <td>0.448750</td>\n",
       "      <td>0.022717</td>\n",
       "      <td>2.302627</td>\n",
       "      <td>-1.019629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop</th>\n",
       "      <td>-0.379589</td>\n",
       "      <td>1.086324</td>\n",
       "      <td>0.604756</td>\n",
       "      <td>0.868796</td>\n",
       "      <td>0.470319</td>\n",
       "      <td>-0.574214</td>\n",
       "      <td>-0.435753</td>\n",
       "      <td>-0.199041</td>\n",
       "      <td>1.171326</td>\n",
       "      <td>0.016524</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014758</td>\n",
       "      <td>-0.832758</td>\n",
       "      <td>-0.097466</td>\n",
       "      <td>-0.705530</td>\n",
       "      <td>0.246519</td>\n",
       "      <td>0.672909</td>\n",
       "      <td>-0.166925</td>\n",
       "      <td>-0.463790</td>\n",
       "      <td>0.901840</td>\n",
       "      <td>-0.662427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lies</th>\n",
       "      <td>-0.977556</td>\n",
       "      <td>-0.248007</td>\n",
       "      <td>0.090782</td>\n",
       "      <td>-0.292039</td>\n",
       "      <td>0.379848</td>\n",
       "      <td>-1.037655</td>\n",
       "      <td>-1.575040</td>\n",
       "      <td>3.082653</td>\n",
       "      <td>1.051682</td>\n",
       "      <td>-0.379660</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.785943</td>\n",
       "      <td>-1.232038</td>\n",
       "      <td>0.067811</td>\n",
       "      <td>1.020917</td>\n",
       "      <td>-0.836405</td>\n",
       "      <td>1.034436</td>\n",
       "      <td>0.101547</td>\n",
       "      <td>1.130855</td>\n",
       "      <td>-2.178795</td>\n",
       "      <td>1.563543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pitiful</th>\n",
       "      <td>-0.570348</td>\n",
       "      <td>0.241022</td>\n",
       "      <td>-0.672121</td>\n",
       "      <td>-0.916047</td>\n",
       "      <td>-1.097100</td>\n",
       "      <td>0.204070</td>\n",
       "      <td>-0.357396</td>\n",
       "      <td>-1.320163</td>\n",
       "      <td>0.658594</td>\n",
       "      <td>-0.388395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.688965</td>\n",
       "      <td>1.634827</td>\n",
       "      <td>-0.677148</td>\n",
       "      <td>0.342986</td>\n",
       "      <td>0.830902</td>\n",
       "      <td>0.695931</td>\n",
       "      <td>-0.724482</td>\n",
       "      <td>-0.698650</td>\n",
       "      <td>-0.304619</td>\n",
       "      <td>2.219257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nerd</th>\n",
       "      <td>0.156682</td>\n",
       "      <td>1.875443</td>\n",
       "      <td>-0.698519</td>\n",
       "      <td>0.132605</td>\n",
       "      <td>0.921071</td>\n",
       "      <td>-1.285293</td>\n",
       "      <td>-0.713844</td>\n",
       "      <td>1.286650</td>\n",
       "      <td>0.374842</td>\n",
       "      <td>-0.189742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.412071</td>\n",
       "      <td>1.299997</td>\n",
       "      <td>1.699270</td>\n",
       "      <td>2.645861</td>\n",
       "      <td>-1.475867</td>\n",
       "      <td>0.150954</td>\n",
       "      <td>-0.909161</td>\n",
       "      <td>0.319953</td>\n",
       "      <td>-0.615504</td>\n",
       "      <td>0.705211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excellent</th>\n",
       "      <td>-0.888726</td>\n",
       "      <td>0.449490</td>\n",
       "      <td>1.110799</td>\n",
       "      <td>-0.151094</td>\n",
       "      <td>1.569268</td>\n",
       "      <td>0.872407</td>\n",
       "      <td>-0.601829</td>\n",
       "      <td>1.637835</td>\n",
       "      <td>0.062063</td>\n",
       "      <td>-0.386323</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.221736</td>\n",
       "      <td>-1.512219</td>\n",
       "      <td>1.017177</td>\n",
       "      <td>1.216765</td>\n",
       "      <td>1.258354</td>\n",
       "      <td>-0.854458</td>\n",
       "      <td>1.749661</td>\n",
       "      <td>0.076489</td>\n",
       "      <td>-1.552990</td>\n",
       "      <td>0.389118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>-1.027474</td>\n",
       "      <td>0.918590</td>\n",
       "      <td>-1.105216</td>\n",
       "      <td>2.320172</td>\n",
       "      <td>-0.423239</td>\n",
       "      <td>0.757634</td>\n",
       "      <td>-1.684840</td>\n",
       "      <td>1.922731</td>\n",
       "      <td>-1.244830</td>\n",
       "      <td>-1.090028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244394</td>\n",
       "      <td>1.112008</td>\n",
       "      <td>0.160857</td>\n",
       "      <td>0.470281</td>\n",
       "      <td>0.440615</td>\n",
       "      <td>-0.564859</td>\n",
       "      <td>-0.218933</td>\n",
       "      <td>-1.449970</td>\n",
       "      <td>1.357830</td>\n",
       "      <td>1.170986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>supreme</th>\n",
       "      <td>0.636458</td>\n",
       "      <td>0.488970</td>\n",
       "      <td>-1.602001</td>\n",
       "      <td>-1.056778</td>\n",
       "      <td>-0.528976</td>\n",
       "      <td>1.444173</td>\n",
       "      <td>0.090918</td>\n",
       "      <td>-2.166413</td>\n",
       "      <td>-1.640407</td>\n",
       "      <td>-0.326680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089671</td>\n",
       "      <td>0.808536</td>\n",
       "      <td>-0.937243</td>\n",
       "      <td>1.116811</td>\n",
       "      <td>0.009757</td>\n",
       "      <td>-0.071927</td>\n",
       "      <td>2.253617</td>\n",
       "      <td>0.645218</td>\n",
       "      <td>0.634546</td>\n",
       "      <td>-0.305573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <td>-0.260078</td>\n",
       "      <td>0.627458</td>\n",
       "      <td>1.161235</td>\n",
       "      <td>0.538225</td>\n",
       "      <td>-0.451434</td>\n",
       "      <td>-1.842133</td>\n",
       "      <td>0.649467</td>\n",
       "      <td>-0.079331</td>\n",
       "      <td>0.173815</td>\n",
       "      <td>-3.237617</td>\n",
       "      <td>...</td>\n",
       "      <td>1.702684</td>\n",
       "      <td>-0.987450</td>\n",
       "      <td>-1.291052</td>\n",
       "      <td>-0.452304</td>\n",
       "      <td>0.022972</td>\n",
       "      <td>-0.362454</td>\n",
       "      <td>0.547394</td>\n",
       "      <td>-0.438130</td>\n",
       "      <td>0.771480</td>\n",
       "      <td>0.425637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>1.035124</td>\n",
       "      <td>-1.443182</td>\n",
       "      <td>-1.943558</td>\n",
       "      <td>0.569999</td>\n",
       "      <td>0.948551</td>\n",
       "      <td>-0.251659</td>\n",
       "      <td>0.715463</td>\n",
       "      <td>-0.260863</td>\n",
       "      <td>-0.708279</td>\n",
       "      <td>2.685955</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.436023</td>\n",
       "      <td>-0.214262</td>\n",
       "      <td>-0.039624</td>\n",
       "      <td>1.712429</td>\n",
       "      <td>-0.068358</td>\n",
       "      <td>1.615396</td>\n",
       "      <td>0.055955</td>\n",
       "      <td>-1.906902</td>\n",
       "      <td>-1.039114</td>\n",
       "      <td>-0.611574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>highly</th>\n",
       "      <td>0.575649</td>\n",
       "      <td>-0.247651</td>\n",
       "      <td>-0.659791</td>\n",
       "      <td>-2.553438</td>\n",
       "      <td>0.377293</td>\n",
       "      <td>0.452699</td>\n",
       "      <td>0.526918</td>\n",
       "      <td>-0.753010</td>\n",
       "      <td>-1.365008</td>\n",
       "      <td>-1.030152</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.925240</td>\n",
       "      <td>-1.556715</td>\n",
       "      <td>-1.057523</td>\n",
       "      <td>0.697230</td>\n",
       "      <td>0.655193</td>\n",
       "      <td>0.461879</td>\n",
       "      <td>-0.144606</td>\n",
       "      <td>0.287868</td>\n",
       "      <td>-1.445977</td>\n",
       "      <td>-1.585508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>respectable</th>\n",
       "      <td>0.409624</td>\n",
       "      <td>-1.293193</td>\n",
       "      <td>-0.703747</td>\n",
       "      <td>-1.021072</td>\n",
       "      <td>-0.348470</td>\n",
       "      <td>0.159726</td>\n",
       "      <td>0.472963</td>\n",
       "      <td>-0.980147</td>\n",
       "      <td>0.578435</td>\n",
       "      <td>0.673086</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.875971</td>\n",
       "      <td>-1.421256</td>\n",
       "      <td>-0.762325</td>\n",
       "      <td>0.492543</td>\n",
       "      <td>-0.877740</td>\n",
       "      <td>0.686262</td>\n",
       "      <td>-0.478210</td>\n",
       "      <td>-0.347625</td>\n",
       "      <td>-0.850844</td>\n",
       "      <td>-1.083814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4         5   \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "<UNK>        0.067155  0.450458  1.197854 -1.593690  0.261582 -0.589503   \n",
       "nice        -0.961702  0.222218 -0.243321 -0.836248 -1.297869 -0.161578   \n",
       "great       -0.164069  0.590218 -0.240765  1.974253  0.461243 -0.856731   \n",
       "best         0.574102 -1.687522 -0.559748  0.009251  1.022382  1.940633   \n",
       "amazing     -0.226293  0.105611  0.224042 -0.531338 -0.901778 -2.720743   \n",
       "stop        -0.379589  1.086324  0.604756  0.868796  0.470319 -0.574214   \n",
       "lies        -0.977556 -0.248007  0.090782 -0.292039  0.379848 -1.037655   \n",
       "pitiful     -0.570348  0.241022 -0.672121 -0.916047 -1.097100  0.204070   \n",
       "nerd         0.156682  1.875443 -0.698519  0.132605  0.921071 -1.285293   \n",
       "excellent   -0.888726  0.449490  1.110799 -0.151094  1.569268  0.872407   \n",
       "work        -1.027474  0.918590 -1.105216  2.320172 -0.423239  0.757634   \n",
       "supreme      0.636458  0.488970 -1.602001 -1.056778 -0.528976  1.444173   \n",
       "quality     -0.260078  0.627458  1.161235  0.538225 -0.451434 -1.842133   \n",
       "bad          1.035124 -1.443182 -1.943558  0.569999  0.948551 -0.251659   \n",
       "highly       0.575649 -0.247651 -0.659791 -2.553438  0.377293  0.452699   \n",
       "respectable  0.409624 -1.293193 -0.703747 -1.021072 -0.348470  0.159726   \n",
       "\n",
       "                   6         7         8         9   ...        90        91  \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "<UNK>       -0.752261  2.182404  2.608035  1.115606  ...  1.127545 -0.862246   \n",
       "nice         0.085727 -1.050393  0.948098 -0.696058  ... -1.104610 -0.752611   \n",
       "great       -1.351909 -0.218527  0.181933 -0.335685  ... -0.301831 -0.357120   \n",
       "best         0.743564 -0.636236 -0.726370 -0.366768  ...  0.493886 -0.048740   \n",
       "amazing     -1.007496  0.216615  0.145610  0.594590  ...  1.547327 -0.000321   \n",
       "stop        -0.435753 -0.199041  1.171326  0.016524  ... -0.014758 -0.832758   \n",
       "lies        -1.575040  3.082653  1.051682 -0.379660  ... -0.785943 -1.232038   \n",
       "pitiful     -0.357396 -1.320163  0.658594 -0.388395  ...  0.688965  1.634827   \n",
       "nerd        -0.713844  1.286650  0.374842 -0.189742  ...  0.412071  1.299997   \n",
       "excellent   -0.601829  1.637835  0.062063 -0.386323  ... -1.221736 -1.512219   \n",
       "work        -1.684840  1.922731 -1.244830 -1.090028  ...  0.244394  1.112008   \n",
       "supreme      0.090918 -2.166413 -1.640407 -0.326680  ...  0.089671  0.808536   \n",
       "quality      0.649467 -0.079331  0.173815 -3.237617  ...  1.702684 -0.987450   \n",
       "bad          0.715463 -0.260863 -0.708279  2.685955  ... -1.436023 -0.214262   \n",
       "highly       0.526918 -0.753010 -1.365008 -1.030152  ... -0.925240 -1.556715   \n",
       "respectable  0.472963 -0.980147  0.578435  0.673086  ... -0.875971 -1.421256   \n",
       "\n",
       "                   92        93        94        95        96        97  \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "<UNK>       -0.218617  1.591112 -1.003133 -0.399965  0.870370  0.658544   \n",
       "nice        -0.158800  0.496218 -0.555352  0.857601 -1.245629 -1.056859   \n",
       "great       -0.135636 -0.329625  0.019454 -1.991897  1.797811 -2.292777   \n",
       "best         1.090409 -0.540918  0.007862 -0.658797 -1.218989  1.711116   \n",
       "amazing      0.522299  0.294366 -0.707502  0.336025  0.448750  0.022717   \n",
       "stop        -0.097466 -0.705530  0.246519  0.672909 -0.166925 -0.463790   \n",
       "lies         0.067811  1.020917 -0.836405  1.034436  0.101547  1.130855   \n",
       "pitiful     -0.677148  0.342986  0.830902  0.695931 -0.724482 -0.698650   \n",
       "nerd         1.699270  2.645861 -1.475867  0.150954 -0.909161  0.319953   \n",
       "excellent    1.017177  1.216765  1.258354 -0.854458  1.749661  0.076489   \n",
       "work         0.160857  0.470281  0.440615 -0.564859 -0.218933 -1.449970   \n",
       "supreme     -0.937243  1.116811  0.009757 -0.071927  2.253617  0.645218   \n",
       "quality     -1.291052 -0.452304  0.022972 -0.362454  0.547394 -0.438130   \n",
       "bad         -0.039624  1.712429 -0.068358  1.615396  0.055955 -1.906902   \n",
       "highly      -1.057523  0.697230  0.655193  0.461879 -0.144606  0.287868   \n",
       "respectable -0.762325  0.492543 -0.877740  0.686262 -0.478210 -0.347625   \n",
       "\n",
       "                   98        99  \n",
       "<PAD>        0.000000  0.000000  \n",
       "<UNK>       -2.109090 -0.630291  \n",
       "nice         1.768178 -1.671158  \n",
       "great       -1.222405 -1.054984  \n",
       "best         0.087020 -0.461664  \n",
       "amazing      2.302627 -1.019629  \n",
       "stop         0.901840 -0.662427  \n",
       "lies        -2.178795  1.563543  \n",
       "pitiful     -0.304619  2.219257  \n",
       "nerd        -0.615504  0.705211  \n",
       "excellent   -1.552990  0.389118  \n",
       "work         1.357830  1.170986  \n",
       "supreme      0.634546 -0.305573  \n",
       "quality      0.771480  0.425637  \n",
       "bad         -1.039114 -0.611574  \n",
       "highly      -1.445977 -1.585508  \n",
       "respectable -0.850844 -1.083814  \n",
       "\n",
       "[17 rows x 100 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 임베딩 가중치 확인 : 학습 전/후 Embedding 테이블과 단어별 벡터 조회\n",
    "import pandas as pd\n",
    "\n",
    "# 학습 전 임베딩 벡터\n",
    "wv = model.embedding.weight.data  # Embedding 층의 가중치 행렬(단어ID x 임베딩 차원) 추출\n",
    "print(wv.shape)  # (vocab_size, embedding_dim)\n",
    "\n",
    "# 특정 단어 벡터\n",
    "vocab = word_to_index.keys()    # 단어사전에서 단어만 뽑아온다.\n",
    "pd.DataFrame(wv, index=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e3fde5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch 학습 준비 : 텐서 변환 -> DataLoader 구성 -> 손실함수/옵티마이저 설정\n",
    "X = torch.tensor(padded_sequences, dtype=torch.long)      # 입력 시퀀스(정수 ID)를 LongTensor로 변환\n",
    "y = torch.tensor(labels, dtype=torch.float).unsqueeze(1)  # 라벨을 float으로 변환 후 (N,) -> (N, 1)로 차원 맞춤\n",
    "\n",
    "dataset = TensorDataset(X, y)  # (X, y) 쌍을 Dataset 객체로 묶음\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)  # 배치 단위로 섞어서 공급하는 로더\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # 출력 logit과 정답(0/1)로 이진분류 손실 계산 (시그모이드 포함)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)  # 모델 파라미터는 Adam으로 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090fffa7",
   "metadata": {},
   "source": [
    "BCEWithLogitsLoss을 사용할 때에는 모델 출력이 Sigmoid를 거치지 않은 logit이어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1374a80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss 0.9976607114076614\n",
      "Epoch 2: Loss 0.7189708054065704\n",
      "Epoch 3: Loss 0.598182812333107\n",
      "Epoch 4: Loss 0.49509794265031815\n",
      "Epoch 5: Loss 0.4592989534139633\n",
      "Epoch 6: Loss 0.39057567715644836\n",
      "Epoch 7: Loss 0.3160467892885208\n",
      "Epoch 8: Loss 0.24247897416353226\n",
      "Epoch 9: Loss 0.18785091117024422\n",
      "Epoch 10: Loss 0.14763448759913445\n",
      "Epoch 11: Loss 0.11639018729329109\n",
      "Epoch 12: Loss 0.09219682775437832\n",
      "Epoch 13: Loss 0.07494872622191906\n",
      "Epoch 14: Loss 0.062301814556121826\n",
      "Epoch 15: Loss 0.05120605602860451\n",
      "Epoch 16: Loss 0.0448520565405488\n",
      "Epoch 17: Loss 0.03867247514426708\n",
      "Epoch 18: Loss 0.03508199658244848\n",
      "Epoch 19: Loss 0.031099062878638506\n",
      "Epoch 20: Loss 0.027309920638799667\n"
     ]
    }
   ],
   "source": [
    "# 학습 루프 : 미니배치 단위로 20 epoch 학습하며 평균 손실 출력\n",
    "for epoch in range(20):\n",
    "    epoch_loss = 0    # 손실 누적\n",
    "\n",
    "    for x_batch, y_batch in dataloader:    # 미니배치 단위로 (X, y) 가져오기\n",
    "        optimizer.zero_grad()              # 이전 배치 기울기 초기화\n",
    "        output = model(x_batch)            # 순전파로 logit 계산\n",
    "        loss = criterion(output, y_batch)  # 예측 logit과 정답으로 손실 계산\n",
    "        loss.backward()                    # 역전파로 기울기 계산\n",
    "        optimizer.step()                   # 파라미터 업데이트\n",
    "\n",
    "        epoch_loss += loss.item()  # 배치손실을 float으로 누적\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}: Loss {epoch_loss / len(dataloader)}\")  # epoch별 평균 손실 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "68f35739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 1, 1, 0, 1]\n",
      "[1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# 평가 / 예측 : 학습된 모델로 확률 -> 0/1 예측값 생성 후 정답과 비교\n",
    "model.eval()                        # 평가 모드\n",
    "with torch.no_grad():               # 기울기 계산 비활성화\n",
    "    output = model(X)               # 전체 샘플에 대한 예측 logit 계산\n",
    "    prob = torch.sigmoid(output)    # logit에 0~1 확률로 변환\n",
    "    pred = (prob >= 0.5).int()      # 임계값 0.5 기준으로 이진 분류(0/1) 예측값 생성\n",
    "\n",
    "print(labels)\n",
    "print(pred.squeeze().detach().numpy())  # 예측라벨을 1차원 numpy 배열로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f200e5e1",
   "metadata": {},
   "source": [
    "## 사전학습된 임베딩을 사용하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d93011d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000000, 300)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_wv = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "model_wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4996d30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "(17, 300)\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 매트릭스 초기화 : 사전학습 벡터로 Embedding 레이어를 채우기 위한 준비\n",
    "print(len(word_to_index))  # 어휘 크기 (vocab_size) 확인\n",
    "\n",
    "# (vocab_size, embedding_dim) 크기의 0 행렬 생성\n",
    "embedding_matrix = np.zeros((len(word_to_index), model_wv.vectors.shape[1]))\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c9c8ab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전학습 임베딩 매핑 : 내 단어사전을 GoogleNews 벡터로 채워 embedding_matrix 구성\n",
    "# model_wv.key_to_index['bad']  # 'bad'의 내부 인덱스 확인 (706)\n",
    "# model_wv.vectors[240]         # 특정 인덱스 벡터 직접 조회\n",
    "\n",
    "# 단어가 사전학습 모델에 있으면 임베딩 벡터(np.ndarray)를 반환, 없으면 None 반환\n",
    "def get_word_embedding(word):\n",
    "    if word in model_wv:          # 사전학습 단어가 존재하면\n",
    "        return model_wv[word]     # 해당 단어 임베딩 벡터 반환\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# get_word_embedding('bad')\n",
    "\n",
    "for word, index in word_to_index.items():  # 내 단어사전(단어-> 인덱스)를 순회\n",
    "    if index >= 2:                         # 특수토큰 제외\n",
    "        emb = get_word_embedding(word)     # 사전학습 임베딩에서 해당 단어 벡터 조회\n",
    "        if emb is not None:                # 벡터가 존재하면\n",
    "            embedding_matrix[index] = emb  # 내 인덱스 위치에 사전학습 벡터를 복사해서 채운다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "348f8768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;PAD&gt;</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;UNK&gt;</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nice</th>\n",
       "      <td>0.158203</td>\n",
       "      <td>0.105957</td>\n",
       "      <td>-0.189453</td>\n",
       "      <td>0.386719</td>\n",
       "      <td>0.083496</td>\n",
       "      <td>-0.267578</td>\n",
       "      <td>0.083496</td>\n",
       "      <td>0.113281</td>\n",
       "      <td>-0.104004</td>\n",
       "      <td>0.178711</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085449</td>\n",
       "      <td>0.189453</td>\n",
       "      <td>-0.146484</td>\n",
       "      <td>0.134766</td>\n",
       "      <td>-0.040771</td>\n",
       "      <td>0.032715</td>\n",
       "      <td>0.089355</td>\n",
       "      <td>-0.267578</td>\n",
       "      <td>0.008362</td>\n",
       "      <td>-0.213867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>0.071777</td>\n",
       "      <td>0.208008</td>\n",
       "      <td>-0.028442</td>\n",
       "      <td>0.178711</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>-0.099609</td>\n",
       "      <td>0.096191</td>\n",
       "      <td>-0.116699</td>\n",
       "      <td>-0.008545</td>\n",
       "      <td>0.148438</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011475</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>-0.289062</td>\n",
       "      <td>-0.048096</td>\n",
       "      <td>-0.199219</td>\n",
       "      <td>-0.071289</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>-0.167969</td>\n",
       "      <td>-0.020874</td>\n",
       "      <td>-0.142578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>-0.126953</td>\n",
       "      <td>0.021973</td>\n",
       "      <td>0.287109</td>\n",
       "      <td>0.153320</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>0.032715</td>\n",
       "      <td>-0.115723</td>\n",
       "      <td>-0.029541</td>\n",
       "      <td>0.153320</td>\n",
       "      <td>0.011292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006439</td>\n",
       "      <td>-0.033936</td>\n",
       "      <td>-0.166016</td>\n",
       "      <td>-0.016846</td>\n",
       "      <td>-0.048584</td>\n",
       "      <td>-0.022827</td>\n",
       "      <td>-0.152344</td>\n",
       "      <td>-0.101562</td>\n",
       "      <td>-0.090332</td>\n",
       "      <td>0.088379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazing</th>\n",
       "      <td>0.073730</td>\n",
       "      <td>0.004059</td>\n",
       "      <td>-0.135742</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>0.180664</td>\n",
       "      <td>-0.046631</td>\n",
       "      <td>0.224609</td>\n",
       "      <td>-0.229492</td>\n",
       "      <td>-0.040039</td>\n",
       "      <td>0.225586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018433</td>\n",
       "      <td>-0.021240</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.020142</td>\n",
       "      <td>-0.310547</td>\n",
       "      <td>-0.207031</td>\n",
       "      <td>-0.006317</td>\n",
       "      <td>-0.141602</td>\n",
       "      <td>-0.150391</td>\n",
       "      <td>-0.137695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop</th>\n",
       "      <td>-0.057861</td>\n",
       "      <td>0.013184</td>\n",
       "      <td>0.115234</td>\n",
       "      <td>0.069824</td>\n",
       "      <td>-0.306641</td>\n",
       "      <td>-0.044678</td>\n",
       "      <td>0.048584</td>\n",
       "      <td>0.152344</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>-0.100098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100098</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>-0.113281</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>-0.115723</td>\n",
       "      <td>0.048096</td>\n",
       "      <td>-0.004822</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>0.029907</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lies</th>\n",
       "      <td>0.149414</td>\n",
       "      <td>-0.012817</td>\n",
       "      <td>0.328125</td>\n",
       "      <td>0.025513</td>\n",
       "      <td>0.017334</td>\n",
       "      <td>0.190430</td>\n",
       "      <td>0.188477</td>\n",
       "      <td>-0.143555</td>\n",
       "      <td>-0.090820</td>\n",
       "      <td>0.206055</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.308594</td>\n",
       "      <td>0.183594</td>\n",
       "      <td>-0.202148</td>\n",
       "      <td>0.031494</td>\n",
       "      <td>-0.164062</td>\n",
       "      <td>-0.201172</td>\n",
       "      <td>0.080078</td>\n",
       "      <td>-0.105469</td>\n",
       "      <td>0.149414</td>\n",
       "      <td>0.157227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pitiful</th>\n",
       "      <td>0.269531</td>\n",
       "      <td>0.253906</td>\n",
       "      <td>-0.020996</td>\n",
       "      <td>0.060303</td>\n",
       "      <td>-0.010925</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>0.139648</td>\n",
       "      <td>-0.057617</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.253906</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063477</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>-0.094238</td>\n",
       "      <td>0.089355</td>\n",
       "      <td>-0.065430</td>\n",
       "      <td>-0.016235</td>\n",
       "      <td>-0.107910</td>\n",
       "      <td>-0.072266</td>\n",
       "      <td>-0.094238</td>\n",
       "      <td>0.028809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nerd</th>\n",
       "      <td>0.265625</td>\n",
       "      <td>-0.207031</td>\n",
       "      <td>-0.026611</td>\n",
       "      <td>0.419922</td>\n",
       "      <td>-0.208984</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.063965</td>\n",
       "      <td>0.149414</td>\n",
       "      <td>-0.017700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215820</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>-0.227539</td>\n",
       "      <td>-0.310547</td>\n",
       "      <td>-0.112793</td>\n",
       "      <td>-0.096680</td>\n",
       "      <td>0.255859</td>\n",
       "      <td>0.124023</td>\n",
       "      <td>-0.030273</td>\n",
       "      <td>0.082031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excellent</th>\n",
       "      <td>-0.212891</td>\n",
       "      <td>-0.004303</td>\n",
       "      <td>-0.180664</td>\n",
       "      <td>-0.007568</td>\n",
       "      <td>0.112793</td>\n",
       "      <td>0.163086</td>\n",
       "      <td>-0.014709</td>\n",
       "      <td>-0.078613</td>\n",
       "      <td>-0.164062</td>\n",
       "      <td>0.279297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>-0.173828</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>-0.081055</td>\n",
       "      <td>0.013550</td>\n",
       "      <td>-0.008362</td>\n",
       "      <td>-0.129883</td>\n",
       "      <td>-0.215820</td>\n",
       "      <td>0.012268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>-0.075684</td>\n",
       "      <td>0.033691</td>\n",
       "      <td>-0.064941</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>0.050537</td>\n",
       "      <td>0.149414</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>-0.133789</td>\n",
       "      <td>-0.020874</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.101562</td>\n",
       "      <td>-0.091309</td>\n",
       "      <td>0.052246</td>\n",
       "      <td>-0.164062</td>\n",
       "      <td>0.121582</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.012024</td>\n",
       "      <td>0.135742</td>\n",
       "      <td>-0.091309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>supreme</th>\n",
       "      <td>0.173828</td>\n",
       "      <td>0.172852</td>\n",
       "      <td>0.112793</td>\n",
       "      <td>0.166016</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>-0.014221</td>\n",
       "      <td>0.128906</td>\n",
       "      <td>-0.217773</td>\n",
       "      <td>0.073730</td>\n",
       "      <td>0.205078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>-0.221680</td>\n",
       "      <td>-0.055664</td>\n",
       "      <td>0.034424</td>\n",
       "      <td>-0.119629</td>\n",
       "      <td>-0.081543</td>\n",
       "      <td>0.121094</td>\n",
       "      <td>-0.164062</td>\n",
       "      <td>-0.127930</td>\n",
       "      <td>0.097656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <td>-0.259766</td>\n",
       "      <td>0.271484</td>\n",
       "      <td>0.119629</td>\n",
       "      <td>0.007233</td>\n",
       "      <td>0.057373</td>\n",
       "      <td>0.113770</td>\n",
       "      <td>0.166992</td>\n",
       "      <td>0.025024</td>\n",
       "      <td>0.067871</td>\n",
       "      <td>0.120117</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145508</td>\n",
       "      <td>0.120117</td>\n",
       "      <td>-0.314453</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>-0.010254</td>\n",
       "      <td>0.298828</td>\n",
       "      <td>0.046387</td>\n",
       "      <td>-0.179688</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>-0.014099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>0.062988</td>\n",
       "      <td>0.124512</td>\n",
       "      <td>0.113281</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>0.038818</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.096191</td>\n",
       "      <td>0.220703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011353</td>\n",
       "      <td>0.341797</td>\n",
       "      <td>-0.090332</td>\n",
       "      <td>0.076660</td>\n",
       "      <td>-0.032471</td>\n",
       "      <td>0.133789</td>\n",
       "      <td>-0.154297</td>\n",
       "      <td>-0.063477</td>\n",
       "      <td>0.114746</td>\n",
       "      <td>0.031006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>highly</th>\n",
       "      <td>0.050781</td>\n",
       "      <td>-0.227539</td>\n",
       "      <td>-0.130859</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>-0.165039</td>\n",
       "      <td>0.118652</td>\n",
       "      <td>-0.230469</td>\n",
       "      <td>-0.225586</td>\n",
       "      <td>0.245117</td>\n",
       "      <td>-0.086914</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013733</td>\n",
       "      <td>-0.013489</td>\n",
       "      <td>0.175781</td>\n",
       "      <td>0.226562</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>-0.210938</td>\n",
       "      <td>-0.111816</td>\n",
       "      <td>-0.056641</td>\n",
       "      <td>0.102539</td>\n",
       "      <td>0.074707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>respectable</th>\n",
       "      <td>0.150391</td>\n",
       "      <td>0.285156</td>\n",
       "      <td>-0.023193</td>\n",
       "      <td>0.095703</td>\n",
       "      <td>-0.022095</td>\n",
       "      <td>0.058350</td>\n",
       "      <td>-0.155273</td>\n",
       "      <td>-0.051025</td>\n",
       "      <td>0.100098</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058838</td>\n",
       "      <td>0.056885</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.138672</td>\n",
       "      <td>-0.227539</td>\n",
       "      <td>0.183594</td>\n",
       "      <td>-0.033447</td>\n",
       "      <td>-0.200195</td>\n",
       "      <td>-0.112305</td>\n",
       "      <td>0.103027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0         1         2         3         4         5    \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "<UNK>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "nice         0.158203  0.105957 -0.189453  0.386719  0.083496 -0.267578   \n",
       "great        0.071777  0.208008 -0.028442  0.178711  0.132812 -0.099609   \n",
       "best        -0.126953  0.021973  0.287109  0.153320  0.127930  0.032715   \n",
       "amazing      0.073730  0.004059 -0.135742  0.022095  0.180664 -0.046631   \n",
       "stop        -0.057861  0.013184  0.115234  0.069824 -0.306641 -0.044678   \n",
       "lies         0.149414 -0.012817  0.328125  0.025513  0.017334  0.190430   \n",
       "pitiful      0.269531  0.253906 -0.020996  0.060303 -0.010925  0.217773   \n",
       "nerd         0.265625 -0.207031 -0.026611  0.419922 -0.208984  0.390625   \n",
       "excellent   -0.212891 -0.004303 -0.180664 -0.007568  0.112793  0.163086   \n",
       "work        -0.075684  0.033691 -0.064941  0.131836  0.050537  0.149414   \n",
       "supreme      0.173828  0.172852  0.112793  0.166016  0.058594 -0.014221   \n",
       "quality     -0.259766  0.271484  0.119629  0.007233  0.057373  0.113770   \n",
       "bad          0.062988  0.124512  0.113281  0.073242  0.038818  0.079102   \n",
       "highly       0.050781 -0.227539 -0.130859  0.062500 -0.165039  0.118652   \n",
       "respectable  0.150391  0.285156 -0.023193  0.095703 -0.022095  0.058350   \n",
       "\n",
       "                  6         7         8         9    ...       290       291  \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "<UNK>        0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "nice         0.083496  0.113281 -0.104004  0.178711  ... -0.085449  0.189453   \n",
       "great        0.096191 -0.116699 -0.008545  0.148438  ... -0.011475  0.064453   \n",
       "best        -0.115723 -0.029541  0.153320  0.011292  ...  0.006439 -0.033936   \n",
       "amazing      0.224609 -0.229492 -0.040039  0.225586  ...  0.018433 -0.021240   \n",
       "stop         0.048584  0.152344  0.073242 -0.100098  ...  0.100098  0.171875   \n",
       "lies         0.188477 -0.143555 -0.090820  0.206055  ... -0.308594  0.183594   \n",
       "pitiful      0.139648 -0.057617  0.312500  0.253906  ... -0.063477  0.132812   \n",
       "nerd         0.164062  0.063965  0.149414 -0.017700  ...  0.215820  0.125000   \n",
       "excellent   -0.014709 -0.078613 -0.164062  0.279297  ...  0.136719  0.000282   \n",
       "work         0.109375 -0.133789 -0.020874  0.054688  ... -0.187500  0.101562   \n",
       "supreme      0.128906 -0.217773  0.073730  0.205078  ...  0.140625 -0.221680   \n",
       "quality      0.166992  0.025024  0.067871  0.120117  ... -0.145508  0.120117   \n",
       "bad          0.050781  0.171875  0.096191  0.220703  ...  0.011353  0.341797   \n",
       "highly      -0.230469 -0.225586  0.245117 -0.086914  ... -0.013733 -0.013489   \n",
       "respectable -0.155273 -0.051025  0.100098  0.001183  ...  0.058838  0.056885   \n",
       "\n",
       "                  292       293       294       295       296       297  \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "<UNK>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "nice        -0.146484  0.134766 -0.040771  0.032715  0.089355 -0.267578   \n",
       "great       -0.289062 -0.048096 -0.199219 -0.071289  0.064453 -0.167969   \n",
       "best        -0.166016 -0.016846 -0.048584 -0.022827 -0.152344 -0.101562   \n",
       "amazing     -0.250000 -0.020142 -0.310547 -0.207031 -0.006317 -0.141602   \n",
       "stop        -0.113281  0.064453 -0.115723  0.048096 -0.004822  0.086426   \n",
       "lies        -0.202148  0.031494 -0.164062 -0.201172  0.080078 -0.105469   \n",
       "pitiful     -0.094238  0.089355 -0.065430 -0.016235 -0.107910 -0.072266   \n",
       "nerd        -0.227539 -0.310547 -0.112793 -0.096680  0.255859  0.124023   \n",
       "excellent   -0.173828  0.004242 -0.081055  0.013550 -0.008362 -0.129883   \n",
       "work        -0.091309  0.052246 -0.164062  0.121582  0.062500  0.012024   \n",
       "supreme     -0.055664  0.034424 -0.119629 -0.081543  0.121094 -0.164062   \n",
       "quality     -0.314453  0.022095 -0.010254  0.298828  0.046387 -0.179688   \n",
       "bad         -0.090332  0.076660 -0.032471  0.133789 -0.154297 -0.063477   \n",
       "highly       0.175781  0.226562  0.086914 -0.210938 -0.111816 -0.056641   \n",
       "respectable -0.187500  0.138672 -0.227539  0.183594 -0.033447 -0.200195   \n",
       "\n",
       "                  298       299  \n",
       "<PAD>        0.000000  0.000000  \n",
       "<UNK>        0.000000  0.000000  \n",
       "nice         0.008362 -0.213867  \n",
       "great       -0.020874 -0.142578  \n",
       "best        -0.090332  0.088379  \n",
       "amazing     -0.150391 -0.137695  \n",
       "stop         0.029907  0.007812  \n",
       "lies         0.149414  0.157227  \n",
       "pitiful     -0.094238  0.028809  \n",
       "nerd        -0.030273  0.082031  \n",
       "excellent   -0.215820  0.012268  \n",
       "work         0.135742 -0.091309  \n",
       "supreme     -0.127930  0.097656  \n",
       "quality      0.000706 -0.014099  \n",
       "bad          0.114746  0.031006  \n",
       "highly       0.102539  0.074707  \n",
       "respectable -0.112305  0.103027  \n",
       "\n",
       "[17 rows x 300 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(embedding_matrix, index=word_to_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "662bcec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNet(\n",
      "  (embedding): Embedding(17, 300, padding_idx=0)\n",
      "  (rnn): RNN(300, 16, batch_first=True)\n",
      "  (out): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Pytorch 텍스트 분류 모델 : Embedding + RNN + Linear로 이진 분류(logit) 출력\n",
    "import torch\n",
    "import torch.nn as nn           # 신경망 레이어\n",
    "import torch.optim as optim     # 옵티마이저(활성화함수)\n",
    "from torch.utils.data import DataLoader, TensorDataset  # 배치 로더 / 데이터셋 유틸\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    # 정수 시퀀스를 임베딩 -> RNN -> 선형층으로 처리해 이진 분류 logit(1개)를 출력\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super().__init__()                    # nn.Module 초기화\n",
    "        self.embedding = nn.Embedding(        # 단어 ID를 밀집 벡터로 변환하는 임베딩 층\n",
    "            num_embeddings = vocab_size,      # 단어 사전 크기 (어휘 수)\n",
    "            embedding_dim = embedding_dim,    # 임베딩 차원\n",
    "            padding_idx = 0                   # PAD(0) 인덱스는 0 그대로 사용\n",
    "        )\n",
    "\n",
    "        # 사전학습된 임베딩 벡터로 초기화 : Embedding 가중치를 사전학습 행렬로 덮어쓰기\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float))\n",
    "\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)    # 입력(배치, 길이, 차원) 형태의 RNN\n",
    "        self.out = nn.Linear(hidden_size, 1)     # 마지막 은닉 상태를 1차원 logit으로 변환\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)     # (batch, seq_len) -> (batch, seq_len, embedding_dim)\n",
    "        out, h_n = self.rnn(embedded)    # h_n: (num_layers*directions, batch, hidden_size)\n",
    "        out = self.out(h_n.squeeze(0))   # (batch_size, hidden_size) -> (batch, 1)\n",
    "        return out  # 출력 : 시그모이드 전 logit(확률이 아님)\n",
    "    \n",
    "embedding_dim = model_wv.vectors.shape[1]  # 사전학습 임베딩 차원 (300)으로 임베딩 차원 설정\n",
    "model = SimpleNet(vocab_size, embedding_dim, hidden_size=16)  # 어휘 크기 / 임베딩 차원/ 은닉크기로 모델 생성\n",
    "print(model)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # 출력 logit과 정답(0/1)로 이진분류 손실 계산 (시그모이드 포함)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)  # 모델 파라미터는 Adam으로 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e981c48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss 0.6922577470541\n",
      "Epoch 2: Loss 0.5622858107089996\n",
      "Epoch 3: Loss 0.4439684972167015\n",
      "Epoch 4: Loss 0.316558800637722\n",
      "Epoch 5: Loss 0.21686740964651108\n",
      "Epoch 6: Loss 0.13932573050260544\n",
      "Epoch 7: Loss 0.0961643336340785\n",
      "Epoch 8: Loss 0.07127263024449348\n",
      "Epoch 9: Loss 0.05419974774122238\n",
      "Epoch 10: Loss 0.04231053590774536\n",
      "Epoch 11: Loss 0.033689807169139385\n",
      "Epoch 12: Loss 0.02824466908350587\n",
      "Epoch 13: Loss 0.024168975185602903\n",
      "Epoch 14: Loss 0.0210575251840055\n",
      "Epoch 15: Loss 0.018407278694212437\n",
      "Epoch 16: Loss 0.016493213130161166\n",
      "Epoch 17: Loss 0.014807967934757471\n",
      "Epoch 18: Loss 0.013822223525494337\n",
      "Epoch 19: Loss 0.012630525976419449\n",
      "Epoch 20: Loss 0.011683398624882102\n"
     ]
    }
   ],
   "source": [
    "# 학습 루프 : 미니배치 단위로 20 epoch 학습하며 평균 손실 출력\n",
    "for epoch in range(20):\n",
    "    epoch_loss = 0    # 손실 누적\n",
    "\n",
    "    for x_batch, y_batch in dataloader:    # 미니배치 단위로 (X, y) 가져오기\n",
    "        optimizer.zero_grad()              # 이전 배치 기울기 초기화\n",
    "        output = model(x_batch)            # 순전파로 logit 계산\n",
    "        loss = criterion(output, y_batch)  # 예측 logit과 정답으로 손실 계산\n",
    "        loss.backward()                    # 역전파로 기울기 계산\n",
    "        optimizer.step()                   # 파라미터 업데이트\n",
    "\n",
    "        epoch_loss += loss.item()  # 배치손실을 float으로 누적\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}: Loss {epoch_loss / len(dataloader)}\")  # epoch별 평균 손실 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ce55d502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 1, 1, 0, 1]\n",
      "[1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# 평가 / 예측 : 학습된 모델로 확률 -> 0/1 예측값 생성 후 정답과 비교\n",
    "model.eval()                        # 평가 모드\n",
    "with torch.no_grad():               # 기울기 계산 비활성화\n",
    "    output = model(X)               # 전체 샘플에 대한 예측 logit 계산\n",
    "    prob = torch.sigmoid(output)    # logit에 0~1 확률로 변환\n",
    "    pred = (prob >= 0.5).int()      # 임계값 0.5 기준으로 이진 분류(0/1) 예측값 생성\n",
    "\n",
    "print(labels)\n",
    "print(pred.squeeze().detach().numpy())  # 예측라벨을 1차원 numpy 배열로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850bbf48",
   "metadata": {},
   "source": [
    "사전학습 임베딩을 사용했을 때에도 학습 데이터 분류가 잘 되는지 파악한다.  \n",
    "만약 틀린 샘플이 있다면 해당 문장이 OOV(0벡터) 비중이 큰지 확인해봐야 한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
