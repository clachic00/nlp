{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12cd7689",
   "metadata": {},
   "source": [
    "# torch `nn.Embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ae01602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Playdata/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Playdata/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Playdata/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468c0c85",
   "metadata": {},
   "source": [
    "## 사전학습된 임베딩을 사용하지 않는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d7c8092",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [          \n",
    "    'nice great best amazing',  # 긍정 문장 예시\n",
    "    'stop lies',                # 부정/비판 문장 예시\n",
    "    'pitiful nerd',             # 부정 문장 예시\n",
    "    'excellent work',           # 긍정 문장 예시\n",
    "    'supreme quality',          # 긍정 문장 예시\n",
    "    'bad',                      # 부정 문장 예시\n",
    "    'highly respectable'        # 긍정 문장 예시\n",
    "]                               # 분류 모델에 넣을 입력 문장 리스트(list[str])\n",
    "labels = [1, 0, 0, 1, 1, 0, 1]  # 각 문장에 대한 이진 라벨(1=긍정, 0=부정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a4f3684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nice', 'great', 'best', 'amazing'],\n",
       " ['stop', 'lies'],\n",
       " ['pitiful', 'nerd'],\n",
       " ['excellent', 'work'],\n",
       " ['supreme', 'quality'],\n",
       " ['bad'],\n",
       " ['highly', 'respectable']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰화\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_sentences = [word_tokenize(sent) for sent in sentences]    # 각 문장을 토큰 리스트(list(list[str]))로 변환\n",
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bc1ce77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'nice': 1, 'great': 1, 'best': 1, 'amazing': 1, 'stop': 1, 'lies': 1, 'pitiful': 1, 'nerd': 1, 'excellent': 1, 'work': 1, 'supreme': 1, 'quality': 1, 'bad': 1, 'highly': 1, 'respectable': 1})\n",
      "{'<PAD>': 0, '<UNK>': 1, 'nice': 2, 'great': 3, 'best': 4, 'amazing': 5, 'stop': 6, 'lies': 7, 'pitiful': 8, 'nerd': 9, 'excellent': 10, 'work': 11, 'supreme': 12, 'quality': 13, 'bad': 14, 'highly': 15, 'respectable': 16}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 사전 생성 + 정수 인코딩\n",
    "from collections import Counter\n",
    "\n",
    "tokens = [token for sent in tokenized_sentences for token in sent]  # 문장 리스트를 평탄화하여 전체 토큰 리스트 생성\n",
    "word_counts = Counter(tokens)  # 전체 토큰 등장 빈도 계산\n",
    "print(word_counts)  # 토큰별 빈도 딕셔너리 형태\n",
    "\n",
    "word_to_index = {word: index + 2 for index, word in enumerate(tokens)}  # 토큰을 순서대로 인덱싱(+2 : 특수토큰용)\n",
    "word_to_index['<PAD>'] = 0    # 패딩 토큰 (길이 맞추기용)\n",
    "word_to_index['<UNK>'] = 1    # OOV 토큰 (처리 불가 단어 대체)\n",
    "word_to_index = dict(sorted(word_to_index.items(), key=lambda x: x[1]))  # 인덱스를 기준으로 정렬\n",
    "print(word_to_index)  # 단어 -> 인덱스 사전\n",
    "\n",
    "vocab_size = len(word_to_index)  # 전체 어휘 수 (특수토큰 포함)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5e4e32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14], [15, 16]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정수 인코딩 함수\n",
    "# : 토큰화된 문장 리스트를 단어 → 인덱스 사전을 이용해\n",
    "#   정수 시퀀스(list[list[int]])로 변환\n",
    "def texts_to_sequences(sentences, word_to_index):\n",
    "    sequences = []                                  # 전체 문장 시퀀스를 저장할 리스트\n",
    "\n",
    "    for sent in sentences:                          # 문장 단위로 반복\n",
    "        sequence = []                               # 현재 문장의 정수 시퀀스\n",
    "\n",
    "        for token in sent:                          # 문장 내 토큰(단어) 단위 반복\n",
    "            if token in word_to_index:              # 단어가 사전에 존재하면\n",
    "                sequence.append(word_to_index[token])  # 해당 단어 인덱스 추가\n",
    "            else:\n",
    "                sequence.append(word_to_index['<UNK>'])  # 사전에 없으면 UNK 토큰 인덱스 추가\n",
    "        \n",
    "        sequences.append(sequence)                  # 문장 하나의 시퀀스를 결과에 저장\n",
    "    \n",
    "    return sequences                                # 전체 문장 정수 시퀀스 반환\n",
    "\n",
    "\n",
    "sequences = texts_to_sequences(                     # 토큰화된 문장들을 정수 시퀀스로 변환\n",
    "    tokenized_sentences,\n",
    "    word_to_index\n",
    ")\n",
    "sequences                                           # 변환 결과 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33b427e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  3,  4,  5],\n",
       "       [ 6,  7,  0,  0],\n",
       "       [ 8,  9,  0,  0],\n",
       "       [10, 11,  0,  0],\n",
       "       [12, 13,  0,  0],\n",
       "       [14,  0,  0,  0],\n",
       "       [15, 16,  0,  0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 서로 다른 길이의 정수 시퀀스를 0(<PAD>)으로 채우거나 잘라내 (문장수, maxlen) 형태에 맞춰주는 함수\n",
    "def pad_sequences(sequences, maxlen):\n",
    "    padded_sequences = np.zeros((len(sequences), maxlen), dtype=int)  # (문장수 x maxlen) 크기의 0 패딩 배열\n",
    "    \n",
    "    for index, seq in enumerate(sequences):  # 각 문장 시퀀스 순회\n",
    "        padded_sequences[index, :len(seq)] = seq[:maxlen]  # 앞에서부터 시퀀스 채운다. 길면 maxlen까지만 채워 자른다.\n",
    "    \n",
    "    return padded_sequences  # 패딩 작업 완료된 2D 배열\n",
    "\n",
    "padded_sequences = pad_sequences(sequences, maxlen=4)  # 모든 문장 길이 4로 패딩/자르기\n",
    "padded_sequences  # (문장 수, 4) 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6cd7bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93751631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNet(\n",
       "  (embedding): Embedding(17, 100, padding_idx=0)\n",
       "  (rnn): RNN(100, 16, batch_first=True)\n",
       "  (out): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pytorch 텍스트 분류 모델 : Embedding + RNN + Linear로 이진 분류(logit) 출력\n",
    "import torch\n",
    "import torch.nn as nn           # 신경망 레이어\n",
    "import torch.optim as optim     # 옵티마이저(활성화함수)\n",
    "from torch.utils.data import DataLoader, TensorDataset  # 배치 로더 / 데이터셋 유틸\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    # 정수 시퀀스를 임베딩 -> RNN -> 선형층으로 처리해 이진 분류 logit(1개)를 출력\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super().__init__()                    # nn.Module 초기화\n",
    "        self.embedding = nn.Embedding(        # 단어 ID를 밀집 벡터로 변환하는 임베딩 층\n",
    "            num_embeddings = vocab_size,      # 단어 사전 크기 (어휘 수)\n",
    "            embedding_dim = embedding_dim,    # 임베딩 차원\n",
    "            padding_idx = 0                   # PAD(0) 인덱스는 0 그대로 사용\n",
    "        )\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)    # 입력(배치, 길이, 차원) 형태의 RNN\n",
    "        self.out = nn.Linear(hidden_size, 1)     # 마지막 은닉 상태를 1차원 logit으로 변환\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)     # (batch, seq_len) -> (batch, seq_len, embedding_dim)\n",
    "        out, h_n = self.rnn(embedded)    # h_n: (num_layers*directions, batch, hidden_size)\n",
    "        out = self.out(h_n.squeeze(0))   # (batch_size, hidden_size) -> (batch, 1)\n",
    "        return out  # 출력 : 시그모이드 전 logit(확률이 아님)\n",
    "    \n",
    "embedding_dim = 100  # 단어 벡터 차원 설정\n",
    "model = SimpleNet(vocab_size, embedding_dim, hidden_size=16)  # 어휘 크기 / 임베딩 차원/ 은닉크기로 모델 생성\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f45bf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (1.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2e27f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "SimpleNet                                --\n",
       "├─Embedding: 1-1                         1,700\n",
       "├─RNN: 1-2                               1,888\n",
       "├─Linear: 1-3                            17\n",
       "=================================================================\n",
       "Total params: 3,605\n",
       "Trainable params: 3,605\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary  # 모델 구조를 표 형태로 요약\n",
    "\n",
    "summary(model)  # model의 레이어 구성 / 파라미터 수를 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a284e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17, 100])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;PAD&gt;</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;UNK&gt;</th>\n",
       "      <td>0.288928</td>\n",
       "      <td>1.607660</td>\n",
       "      <td>-0.056371</td>\n",
       "      <td>-0.469463</td>\n",
       "      <td>0.469999</td>\n",
       "      <td>-0.649331</td>\n",
       "      <td>1.339970</td>\n",
       "      <td>0.047787</td>\n",
       "      <td>1.187581</td>\n",
       "      <td>-0.378604</td>\n",
       "      <td>...</td>\n",
       "      <td>1.163199</td>\n",
       "      <td>-0.957975</td>\n",
       "      <td>-0.136281</td>\n",
       "      <td>0.781986</td>\n",
       "      <td>1.577181</td>\n",
       "      <td>-0.096898</td>\n",
       "      <td>1.777398</td>\n",
       "      <td>-0.315396</td>\n",
       "      <td>-0.166276</td>\n",
       "      <td>-0.431706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nice</th>\n",
       "      <td>-0.375107</td>\n",
       "      <td>-1.502437</td>\n",
       "      <td>-0.923492</td>\n",
       "      <td>-0.126488</td>\n",
       "      <td>0.362706</td>\n",
       "      <td>0.342689</td>\n",
       "      <td>-1.466642</td>\n",
       "      <td>1.847192</td>\n",
       "      <td>-0.179850</td>\n",
       "      <td>-0.388620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375694</td>\n",
       "      <td>0.324467</td>\n",
       "      <td>1.260270</td>\n",
       "      <td>0.276454</td>\n",
       "      <td>0.188943</td>\n",
       "      <td>1.352863</td>\n",
       "      <td>-0.998862</td>\n",
       "      <td>-0.222001</td>\n",
       "      <td>-0.426115</td>\n",
       "      <td>-1.941638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>-2.070317</td>\n",
       "      <td>0.062140</td>\n",
       "      <td>-0.049227</td>\n",
       "      <td>-0.107032</td>\n",
       "      <td>-0.310520</td>\n",
       "      <td>1.134613</td>\n",
       "      <td>1.739478</td>\n",
       "      <td>-0.535167</td>\n",
       "      <td>-0.148535</td>\n",
       "      <td>0.059957</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.768177</td>\n",
       "      <td>1.688066</td>\n",
       "      <td>0.556124</td>\n",
       "      <td>-2.068017</td>\n",
       "      <td>-1.331687</td>\n",
       "      <td>-0.080085</td>\n",
       "      <td>-0.362097</td>\n",
       "      <td>-1.092865</td>\n",
       "      <td>1.689988</td>\n",
       "      <td>0.510178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>-0.734108</td>\n",
       "      <td>-0.694109</td>\n",
       "      <td>0.215780</td>\n",
       "      <td>0.343054</td>\n",
       "      <td>2.067814</td>\n",
       "      <td>-1.004351</td>\n",
       "      <td>-0.709888</td>\n",
       "      <td>-1.500188</td>\n",
       "      <td>0.051568</td>\n",
       "      <td>0.479909</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.976840</td>\n",
       "      <td>1.053008</td>\n",
       "      <td>0.526363</td>\n",
       "      <td>-0.673199</td>\n",
       "      <td>0.777457</td>\n",
       "      <td>-0.028395</td>\n",
       "      <td>-0.826402</td>\n",
       "      <td>-0.258068</td>\n",
       "      <td>0.024702</td>\n",
       "      <td>0.888323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazing</th>\n",
       "      <td>0.110546</td>\n",
       "      <td>0.140450</td>\n",
       "      <td>-1.180040</td>\n",
       "      <td>-0.194728</td>\n",
       "      <td>-0.007513</td>\n",
       "      <td>0.135330</td>\n",
       "      <td>-0.224973</td>\n",
       "      <td>0.292712</td>\n",
       "      <td>1.269658</td>\n",
       "      <td>-1.625910</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.258460</td>\n",
       "      <td>0.843542</td>\n",
       "      <td>-0.818338</td>\n",
       "      <td>0.126159</td>\n",
       "      <td>0.631749</td>\n",
       "      <td>0.263715</td>\n",
       "      <td>-0.829794</td>\n",
       "      <td>-1.229905</td>\n",
       "      <td>-1.592322</td>\n",
       "      <td>0.011544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop</th>\n",
       "      <td>1.161571</td>\n",
       "      <td>-0.293670</td>\n",
       "      <td>-0.926437</td>\n",
       "      <td>-0.171423</td>\n",
       "      <td>0.147151</td>\n",
       "      <td>-0.434187</td>\n",
       "      <td>0.556416</td>\n",
       "      <td>-2.125310</td>\n",
       "      <td>0.109032</td>\n",
       "      <td>-0.475795</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.337052</td>\n",
       "      <td>-0.262313</td>\n",
       "      <td>0.527860</td>\n",
       "      <td>-1.155204</td>\n",
       "      <td>-0.020593</td>\n",
       "      <td>1.073358</td>\n",
       "      <td>0.721517</td>\n",
       "      <td>-1.528801</td>\n",
       "      <td>2.115555</td>\n",
       "      <td>0.229224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lies</th>\n",
       "      <td>-0.550596</td>\n",
       "      <td>-0.910341</td>\n",
       "      <td>0.699479</td>\n",
       "      <td>-2.037829</td>\n",
       "      <td>-0.102543</td>\n",
       "      <td>-0.621193</td>\n",
       "      <td>-0.988834</td>\n",
       "      <td>2.027308</td>\n",
       "      <td>1.513137</td>\n",
       "      <td>0.382081</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.364772</td>\n",
       "      <td>0.934649</td>\n",
       "      <td>-1.553773</td>\n",
       "      <td>-0.259093</td>\n",
       "      <td>1.016199</td>\n",
       "      <td>0.063753</td>\n",
       "      <td>2.199706</td>\n",
       "      <td>-1.115450</td>\n",
       "      <td>-1.433277</td>\n",
       "      <td>0.827748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pitiful</th>\n",
       "      <td>-0.255950</td>\n",
       "      <td>0.279579</td>\n",
       "      <td>0.929856</td>\n",
       "      <td>1.384931</td>\n",
       "      <td>0.032164</td>\n",
       "      <td>0.319030</td>\n",
       "      <td>0.105520</td>\n",
       "      <td>0.527916</td>\n",
       "      <td>0.662397</td>\n",
       "      <td>1.087102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.722323</td>\n",
       "      <td>0.617160</td>\n",
       "      <td>1.415717</td>\n",
       "      <td>-0.759201</td>\n",
       "      <td>0.341842</td>\n",
       "      <td>1.224626</td>\n",
       "      <td>1.466413</td>\n",
       "      <td>-0.349043</td>\n",
       "      <td>-0.000191</td>\n",
       "      <td>-1.460817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nerd</th>\n",
       "      <td>1.075786</td>\n",
       "      <td>0.115069</td>\n",
       "      <td>-0.544386</td>\n",
       "      <td>-0.771980</td>\n",
       "      <td>0.644482</td>\n",
       "      <td>1.079942</td>\n",
       "      <td>-0.767168</td>\n",
       "      <td>-0.080798</td>\n",
       "      <td>0.439931</td>\n",
       "      <td>-1.540084</td>\n",
       "      <td>...</td>\n",
       "      <td>1.107514</td>\n",
       "      <td>0.151260</td>\n",
       "      <td>-1.319436</td>\n",
       "      <td>1.312837</td>\n",
       "      <td>1.290813</td>\n",
       "      <td>1.624247</td>\n",
       "      <td>0.686019</td>\n",
       "      <td>0.273804</td>\n",
       "      <td>0.775039</td>\n",
       "      <td>0.060528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excellent</th>\n",
       "      <td>0.451461</td>\n",
       "      <td>0.696921</td>\n",
       "      <td>-0.159499</td>\n",
       "      <td>-0.324683</td>\n",
       "      <td>-0.614469</td>\n",
       "      <td>0.711076</td>\n",
       "      <td>-1.084300</td>\n",
       "      <td>0.999780</td>\n",
       "      <td>0.168365</td>\n",
       "      <td>1.839730</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.298784</td>\n",
       "      <td>0.777314</td>\n",
       "      <td>-0.469424</td>\n",
       "      <td>0.013878</td>\n",
       "      <td>1.030626</td>\n",
       "      <td>-1.577635</td>\n",
       "      <td>0.325976</td>\n",
       "      <td>-1.058802</td>\n",
       "      <td>-1.411685</td>\n",
       "      <td>-0.298344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>-1.439708</td>\n",
       "      <td>-0.209003</td>\n",
       "      <td>-2.084174</td>\n",
       "      <td>-0.916557</td>\n",
       "      <td>-1.608510</td>\n",
       "      <td>-0.857398</td>\n",
       "      <td>-0.249476</td>\n",
       "      <td>0.976070</td>\n",
       "      <td>-0.382443</td>\n",
       "      <td>0.075832</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.625777</td>\n",
       "      <td>0.330538</td>\n",
       "      <td>-0.872296</td>\n",
       "      <td>0.020790</td>\n",
       "      <td>1.488113</td>\n",
       "      <td>0.213108</td>\n",
       "      <td>-1.583962</td>\n",
       "      <td>0.238860</td>\n",
       "      <td>-0.419670</td>\n",
       "      <td>-1.185783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>supreme</th>\n",
       "      <td>0.223013</td>\n",
       "      <td>-0.028687</td>\n",
       "      <td>0.564882</td>\n",
       "      <td>0.675098</td>\n",
       "      <td>0.341677</td>\n",
       "      <td>-0.191852</td>\n",
       "      <td>0.086266</td>\n",
       "      <td>-0.040405</td>\n",
       "      <td>-0.129255</td>\n",
       "      <td>2.013664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.658365</td>\n",
       "      <td>1.371161</td>\n",
       "      <td>-2.001544</td>\n",
       "      <td>-0.631843</td>\n",
       "      <td>-1.892988</td>\n",
       "      <td>-1.321406</td>\n",
       "      <td>-0.109347</td>\n",
       "      <td>0.755093</td>\n",
       "      <td>-1.208966</td>\n",
       "      <td>0.588318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <td>1.214680</td>\n",
       "      <td>0.817608</td>\n",
       "      <td>-2.055127</td>\n",
       "      <td>0.283380</td>\n",
       "      <td>0.685743</td>\n",
       "      <td>-0.411127</td>\n",
       "      <td>-0.803183</td>\n",
       "      <td>-0.009372</td>\n",
       "      <td>-0.772525</td>\n",
       "      <td>-0.658445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.460884</td>\n",
       "      <td>0.103257</td>\n",
       "      <td>0.015141</td>\n",
       "      <td>0.598679</td>\n",
       "      <td>-0.562882</td>\n",
       "      <td>-1.093343</td>\n",
       "      <td>-0.529995</td>\n",
       "      <td>1.151136</td>\n",
       "      <td>-0.687031</td>\n",
       "      <td>-1.288582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>0.302688</td>\n",
       "      <td>0.601107</td>\n",
       "      <td>1.501010</td>\n",
       "      <td>-0.421628</td>\n",
       "      <td>0.284841</td>\n",
       "      <td>-0.091815</td>\n",
       "      <td>1.130324</td>\n",
       "      <td>-0.284404</td>\n",
       "      <td>0.647604</td>\n",
       "      <td>-0.149717</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.821937</td>\n",
       "      <td>2.214347</td>\n",
       "      <td>1.129811</td>\n",
       "      <td>-0.740424</td>\n",
       "      <td>0.407341</td>\n",
       "      <td>-0.390186</td>\n",
       "      <td>0.169043</td>\n",
       "      <td>-0.678800</td>\n",
       "      <td>-0.010013</td>\n",
       "      <td>1.372044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>highly</th>\n",
       "      <td>0.329463</td>\n",
       "      <td>0.095163</td>\n",
       "      <td>-0.303135</td>\n",
       "      <td>-0.078614</td>\n",
       "      <td>-1.022818</td>\n",
       "      <td>1.102807</td>\n",
       "      <td>1.365102</td>\n",
       "      <td>-1.408793</td>\n",
       "      <td>0.362040</td>\n",
       "      <td>-1.527460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.782015</td>\n",
       "      <td>0.052320</td>\n",
       "      <td>1.736616</td>\n",
       "      <td>1.610907</td>\n",
       "      <td>0.569710</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>-0.832081</td>\n",
       "      <td>0.060972</td>\n",
       "      <td>-0.839275</td>\n",
       "      <td>1.701618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>respectable</th>\n",
       "      <td>0.549015</td>\n",
       "      <td>-1.398794</td>\n",
       "      <td>0.943506</td>\n",
       "      <td>-1.986844</td>\n",
       "      <td>-0.327049</td>\n",
       "      <td>-0.588285</td>\n",
       "      <td>-1.076976</td>\n",
       "      <td>-1.540396</td>\n",
       "      <td>-0.152022</td>\n",
       "      <td>-0.198329</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.291001</td>\n",
       "      <td>0.351746</td>\n",
       "      <td>0.714800</td>\n",
       "      <td>1.081521</td>\n",
       "      <td>-1.739781</td>\n",
       "      <td>0.195157</td>\n",
       "      <td>0.656927</td>\n",
       "      <td>-0.060777</td>\n",
       "      <td>-0.046385</td>\n",
       "      <td>1.252500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4         5   \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "<UNK>        0.288928  1.607660 -0.056371 -0.469463  0.469999 -0.649331   \n",
       "nice        -0.375107 -1.502437 -0.923492 -0.126488  0.362706  0.342689   \n",
       "great       -2.070317  0.062140 -0.049227 -0.107032 -0.310520  1.134613   \n",
       "best        -0.734108 -0.694109  0.215780  0.343054  2.067814 -1.004351   \n",
       "amazing      0.110546  0.140450 -1.180040 -0.194728 -0.007513  0.135330   \n",
       "stop         1.161571 -0.293670 -0.926437 -0.171423  0.147151 -0.434187   \n",
       "lies        -0.550596 -0.910341  0.699479 -2.037829 -0.102543 -0.621193   \n",
       "pitiful     -0.255950  0.279579  0.929856  1.384931  0.032164  0.319030   \n",
       "nerd         1.075786  0.115069 -0.544386 -0.771980  0.644482  1.079942   \n",
       "excellent    0.451461  0.696921 -0.159499 -0.324683 -0.614469  0.711076   \n",
       "work        -1.439708 -0.209003 -2.084174 -0.916557 -1.608510 -0.857398   \n",
       "supreme      0.223013 -0.028687  0.564882  0.675098  0.341677 -0.191852   \n",
       "quality      1.214680  0.817608 -2.055127  0.283380  0.685743 -0.411127   \n",
       "bad          0.302688  0.601107  1.501010 -0.421628  0.284841 -0.091815   \n",
       "highly       0.329463  0.095163 -0.303135 -0.078614 -1.022818  1.102807   \n",
       "respectable  0.549015 -1.398794  0.943506 -1.986844 -0.327049 -0.588285   \n",
       "\n",
       "                   6         7         8         9   ...        90        91  \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "<UNK>        1.339970  0.047787  1.187581 -0.378604  ...  1.163199 -0.957975   \n",
       "nice        -1.466642  1.847192 -0.179850 -0.388620  ...  0.375694  0.324467   \n",
       "great        1.739478 -0.535167 -0.148535  0.059957  ... -0.768177  1.688066   \n",
       "best        -0.709888 -1.500188  0.051568  0.479909  ... -1.976840  1.053008   \n",
       "amazing     -0.224973  0.292712  1.269658 -1.625910  ... -2.258460  0.843542   \n",
       "stop         0.556416 -2.125310  0.109032 -0.475795  ... -0.337052 -0.262313   \n",
       "lies        -0.988834  2.027308  1.513137  0.382081  ... -0.364772  0.934649   \n",
       "pitiful      0.105520  0.527916  0.662397  1.087102  ... -0.722323  0.617160   \n",
       "nerd        -0.767168 -0.080798  0.439931 -1.540084  ...  1.107514  0.151260   \n",
       "excellent   -1.084300  0.999780  0.168365  1.839730  ... -0.298784  0.777314   \n",
       "work        -0.249476  0.976070 -0.382443  0.075832  ... -0.625777  0.330538   \n",
       "supreme      0.086266 -0.040405 -0.129255  2.013664  ...  0.658365  1.371161   \n",
       "quality     -0.803183 -0.009372 -0.772525 -0.658445  ...  0.460884  0.103257   \n",
       "bad          1.130324 -0.284404  0.647604 -0.149717  ... -1.821937  2.214347   \n",
       "highly       1.365102 -1.408793  0.362040 -1.527460  ...  0.782015  0.052320   \n",
       "respectable -1.076976 -1.540396 -0.152022 -0.198329  ... -0.291001  0.351746   \n",
       "\n",
       "                   92        93        94        95        96        97  \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "<UNK>       -0.136281  0.781986  1.577181 -0.096898  1.777398 -0.315396   \n",
       "nice         1.260270  0.276454  0.188943  1.352863 -0.998862 -0.222001   \n",
       "great        0.556124 -2.068017 -1.331687 -0.080085 -0.362097 -1.092865   \n",
       "best         0.526363 -0.673199  0.777457 -0.028395 -0.826402 -0.258068   \n",
       "amazing     -0.818338  0.126159  0.631749  0.263715 -0.829794 -1.229905   \n",
       "stop         0.527860 -1.155204 -0.020593  1.073358  0.721517 -1.528801   \n",
       "lies        -1.553773 -0.259093  1.016199  0.063753  2.199706 -1.115450   \n",
       "pitiful      1.415717 -0.759201  0.341842  1.224626  1.466413 -0.349043   \n",
       "nerd        -1.319436  1.312837  1.290813  1.624247  0.686019  0.273804   \n",
       "excellent   -0.469424  0.013878  1.030626 -1.577635  0.325976 -1.058802   \n",
       "work        -0.872296  0.020790  1.488113  0.213108 -1.583962  0.238860   \n",
       "supreme     -2.001544 -0.631843 -1.892988 -1.321406 -0.109347  0.755093   \n",
       "quality      0.015141  0.598679 -0.562882 -1.093343 -0.529995  1.151136   \n",
       "bad          1.129811 -0.740424  0.407341 -0.390186  0.169043 -0.678800   \n",
       "highly       1.736616  1.610907  0.569710  0.003800 -0.832081  0.060972   \n",
       "respectable  0.714800  1.081521 -1.739781  0.195157  0.656927 -0.060777   \n",
       "\n",
       "                   98        99  \n",
       "<PAD>        0.000000  0.000000  \n",
       "<UNK>       -0.166276 -0.431706  \n",
       "nice        -0.426115 -1.941638  \n",
       "great        1.689988  0.510178  \n",
       "best         0.024702  0.888323  \n",
       "amazing     -1.592322  0.011544  \n",
       "stop         2.115555  0.229224  \n",
       "lies        -1.433277  0.827748  \n",
       "pitiful     -0.000191 -1.460817  \n",
       "nerd         0.775039  0.060528  \n",
       "excellent   -1.411685 -0.298344  \n",
       "work        -0.419670 -1.185783  \n",
       "supreme     -1.208966  0.588318  \n",
       "quality     -0.687031 -1.288582  \n",
       "bad         -0.010013  1.372044  \n",
       "highly      -0.839275  1.701618  \n",
       "respectable -0.046385  1.252500  \n",
       "\n",
       "[17 rows x 100 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 임베딩 가중치 확인 : 학습 전/후 Embedding 테이블과 단어별 벡터 조회\n",
    "import pandas as pd\n",
    "\n",
    "# 학습 전 임베딩 벡터\n",
    "wv = model.embedding.weight.data  # Embedding 층의 가중치 행렬(단어ID x 임베딩 차원) 추출\n",
    "print(wv.shape)  # (vocab_size, embedding_dim)\n",
    "\n",
    "# 특정 단어 벡터\n",
    "vocab = word_to_index.keys()    # 단어사전에서 단어만 뽑아온다.\n",
    "pd.DataFrame(wv, index=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3fde5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch 학습 준비 : 텐서 변환 -> DataLoader 구성 -> 손실함수/옵티마이저 설정\n",
    "X = torch.tensor(padded_sequences, dtype=torch.long)      # 입력 시퀀스(정수 ID)를 LongTensor로 변환\n",
    "y = torch.tensor(labels, dtype=torch.float).unsqueeze(1)  # 라벨을 float으로 변환 후 (N,) -> (N, 1)로 차원 맞춤\n",
    "\n",
    "dataset = TensorDataset(X, y)  # (X, y) 쌍을 Dataset 객체로 묶음\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)  # 배치 단위로 섞어서 공급하는 로더\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # 출력 logit과 정답(0/1)로 이진분류 손실 계산 (시그모이드 포함)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)  # 모델 파라미터는 Adam으로 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090fffa7",
   "metadata": {},
   "source": [
    "BCEWithLogitsLoss을 사용할 때에는 모델 출력이 Sigmoid를 거치지 않은 logit이어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1374a80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss 0.7058625668287277\n",
      "Epoch 2: Loss 0.5749433189630508\n",
      "Epoch 3: Loss 0.5192097276449203\n",
      "Epoch 4: Loss 0.4127691611647606\n",
      "Epoch 5: Loss 0.34768517687916756\n",
      "Epoch 6: Loss 0.25201917067170143\n",
      "Epoch 7: Loss 0.18153125792741776\n",
      "Epoch 8: Loss 0.1340364422649145\n",
      "Epoch 9: Loss 0.10082214698195457\n",
      "Epoch 10: Loss 0.07123453728854656\n",
      "Epoch 11: Loss 0.05424548406153917\n",
      "Epoch 12: Loss 0.042618256993591785\n",
      "Epoch 13: Loss 0.035117856692522764\n",
      "Epoch 14: Loss 0.029273396357893944\n",
      "Epoch 15: Loss 0.024098847527056932\n",
      "Epoch 16: Loss 0.022073124069720507\n",
      "Epoch 17: Loss 0.01889193383976817\n",
      "Epoch 18: Loss 0.017635498195886612\n",
      "Epoch 19: Loss 0.01556430128403008\n",
      "Epoch 20: Loss 0.014929302269592881\n"
     ]
    }
   ],
   "source": [
    "# 학습 루프 : 미니배치 단위로 20 epoch 학습하며 평균 손실 출력\n",
    "for epoch in range(20):\n",
    "    epoch_loss = 0    # 손실 누적\n",
    "\n",
    "    for x_batch, y_batch in dataloader:    # 미니배치 단위로 (X, y) 가져오기\n",
    "        optimizer.zero_grad()              # 이전 배치 기울기 초기화\n",
    "        output = model(x_batch)            # 순전파로 logit 계산\n",
    "        loss = criterion(output, y_batch)  # 예측 logit과 정답으로 손실 계산\n",
    "        loss.backward()                    # 역전파로 기울기 계산\n",
    "        optimizer.step()                   # 파라미터 업데이트\n",
    "\n",
    "        epoch_loss += loss.item()  # 배치손실을 float으로 누적\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}: Loss {epoch_loss / len(dataloader)}\")  # epoch별 평균 손실 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68f35739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 1, 1, 0, 1]\n",
      "[1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# 평가 / 예측 : 학습된 모델로 확률 -> 0/1 예측값 생성 후 정답과 비교\n",
    "model.eval()                        # 평가 모드\n",
    "with torch.no_grad():               # 기울기 계산 비활성화\n",
    "    output = model(X)               # 전체 샘플에 대한 예측 logit 계산\n",
    "    prob = torch.sigmoid(output)    # logit에 0~1 확률로 변환\n",
    "    pred = (prob >= 0.5).int()      # 임계값 0.5 기준으로 이진 분류(0/1) 예측값 생성\n",
    "\n",
    "print(labels)\n",
    "print(pred.squeeze().detach().numpy())  # 예측라벨을 1차원 numpy 배열로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f200e5e1",
   "metadata": {},
   "source": [
    "## 사전학습된 임베딩을 사용하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d93011d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000000, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_wv = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "model_wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4996d30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "(17, 300)\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 매트릭스 초기화 : 사전학습 벡터로 Embedding 레이어를 채우기 위한 준비\n",
    "print(len(word_to_index))  # 어휘 크기 (vocab_size) 확인\n",
    "\n",
    "# (vocab_size, embedding_dim) 크기의 0 행렬 생성\n",
    "embedding_matrix = np.zeros((len(word_to_index), model_wv.vectors.shape[1]))\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c8ab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전학습 임베딩 매핑 : 내 단어사전을 GoogleNews 벡터로 채워 embedding_matrix 구성\n",
    "# model_wv.key_to_index['bad']  # 'bad'의 내부 인덱스 확인 (706)\n",
    "# model_wv.vectors[240]         # 특정 인덱스 벡터 직접 조회\n",
    "\n",
    "# 단어가 사전학습 모델에 있으면 임베딩 벡터(np.ndarray)를 반환, 없으면 None 반환\n",
    "def get_word_embedding(word):\n",
    "    if word in model_wv:          # 사전학습 단어가 존재하면\n",
    "        return model_wv[word]     # 해당 단어 임베딩 벡터 반환\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# get_word_embedding('bad')\n",
    "\n",
    "for word, index in word_to_index.items():  # 내 단어사전(단어-> 인덱스)를 순회\n",
    "    if index >= 2:                         # 특수토큰 제외\n",
    "        emb = get_word_embedding(word)     # 사전학습 임베딩에서 해당 단어 벡터 조회\n",
    "        if emb is not None:                # 벡터가 존재하면\n",
    "            embedding_matrix[index] = emb  # 내 인덱스 위치에 사전학습 벡터를 복사해서 채운다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "348f8768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;PAD&gt;</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;UNK&gt;</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nice</th>\n",
       "      <td>0.158203</td>\n",
       "      <td>0.105957</td>\n",
       "      <td>-0.189453</td>\n",
       "      <td>0.386719</td>\n",
       "      <td>0.083496</td>\n",
       "      <td>-0.267578</td>\n",
       "      <td>0.083496</td>\n",
       "      <td>0.113281</td>\n",
       "      <td>-0.104004</td>\n",
       "      <td>0.178711</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085449</td>\n",
       "      <td>0.189453</td>\n",
       "      <td>-0.146484</td>\n",
       "      <td>0.134766</td>\n",
       "      <td>-0.040771</td>\n",
       "      <td>0.032715</td>\n",
       "      <td>0.089355</td>\n",
       "      <td>-0.267578</td>\n",
       "      <td>0.008362</td>\n",
       "      <td>-0.213867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>0.071777</td>\n",
       "      <td>0.208008</td>\n",
       "      <td>-0.028442</td>\n",
       "      <td>0.178711</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>-0.099609</td>\n",
       "      <td>0.096191</td>\n",
       "      <td>-0.116699</td>\n",
       "      <td>-0.008545</td>\n",
       "      <td>0.148438</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011475</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>-0.289062</td>\n",
       "      <td>-0.048096</td>\n",
       "      <td>-0.199219</td>\n",
       "      <td>-0.071289</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>-0.167969</td>\n",
       "      <td>-0.020874</td>\n",
       "      <td>-0.142578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>-0.126953</td>\n",
       "      <td>0.021973</td>\n",
       "      <td>0.287109</td>\n",
       "      <td>0.153320</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>0.032715</td>\n",
       "      <td>-0.115723</td>\n",
       "      <td>-0.029541</td>\n",
       "      <td>0.153320</td>\n",
       "      <td>0.011292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006439</td>\n",
       "      <td>-0.033936</td>\n",
       "      <td>-0.166016</td>\n",
       "      <td>-0.016846</td>\n",
       "      <td>-0.048584</td>\n",
       "      <td>-0.022827</td>\n",
       "      <td>-0.152344</td>\n",
       "      <td>-0.101562</td>\n",
       "      <td>-0.090332</td>\n",
       "      <td>0.088379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazing</th>\n",
       "      <td>0.073730</td>\n",
       "      <td>0.004059</td>\n",
       "      <td>-0.135742</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>0.180664</td>\n",
       "      <td>-0.046631</td>\n",
       "      <td>0.224609</td>\n",
       "      <td>-0.229492</td>\n",
       "      <td>-0.040039</td>\n",
       "      <td>0.225586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018433</td>\n",
       "      <td>-0.021240</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.020142</td>\n",
       "      <td>-0.310547</td>\n",
       "      <td>-0.207031</td>\n",
       "      <td>-0.006317</td>\n",
       "      <td>-0.141602</td>\n",
       "      <td>-0.150391</td>\n",
       "      <td>-0.137695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop</th>\n",
       "      <td>-0.057861</td>\n",
       "      <td>0.013184</td>\n",
       "      <td>0.115234</td>\n",
       "      <td>0.069824</td>\n",
       "      <td>-0.306641</td>\n",
       "      <td>-0.044678</td>\n",
       "      <td>0.048584</td>\n",
       "      <td>0.152344</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>-0.100098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100098</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>-0.113281</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>-0.115723</td>\n",
       "      <td>0.048096</td>\n",
       "      <td>-0.004822</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>0.029907</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lies</th>\n",
       "      <td>0.149414</td>\n",
       "      <td>-0.012817</td>\n",
       "      <td>0.328125</td>\n",
       "      <td>0.025513</td>\n",
       "      <td>0.017334</td>\n",
       "      <td>0.190430</td>\n",
       "      <td>0.188477</td>\n",
       "      <td>-0.143555</td>\n",
       "      <td>-0.090820</td>\n",
       "      <td>0.206055</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.308594</td>\n",
       "      <td>0.183594</td>\n",
       "      <td>-0.202148</td>\n",
       "      <td>0.031494</td>\n",
       "      <td>-0.164062</td>\n",
       "      <td>-0.201172</td>\n",
       "      <td>0.080078</td>\n",
       "      <td>-0.105469</td>\n",
       "      <td>0.149414</td>\n",
       "      <td>0.157227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pitiful</th>\n",
       "      <td>0.269531</td>\n",
       "      <td>0.253906</td>\n",
       "      <td>-0.020996</td>\n",
       "      <td>0.060303</td>\n",
       "      <td>-0.010925</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>0.139648</td>\n",
       "      <td>-0.057617</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.253906</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063477</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>-0.094238</td>\n",
       "      <td>0.089355</td>\n",
       "      <td>-0.065430</td>\n",
       "      <td>-0.016235</td>\n",
       "      <td>-0.107910</td>\n",
       "      <td>-0.072266</td>\n",
       "      <td>-0.094238</td>\n",
       "      <td>0.028809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nerd</th>\n",
       "      <td>0.265625</td>\n",
       "      <td>-0.207031</td>\n",
       "      <td>-0.026611</td>\n",
       "      <td>0.419922</td>\n",
       "      <td>-0.208984</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.063965</td>\n",
       "      <td>0.149414</td>\n",
       "      <td>-0.017700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215820</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>-0.227539</td>\n",
       "      <td>-0.310547</td>\n",
       "      <td>-0.112793</td>\n",
       "      <td>-0.096680</td>\n",
       "      <td>0.255859</td>\n",
       "      <td>0.124023</td>\n",
       "      <td>-0.030273</td>\n",
       "      <td>0.082031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excellent</th>\n",
       "      <td>-0.212891</td>\n",
       "      <td>-0.004303</td>\n",
       "      <td>-0.180664</td>\n",
       "      <td>-0.007568</td>\n",
       "      <td>0.112793</td>\n",
       "      <td>0.163086</td>\n",
       "      <td>-0.014709</td>\n",
       "      <td>-0.078613</td>\n",
       "      <td>-0.164062</td>\n",
       "      <td>0.279297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>-0.173828</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>-0.081055</td>\n",
       "      <td>0.013550</td>\n",
       "      <td>-0.008362</td>\n",
       "      <td>-0.129883</td>\n",
       "      <td>-0.215820</td>\n",
       "      <td>0.012268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>-0.075684</td>\n",
       "      <td>0.033691</td>\n",
       "      <td>-0.064941</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>0.050537</td>\n",
       "      <td>0.149414</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>-0.133789</td>\n",
       "      <td>-0.020874</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.101562</td>\n",
       "      <td>-0.091309</td>\n",
       "      <td>0.052246</td>\n",
       "      <td>-0.164062</td>\n",
       "      <td>0.121582</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.012024</td>\n",
       "      <td>0.135742</td>\n",
       "      <td>-0.091309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>supreme</th>\n",
       "      <td>0.173828</td>\n",
       "      <td>0.172852</td>\n",
       "      <td>0.112793</td>\n",
       "      <td>0.166016</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>-0.014221</td>\n",
       "      <td>0.128906</td>\n",
       "      <td>-0.217773</td>\n",
       "      <td>0.073730</td>\n",
       "      <td>0.205078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>-0.221680</td>\n",
       "      <td>-0.055664</td>\n",
       "      <td>0.034424</td>\n",
       "      <td>-0.119629</td>\n",
       "      <td>-0.081543</td>\n",
       "      <td>0.121094</td>\n",
       "      <td>-0.164062</td>\n",
       "      <td>-0.127930</td>\n",
       "      <td>0.097656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <td>-0.259766</td>\n",
       "      <td>0.271484</td>\n",
       "      <td>0.119629</td>\n",
       "      <td>0.007233</td>\n",
       "      <td>0.057373</td>\n",
       "      <td>0.113770</td>\n",
       "      <td>0.166992</td>\n",
       "      <td>0.025024</td>\n",
       "      <td>0.067871</td>\n",
       "      <td>0.120117</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145508</td>\n",
       "      <td>0.120117</td>\n",
       "      <td>-0.314453</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>-0.010254</td>\n",
       "      <td>0.298828</td>\n",
       "      <td>0.046387</td>\n",
       "      <td>-0.179688</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>-0.014099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>0.062988</td>\n",
       "      <td>0.124512</td>\n",
       "      <td>0.113281</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>0.038818</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.096191</td>\n",
       "      <td>0.220703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011353</td>\n",
       "      <td>0.341797</td>\n",
       "      <td>-0.090332</td>\n",
       "      <td>0.076660</td>\n",
       "      <td>-0.032471</td>\n",
       "      <td>0.133789</td>\n",
       "      <td>-0.154297</td>\n",
       "      <td>-0.063477</td>\n",
       "      <td>0.114746</td>\n",
       "      <td>0.031006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>highly</th>\n",
       "      <td>0.050781</td>\n",
       "      <td>-0.227539</td>\n",
       "      <td>-0.130859</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>-0.165039</td>\n",
       "      <td>0.118652</td>\n",
       "      <td>-0.230469</td>\n",
       "      <td>-0.225586</td>\n",
       "      <td>0.245117</td>\n",
       "      <td>-0.086914</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013733</td>\n",
       "      <td>-0.013489</td>\n",
       "      <td>0.175781</td>\n",
       "      <td>0.226562</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>-0.210938</td>\n",
       "      <td>-0.111816</td>\n",
       "      <td>-0.056641</td>\n",
       "      <td>0.102539</td>\n",
       "      <td>0.074707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>respectable</th>\n",
       "      <td>0.150391</td>\n",
       "      <td>0.285156</td>\n",
       "      <td>-0.023193</td>\n",
       "      <td>0.095703</td>\n",
       "      <td>-0.022095</td>\n",
       "      <td>0.058350</td>\n",
       "      <td>-0.155273</td>\n",
       "      <td>-0.051025</td>\n",
       "      <td>0.100098</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058838</td>\n",
       "      <td>0.056885</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.138672</td>\n",
       "      <td>-0.227539</td>\n",
       "      <td>0.183594</td>\n",
       "      <td>-0.033447</td>\n",
       "      <td>-0.200195</td>\n",
       "      <td>-0.112305</td>\n",
       "      <td>0.103027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0         1         2         3         4         5    \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "<UNK>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "nice         0.158203  0.105957 -0.189453  0.386719  0.083496 -0.267578   \n",
       "great        0.071777  0.208008 -0.028442  0.178711  0.132812 -0.099609   \n",
       "best        -0.126953  0.021973  0.287109  0.153320  0.127930  0.032715   \n",
       "amazing      0.073730  0.004059 -0.135742  0.022095  0.180664 -0.046631   \n",
       "stop        -0.057861  0.013184  0.115234  0.069824 -0.306641 -0.044678   \n",
       "lies         0.149414 -0.012817  0.328125  0.025513  0.017334  0.190430   \n",
       "pitiful      0.269531  0.253906 -0.020996  0.060303 -0.010925  0.217773   \n",
       "nerd         0.265625 -0.207031 -0.026611  0.419922 -0.208984  0.390625   \n",
       "excellent   -0.212891 -0.004303 -0.180664 -0.007568  0.112793  0.163086   \n",
       "work        -0.075684  0.033691 -0.064941  0.131836  0.050537  0.149414   \n",
       "supreme      0.173828  0.172852  0.112793  0.166016  0.058594 -0.014221   \n",
       "quality     -0.259766  0.271484  0.119629  0.007233  0.057373  0.113770   \n",
       "bad          0.062988  0.124512  0.113281  0.073242  0.038818  0.079102   \n",
       "highly       0.050781 -0.227539 -0.130859  0.062500 -0.165039  0.118652   \n",
       "respectable  0.150391  0.285156 -0.023193  0.095703 -0.022095  0.058350   \n",
       "\n",
       "                  6         7         8         9    ...       290       291  \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "<UNK>        0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "nice         0.083496  0.113281 -0.104004  0.178711  ... -0.085449  0.189453   \n",
       "great        0.096191 -0.116699 -0.008545  0.148438  ... -0.011475  0.064453   \n",
       "best        -0.115723 -0.029541  0.153320  0.011292  ...  0.006439 -0.033936   \n",
       "amazing      0.224609 -0.229492 -0.040039  0.225586  ...  0.018433 -0.021240   \n",
       "stop         0.048584  0.152344  0.073242 -0.100098  ...  0.100098  0.171875   \n",
       "lies         0.188477 -0.143555 -0.090820  0.206055  ... -0.308594  0.183594   \n",
       "pitiful      0.139648 -0.057617  0.312500  0.253906  ... -0.063477  0.132812   \n",
       "nerd         0.164062  0.063965  0.149414 -0.017700  ...  0.215820  0.125000   \n",
       "excellent   -0.014709 -0.078613 -0.164062  0.279297  ...  0.136719  0.000282   \n",
       "work         0.109375 -0.133789 -0.020874  0.054688  ... -0.187500  0.101562   \n",
       "supreme      0.128906 -0.217773  0.073730  0.205078  ...  0.140625 -0.221680   \n",
       "quality      0.166992  0.025024  0.067871  0.120117  ... -0.145508  0.120117   \n",
       "bad          0.050781  0.171875  0.096191  0.220703  ...  0.011353  0.341797   \n",
       "highly      -0.230469 -0.225586  0.245117 -0.086914  ... -0.013733 -0.013489   \n",
       "respectable -0.155273 -0.051025  0.100098  0.001183  ...  0.058838  0.056885   \n",
       "\n",
       "                  292       293       294       295       296       297  \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "<UNK>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "nice        -0.146484  0.134766 -0.040771  0.032715  0.089355 -0.267578   \n",
       "great       -0.289062 -0.048096 -0.199219 -0.071289  0.064453 -0.167969   \n",
       "best        -0.166016 -0.016846 -0.048584 -0.022827 -0.152344 -0.101562   \n",
       "amazing     -0.250000 -0.020142 -0.310547 -0.207031 -0.006317 -0.141602   \n",
       "stop        -0.113281  0.064453 -0.115723  0.048096 -0.004822  0.086426   \n",
       "lies        -0.202148  0.031494 -0.164062 -0.201172  0.080078 -0.105469   \n",
       "pitiful     -0.094238  0.089355 -0.065430 -0.016235 -0.107910 -0.072266   \n",
       "nerd        -0.227539 -0.310547 -0.112793 -0.096680  0.255859  0.124023   \n",
       "excellent   -0.173828  0.004242 -0.081055  0.013550 -0.008362 -0.129883   \n",
       "work        -0.091309  0.052246 -0.164062  0.121582  0.062500  0.012024   \n",
       "supreme     -0.055664  0.034424 -0.119629 -0.081543  0.121094 -0.164062   \n",
       "quality     -0.314453  0.022095 -0.010254  0.298828  0.046387 -0.179688   \n",
       "bad         -0.090332  0.076660 -0.032471  0.133789 -0.154297 -0.063477   \n",
       "highly       0.175781  0.226562  0.086914 -0.210938 -0.111816 -0.056641   \n",
       "respectable -0.187500  0.138672 -0.227539  0.183594 -0.033447 -0.200195   \n",
       "\n",
       "                  298       299  \n",
       "<PAD>        0.000000  0.000000  \n",
       "<UNK>        0.000000  0.000000  \n",
       "nice         0.008362 -0.213867  \n",
       "great       -0.020874 -0.142578  \n",
       "best        -0.090332  0.088379  \n",
       "amazing     -0.150391 -0.137695  \n",
       "stop         0.029907  0.007812  \n",
       "lies         0.149414  0.157227  \n",
       "pitiful     -0.094238  0.028809  \n",
       "nerd        -0.030273  0.082031  \n",
       "excellent   -0.215820  0.012268  \n",
       "work         0.135742 -0.091309  \n",
       "supreme     -0.127930  0.097656  \n",
       "quality      0.000706 -0.014099  \n",
       "bad          0.114746  0.031006  \n",
       "highly       0.102539  0.074707  \n",
       "respectable -0.112305  0.103027  \n",
       "\n",
       "[17 rows x 300 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(embedding_matrix, index=word_to_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "662bcec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNet(\n",
      "  (embedding): Embedding(17, 300, padding_idx=0)\n",
      "  (rnn): RNN(300, 16, batch_first=True)\n",
      "  (out): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Pytorch 텍스트 분류 모델 : Embedding + RNN + Linear로 이진 분류(logit) 출력\n",
    "import torch\n",
    "import torch.nn as nn           # 신경망 레이어\n",
    "import torch.optim as optim     # 옵티마이저(활성화함수)\n",
    "from torch.utils.data import DataLoader, TensorDataset  # 배치 로더 / 데이터셋 유틸\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    # 정수 시퀀스를 임베딩 -> RNN -> 선형층으로 처리해 이진 분류 logit(1개)를 출력\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super().__init__()                    # nn.Module 초기화\n",
    "        self.embedding = nn.Embedding(        # 단어 ID를 밀집 벡터로 변환하는 임베딩 층\n",
    "            num_embeddings = vocab_size,      # 단어 사전 크기 (어휘 수)\n",
    "            embedding_dim = embedding_dim,    # 임베딩 차원\n",
    "            padding_idx = 0                   # PAD(0) 인덱스는 0 그대로 사용\n",
    "        )\n",
    "\n",
    "        # 사전학습된 임베딩 벡터로 초기화 : Embedding 가중치를 사전학습 행렬로 덮어쓰기\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float))\n",
    "\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)    # 입력(배치, 길이, 차원) 형태의 RNN\n",
    "        self.out = nn.Linear(hidden_size, 1)     # 마지막 은닉 상태를 1차원 logit으로 변환\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)     # (batch, seq_len) -> (batch, seq_len, embedding_dim)\n",
    "        out, h_n = self.rnn(embedded)    # h_n: (num_layers*directions, batch, hidden_size)\n",
    "        out = self.out(h_n.squeeze(0))   # (batch_size, hidden_size) -> (batch, 1)\n",
    "        return out  # 출력 : 시그모이드 전 logit(확률이 아님)\n",
    "    \n",
    "embedding_dim = model_wv.vectors.shape[1]  # 사전학습 임베딩 차원 (300)으로 임베딩 차원 설정\n",
    "model = SimpleNet(vocab_size, embedding_dim, hidden_size=16)  # 어휘 크기 / 임베딩 차원/ 은닉크기로 모델 생성\n",
    "print(model)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # 출력 logit과 정답(0/1)로 이진분류 손실 계산 (시그모이드 포함)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)  # 모델 파라미터는 Adam으로 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e981c48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss 0.7078307420015335\n",
      "Epoch 2: Loss 0.5741119906306267\n",
      "Epoch 3: Loss 0.45387016981840134\n",
      "Epoch 4: Loss 0.3885459378361702\n",
      "Epoch 5: Loss 0.289472796022892\n",
      "Epoch 6: Loss 0.2057625986635685\n",
      "Epoch 7: Loss 0.1449330635368824\n",
      "Epoch 8: Loss 0.09873061440885067\n",
      "Epoch 9: Loss 0.07171069271862507\n",
      "Epoch 10: Loss 0.05469982326030731\n",
      "Epoch 11: Loss 0.04191338876262307\n",
      "Epoch 12: Loss 0.032103403471410275\n",
      "Epoch 13: Loss 0.02679432090371847\n",
      "Epoch 14: Loss 0.021011684788390994\n",
      "Epoch 15: Loss 0.018954440485686064\n",
      "Epoch 16: Loss 0.016619653441011906\n",
      "Epoch 17: Loss 0.014636388281360269\n",
      "Epoch 18: Loss 0.013677880400791764\n",
      "Epoch 19: Loss 0.012031156802549958\n",
      "Epoch 20: Loss 0.011078468756750226\n"
     ]
    }
   ],
   "source": [
    "# 학습 루프 : 미니배치 단위로 20 epoch 학습하며 평균 손실 출력\n",
    "for epoch in range(20):\n",
    "    epoch_loss = 0    # 손실 누적\n",
    "\n",
    "    for x_batch, y_batch in dataloader:    # 미니배치 단위로 (X, y) 가져오기\n",
    "        optimizer.zero_grad()              # 이전 배치 기울기 초기화\n",
    "        output = model(x_batch)            # 순전파로 logit 계산\n",
    "        loss = criterion(output, y_batch)  # 예측 logit과 정답으로 손실 계산\n",
    "        loss.backward()                    # 역전파로 기울기 계산\n",
    "        optimizer.step()                   # 파라미터 업데이트\n",
    "\n",
    "        epoch_loss += loss.item()  # 배치손실을 float으로 누적\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}: Loss {epoch_loss / len(dataloader)}\")  # epoch별 평균 손실 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce55d502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 1, 1, 0, 1]\n",
      "[1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# 평가 / 예측 : 학습된 모델로 확률 -> 0/1 예측값 생성 후 정답과 비교\n",
    "model.eval()                        # 평가 모드\n",
    "with torch.no_grad():               # 기울기 계산 비활성화\n",
    "    output = model(X)               # 전체 샘플에 대한 예측 logit 계산\n",
    "    prob = torch.sigmoid(output)    # logit에 0~1 확률로 변환\n",
    "    pred = (prob >= 0.5).int()      # 임계값 0.5 기준으로 이진 분류(0/1) 예측값 생성\n",
    "\n",
    "print(labels)\n",
    "print(pred.squeeze().detach().numpy())  # 예측라벨을 1차원 numpy 배열로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850bbf48",
   "metadata": {},
   "source": [
    "사전학습 임베딩을 사용했을 때에도 학습 데이터 분류가 잘 되는지 파악한다.  \n",
    "만약 틀린 샘플이 있다면 해당 문장이 OOV(0벡터) 비중이 큰지 확인해봐야 한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
