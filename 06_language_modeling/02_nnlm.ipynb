{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a443c0",
   "metadata": {},
   "source": [
    "# NNLM (Neural Network Language Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a118ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d7985809",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNLM(nn.Module):                                      # 신경망 언어 모델 클래스 정의\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, context_size):\n",
    "        super(NNLM, self).__init__()                         # nn.Module 초기화\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)   # 단어 ID → 임베딩 벡터 변환\n",
    "        self.fc1 = nn.Linear(context_size * embed_size, hidden_size)\n",
    "                                                             # context 단어 임베딩을 펼쳐서 hidden layer로\n",
    "        self.relu = nn.ReLU()                                # 비선형 활성화 함수\n",
    "        self.fc2 = nn.Linear(hidden_size, vocab_size)        # hidden → 전체 단어 분포\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)              # 로그 확률 형태로 정규화\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embed(x)                               # 입력 단어 ID → 임베딩 (batch, context, embed)\n",
    "        embeds = embeds.view(embeds.size(0), -1)             # 임베딩을 1차원으로 펼침\n",
    "        output = self.fc1(embeds)                            # 첫 번째 선형 변환\n",
    "        output = self.relu(output)                           # ReLU 적용\n",
    "        output = self.fc2(output)                            # 단어별 점수(logit) 계산\n",
    "        log_probs = self.log_softmax(output)                 # 다음 단어의 로그 확률 분포\n",
    "        return log_probs                                     # 최종 출력 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "da3f5568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNLM(\n",
      "  (embed): Embedding(5000, 300)\n",
      "  (fc1): Linear(in_features=600, out_features=128, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=128, out_features=5000, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 5000                        # 전체 단어 사전 크기 (단어 ID 개수)\n",
    "EMBED_SIZE = 300                        # 각 단어를 표현할 임베딩 벡터 차원\n",
    "HIDDEN_SIZE = 128                       # 은닉층(hidden layer) 뉴런 수\n",
    "CONTEXT_SIZE = 2                        # 입력으로 사용할 이전 단어 개수\n",
    "\n",
    "model = NNLM(                           # NNLM 모델 생성\n",
    "    VOCAB_SIZE,                         # 단어 사전 크기 전달\n",
    "    EMBED_SIZE,                         # 임베딩 차원 전달\n",
    "    HIDDEN_SIZE,                        # 은닉층 크기 전달\n",
    "    CONTEXT_SIZE                        # 문맥 단어 수 전달\n",
    ")\n",
    "\n",
    "print(model)                            # 모델 구조(레이어 구성) 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "68cdd0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy data 생성\n",
    "X = torch.randint(0, VOCAB_SIZE, (8, CONTEXT_SIZE))   # 입력 데이터: 배치 8개, 각 샘플마다 context 단어 ID 2개\n",
    "y = torch.randint(0, VOCAB_SIZE, (8,))                # 정답 데이터: 각 입력에 대한 다음 단어 ID (총 8개_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "937d7ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.476271629333496\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.NLLLoss()                     # 로그 확률 출력에 사용하는 손실 함수 (Negative Log Likelihood)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),                     # 모델의 모든 파라미터를 최적화 대상으로 설정\n",
    "    lr=0.001                                # 학습률\n",
    ")\n",
    "\n",
    "model.train()                               # 모델을 학습 모드로 전환\n",
    "optimizer.zero_grad()                       # 이전 step의 gradient 초기화\n",
    "output = model(X)                           # 입력 X로 예측된 로그 확률 계산\n",
    "loss = criterion(output, y)                 # 예측과 정답(y)으로 손실 계산\n",
    "loss.backward()                             # 역전파로 gradient 계산\n",
    "optimizer.step()                            # 파라미터 업데이트\n",
    "\n",
    "print(loss.item())                          # 현재 step의 loss 값 출력\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
