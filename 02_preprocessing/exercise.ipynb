{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58ce4587",
   "metadata": {},
   "source": [
    "어휘 크기(vocab), OOV → 학습 안정성\n",
    "\n",
    "max_df, min_df ⇒ 과하게 정보를 지우면 필요한 정보도 같이 \n",
    "\n",
    "사라질 수 도 있다. \n",
    "\n",
    "ㅋㅋㅋ/하하하/히히히/ㅋㅋ ⇒ ㅋㅋ or 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dec5f88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리된 텍스트: hello 자연어 처리 is fun do you agree\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 원시 텍스트\n",
    "raw_text = \"Hello!!! 자연어 처리 is FUN.   Do you agree? :)\"\n",
    "\n",
    "# 불필요한 기호 제거 : \\w(문자/숫자/_), \\s(공백)만 남기 나머지 특수문자는 제거\n",
    "clean_text = re.sub(r\"[^\\w\\s]\", \"\", raw_text)\n",
    "# 소문자 변환: 영어 대문자를 소문자로 통일\n",
    "clean_text = clean_text.lower()\n",
    "\n",
    "# 공백 제거 : 연속 공백/앞뒤 공백 제거 후 단어 사이클 공백 1칸으로 정리\n",
    "clean_text = \" \".join(clean_text.split())\n",
    "\n",
    "print(\"처리된 텍스트:\", clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3f1c5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['자연어 처리는 재미있다!', '하지만 배우기는 어렵다.', 'Python으로 가능합니다.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"자연어 처리는 재미있다! 하지만 배우기는 어렵다. Python으로 가능합니다.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d76288a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 결과: ['자연어', '처리는', '데이터', '과학의', '한', '분야다', '여러', '전처리가', '필요하다']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# 원시 텍스트\n",
    "raw_text = \"자연어 처리는 데이터 과학의 한 분야다! 여러 전처리가 필요하다.\"\n",
    "\n",
    "# 특수문자 제거\n",
    "clean_text = raw_text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "# 숫자 제거\n",
    "clean_text = re.sub(r\"\\\\d+\", \"\", clean_text)\n",
    "\n",
    "# 불용어 제거\n",
    "tokens = word_tokenize(clean_text)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "print(\"전처리 결과:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52fd2392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형태소 분석 결과: ['자연어', '처리', '는', '정말', '재미있다', '!']\n",
      "품사 태깅 결과: [('자연어', 'Noun'), ('처리', 'Noun'), ('는', 'Josa'), ('정말', 'Noun'), ('재미있다', 'Adjective'), ('!', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "# 텍스트\n",
    "text = \"자연어 처리는 정말 재미있다!\"\n",
    "\n",
    "# 형태소 분석\n",
    "okt = Okt()\n",
    "morphs = okt.morphs(text)\n",
    "pos = okt.pos(text)\n",
    "\n",
    "print(\"형태소 분석 결과:\", morphs)\n",
    "print(\"품사 태깅 결과:\", pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e6c497",
   "metadata": {},
   "source": [
    "| 항목 | 어간 추출(Stemming) | 표제어 추출(Lemmatization) |\n",
    "|---|---|---|\n",
    "| 목적 | 단어를 규칙으로 “대충” 잘라 어간(어근 비슷한 형태) 만들기 | 사전 기반으로 “기본형(표제어, lemma)”으로 변환 |\n",
    "| 처리 방식 | 접미사 제거 등 규칙 중심(품사/의미 거의 미고려) | 사전 + 품사(POS) 고려해서 변환 |\n",
    "| 결과 자연스러움 | 낮을 수 있음(부자연스러운 형태 가능) | 높음(사전에 있는 자연스러운 기본형) |\n",
    "| 속도 | 보통 빠름 | 상대적으로 느림 |\n",
    "| 예시: playing | play (ing 제거) | play (동사 기본형) |\n",
    "| 참고 예시: studies | studi (부자연스러울 수 있음) | study (자연스러운 기본형) |\n",
    "| 참고 예시: better | 변화 없거나 부정확할 수 있음 | (형용사로 보면) good 가능 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "271d4aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words:\\n [[0 0 0 1 1 1]\n",
      " [0 1 0 1 0 1]\n",
      " [1 0 1 0 0 0]]\n",
      "TF-IDF:\\n [[0.         0.         0.         0.51785612 0.68091856 0.51785612]\n",
      " [0.         0.68091856 0.         0.51785612 0.         0.51785612]\n",
      " [0.70710678 0.         0.70710678 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# 샘플 데이터\n",
    "corpus = [\"자연어 처리는 재미있다.\", \"자연어 처리는 어렵다.\", \"데이터는 유용하다.\"]\n",
    "\n",
    "# Bag-of-Words\n",
    "vectorizer = CountVectorizer()\n",
    "bow = vectorizer.fit_transform(corpus)\n",
    "print(\"Bag-of-Words:\\\\n\", bow.toarray())\n",
    "\n",
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(corpus)\n",
    "print(\"TF-IDF:\\\\n\", tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45d71338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 빈도: Counter({'재미있다.': 2, '자연어': 1, '처리는': 1, '자연어는': 1, '어렵지만': 1})\n",
      "문장 길이: [3, 3]\n"
     ]
    }
   ],
   "source": [
    "# 샘플 텍스트\n",
    "text = \"자연어 처리는 재미있다. 자연어는 어렵지만 재미있다.\"\n",
    "\n",
    "# 단어 빈도 계산\n",
    "from collections import Counter\n",
    "words = text.split()\n",
    "word_count = Counter(words)\n",
    "\n",
    "# 문장 길이 계산\n",
    "sentences = text.split(\". \")\n",
    "sentence_lengths = [len(sentence.split()) for sentence in sentences]\n",
    "\n",
    "print(\"단어 빈도:\", word_count)\n",
    "print(\"문장 길이:\", sentence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1508c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 규칙기반\n",
    "text = \"고객님, 오늘의 할인 코드는 SAVE20입니다.\"\n",
    "\n",
    "if \"할인\" in text:\n",
    "    print(\"\")\n",
    "else:\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def41976",
   "metadata": {},
   "source": [
    "### 기본 Feature(보조 피처)\n",
    "\n",
    "- **문장 길이**: 각 문장의 단어 수 - 스팸확인, 리뷰, 민원 등 일부 TASK\n",
    "\n",
    "---\n",
    "\n",
    "### 머신 러닝 기초 다시보기\n",
    "\n",
    "**피처(Feature)**: 입력 데이터의 특성 - BoW/ TF-IDF/ n-gram 로 벡터화 \n",
    "\n",
    "1. **로지스틱 회귀(Logistic Regression)**: 분류 작업에 사용 - TF-IDF, 스팸 / 감성 / 문서 분류\n",
    "2. **SVM(Support Vector Machine)**: 선형 및 비선형 데이터 분류 \n",
    "3. **랜덤 포레스트(Random Forest)**: 앙상블 기법 - 희소 행렬이랑 같이 쓰면 비효율적. 통계 피처(길이/ 비율) 등과 함께 사용해야 효율적\n",
    "\n",
    "---\n",
    "\n",
    "### NLP 애플리케이션 개발 단계\n",
    "\n",
    "1. 데이터 수집 및 전처리 \n",
    "2. 피처 엔지니어링 - ML ( TF-IDF / n-gram) /// 딥러닝( 토크 나이저 + 임베딩 )\n",
    "3. 머신 러닝 알고리즘 선택 - 로지스틱 회귀 / Linear SVM / 랜덤 포레스트 / 부스팅\n",
    "4. 모델 훈련 및 검증 \n",
    "5. 결과 해석 및 배포"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
