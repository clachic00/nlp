{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58ce4587",
   "metadata": {},
   "source": [
    "어휘 크기(vocab), OOV → 학습 안정성\n",
    "\n",
    "max_df, min_df ⇒ 과하게 정보를 지우면 필요한 정보도 같이 \n",
    "\n",
    "사라질 수 도 있다. \n",
    "\n",
    "ㅋㅋㅋ/하하하/히히히/ㅋㅋ ⇒ ㅋㅋ or 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dec5f88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리된 텍스트: hello 자연어 처리 is fun do you agree\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 원시 텍스트\n",
    "raw_text = \"Hello!!! 자연어 처리 is FUN.   Do you agree? :)\"\n",
    "\n",
    "# 불필요한 기호 제거 : \\w(문자/숫자/_), \\s(공백)만 남기 나머지 특수문자는 제거\n",
    "clean_text = re.sub(r\"[^\\w\\s]\", \"\", raw_text)\n",
    "# 소문자 변환: 영어 대문자를 소문자로 통일\n",
    "clean_text = clean_text.lower()\n",
    "\n",
    "# 공백 제거 : 연속 공백/앞뒤 공백 제거 후 단어 사이클 공백 1칸으로 정리\n",
    "clean_text = \" \".join(clean_text.split())\n",
    "\n",
    "print(\"처리된 텍스트:\", clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3f1c5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['자연어 처리는 재미있다!', '하지만 배우기는 어렵다.', 'Python으로 가능합니다.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"자연어 처리는 재미있다! 하지만 배우기는 어렵다. Python으로 가능합니다.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d76288a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 결과: ['자연어', '처리는', '데이터', '과학의', '한', '분야다', '여러', '전처리가', '필요하다']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# 원시 텍스트\n",
    "raw_text = \"자연어 처리는 데이터 과학의 한 분야다! 여러 전처리가 필요하다.\"\n",
    "\n",
    "# 특수문자 제거\n",
    "clean_text = raw_text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "# 숫자 제거\n",
    "clean_text = re.sub(r\"\\\\d+\", \"\", clean_text)\n",
    "\n",
    "# 불용어 제거\n",
    "tokens = word_tokenize(clean_text)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "print(\"전처리 결과:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52fd2392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형태소 분석 결과: ['자연어', '처리', '는', '정말', '재미있다', '!']\n",
      "품사 태깅 결과: [('자연어', 'Noun'), ('처리', 'Noun'), ('는', 'Josa'), ('정말', 'Noun'), ('재미있다', 'Adjective'), ('!', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "# 텍스트\n",
    "text = \"자연어 처리는 정말 재미있다!\"\n",
    "\n",
    "# 형태소 분석\n",
    "okt = Okt()\n",
    "morphs = okt.morphs(text)\n",
    "pos = okt.pos(text)\n",
    "\n",
    "print(\"형태소 분석 결과:\", morphs)\n",
    "print(\"품사 태깅 결과:\", pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e6c497",
   "metadata": {},
   "source": [
    "| 항목 | 어간 추출(Stemming) | 표제어 추출(Lemmatization) |\n",
    "|---|---|---|\n",
    "| 목적 | 단어를 규칙으로 “대충” 잘라 어간(어근 비슷한 형태) 만들기 | 사전 기반으로 “기본형(표제어, lemma)”으로 변환 |\n",
    "| 처리 방식 | 접미사 제거 등 규칙 중심(품사/의미 거의 미고려) | 사전 + 품사(POS) 고려해서 변환 |\n",
    "| 결과 자연스러움 | 낮을 수 있음(부자연스러운 형태 가능) | 높음(사전에 있는 자연스러운 기본형) |\n",
    "| 속도 | 보통 빠름 | 상대적으로 느림 |\n",
    "| 예시: playing | play (ing 제거) | play (동사 기본형) |\n",
    "| 참고 예시: studies | studi (부자연스러울 수 있음) | study (자연스러운 기본형) |\n",
    "| 참고 예시: better | 변화 없거나 부정확할 수 있음 | (형용사로 보면) good 가능 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "271d4aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words:\\n [[0 0 0 1 1 1]\n",
      " [0 1 0 1 0 1]\n",
      " [1 0 1 0 0 0]]\n",
      "TF-IDF:\\n [[0.         0.         0.         0.51785612 0.68091856 0.51785612]\n",
      " [0.         0.68091856 0.         0.51785612 0.         0.51785612]\n",
      " [0.70710678 0.         0.70710678 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# 샘플 데이터\n",
    "corpus = [\"자연어 처리는 재미있다.\", \"자연어 처리는 어렵다.\", \"데이터는 유용하다.\"]\n",
    "\n",
    "# Bag-of-Words\n",
    "vectorizer = CountVectorizer()\n",
    "bow = vectorizer.fit_transform(corpus)\n",
    "print(\"Bag-of-Words:\\\\n\", bow.toarray())\n",
    "\n",
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(corpus)\n",
    "print(\"TF-IDF:\\\\n\", tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45d71338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 빈도: Counter({'재미있다.': 2, '자연어': 1, '처리는': 1, '자연어는': 1, '어렵지만': 1})\n",
      "문장 길이: [3, 3]\n"
     ]
    }
   ],
   "source": [
    "# 샘플 텍스트\n",
    "text = \"자연어 처리는 재미있다. 자연어는 어렵지만 재미있다.\"\n",
    "\n",
    "# 단어 빈도 계산\n",
    "from collections import Counter\n",
    "words = text.split()\n",
    "word_count = Counter(words)\n",
    "\n",
    "# 문장 길이 계산\n",
    "sentences = text.split(\". \")\n",
    "sentence_lengths = [len(sentence.split()) for sentence in sentences]\n",
    "\n",
    "print(\"단어 빈도:\", word_count)\n",
    "print(\"문장 길이:\", sentence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1508c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 규칙기반\n",
    "text = \"고객님, 오늘의 할인 코드는 SAVE20입니다.\"\n",
    "\n",
    "if \"할인\" in text:\n",
    "    print(\"\")\n",
    "else:\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def41976",
   "metadata": {},
   "source": [
    "### 기본 Feature(보조 피처)\n",
    "\n",
    "- **문장 길이**: 각 문장의 단어 수 - 스팸확인, 리뷰, 민원 등 일부 TASK\n",
    "\n",
    "---\n",
    "\n",
    "### 머신 러닝 기초 다시보기\n",
    "\n",
    "**피처(Feature)**: 입력 데이터의 특성 - BoW/ TF-IDF/ n-gram 로 벡터화 \n",
    "\n",
    "1. **로지스틱 회귀(Logistic Regression)**: 분류 작업에 사용 - TF-IDF, 스팸 / 감성 / 문서 분류\n",
    "2. **SVM(Support Vector Machine)**: 선형 및 비선형 데이터 분류 \n",
    "3. **랜덤 포레스트(Random Forest)**: 앙상블 기법 - 희소 행렬이랑 같이 쓰면 비효율적. 통계 피처(길이/ 비율) 등과 함께 사용해야 효율적\n",
    "\n",
    "---\n",
    "\n",
    "### NLP 애플리케이션 개발 단계\n",
    "\n",
    "1. 데이터 수집 및 전처리 \n",
    "2. 피처 엔지니어링 - ML ( TF-IDF / n-gram) /// 딥러닝( 토크 나이저 + 임베딩 )\n",
    "3. 머신 러닝 알고리즘 선택 - 로지스틱 회귀 / Linear SVM / 랜덤 포레스트 / 부스팅\n",
    "4. 모델 훈련 및 검증 \n",
    "5. 결과 해석 및 배포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c326ee2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "코사인 유사도: 0.9925833339709303\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# 임의의 단어 벡터\n",
    "word1 = np.array([[1, 2, 3]])\n",
    "word2 = np.array([[2, 3, 4]])\n",
    "\n",
    "# 코사인 유사도 계산\n",
    "similarity = cosine_similarity(word1, word2)\n",
    "print(\"코사인 유사도:\", similarity[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfde001",
   "metadata": {},
   "source": [
    "## 자연어 임베딩(Embedding)이란?\n",
    "\n",
    "자연어 임베딩은 **텍스트(단어/문장/문서)를 고정 길이의 실수 벡터(vector)로 바꾸는 표현 방식**이다.  \n",
    "즉, 컴퓨터가 다룰 수 있도록 **문자를 수치화**하되, 단순 ID가 아니라 **의미·문맥·문법적 특징**이 벡터 공간에 반영되도록 만든다.\n",
    "\n",
    "- 임베딩 덕분에 텍스트끼리 **유사도(코사인 유사도 등)**를 계산할 수 있고  \n",
    "- 머신러닝/딥러닝 모델에 **입력 피처**로 넣을 수 있으며  \n",
    "- “king - man + woman ≈ queen” 같은 **의미 연산**이 벡터 공간에서 가능해진다.\n",
    "\n",
    "---\n",
    "\n",
    "## 임베딩 핵심 정리 표\n",
    "\n",
    "| 구분 | 내용 | 예시/포인트 |\n",
    "|---|---|---|\n",
    "| 정의 | 텍스트를 **고정 길이 실수 벡터**로 변환 | 단어/문장/문서 → `R^d` 벡터 |\n",
    "| 목적 | 컴퓨터가 텍스트를 수치로 처리 + 의미를 보존 | 단순 라벨 인코딩과 다름 |\n",
    "| 표현 대상 | 단어 임베딩 / 문장 임베딩 / 문서 임베딩 | word2vec(단어), SBERT(문장) |\n",
    "| 벡터 의미 | 벡터 공간에서 **거리/각도**가 의미적 유사성을 반영 | 가까울수록 의미가 비슷한 경향 |\n",
    "| 대표 유사도 지표 | 코사인 유사도, 유클리드 거리 | 코사인: 방향(의미) 중심 비교 |\n",
    "| 장점 | 의미적 관계 수치화, 모델 입력 가능, 일반화에 유리 | 검색/추천/분류/클러스터링 등 |\n",
    "| 한계 | 데이터·학습 방식에 따라 편향/품질 차이, OOV 문제 | 학습 코퍼스에 없는 단어 처리 |\n",
    "| 활용 | 분류, 검색, 추천, 군집화, QA, RAG 등 | query↔doc 유사도 계산 |\n",
    "| 학습 방식(큰 분류) | 정적(Static) vs 문맥(Contextual) | 정적: word2vec / 문맥: BERT 계열 |\n",
    "| 정적 임베딩 특징 | 단어당 벡터 1개(문맥 변화 반영 어려움) | “bank” (강둑/은행) 구분 약함 |\n",
    "| 문맥 임베딩 특징 | 같은 단어라도 문맥에 따라 벡터가 달라짐 | “bank”가 문장에 따라 의미 분리 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f499b917",
   "metadata": {},
   "source": [
    "## 자연어 임베딩 요약\n",
    "\n",
    "- **자연어 임베딩(Embedding)**: 텍스트(단어/문장/문서)를 **고정 길이의 실수 벡터**로 변환하는 기술  \n",
    "- 목적: 컴퓨터가 텍스트를 **수치로 처리**하면서도 **의미/문법 정보**를 벡터에 담게 함  \n",
    "- 효과:\n",
    "  - 벡터 간 **유사도(코사인 유사도 등)** 계산 가능 → 관련도/의미 유사성 비교\n",
    "  - ML/DL 모델의 **입력 피처**로 사용 가능\n",
    "  - 벡터 공간에서 **의미 관계(유추/연산)** 표현 가능(예: king - man + woman ≈ queen)\n",
    "- 종류(큰 분류):\n",
    "  - **정적 임베딩**: 단어당 벡터 1개(문맥 변화 반영 어려움)  \n",
    "  - **문맥 임베딩**: 같은 단어도 문장 문맥에 따라 벡터가 달라짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be539bec",
   "metadata": {},
   "source": [
    "## 벡터화\n",
    "\n",
    "TF-IDF / BoW → 희소 벡터\n",
    "\n",
    "Embedding / Transformer 출력 → 밀집 벡터 (딥러닝)\n",
    "\n",
    "⇒ 목적은 비슷한 텍스트는 비슷한 벡터로 나타낸다 ⇒ 분류 / 검색 / 추천 시스템"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2e420a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW 벡터화 결과:\\n [[0 0 1 1 1]\n",
      " [1 1 0 0 0]]\n",
      "단어 사전: {'자연어': 2, '처리는': 4, '재미있다': 3, 'python으로': 0, '가능하다': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 문장 데이터\n",
    "corpus = [\"자연어 처리는 재미있다.\", \"Python으로 가능하다.\"]\n",
    "\n",
    "# BoW 벡터화\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)            # 단어 사전 학습 + BoW (빈도) 희소행렬 벡터로 변환\n",
    "\n",
    "print(\"BoW 벡터화 결과:\\\\n\", X.toarray())          # 희소행렬을 밀집 배열 형태로 출력 \n",
    "print(\"단어 사전:\", vectorizer.vocabulary_)       # {단어 : 인덱스} 형태의 단어사전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f8cfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF 결과:\\n [[0.         0.         0.57735027 0.57735027 0.57735027]\n",
      " [0.70710678 0.70710678 0.         0.         0.        ]]\n",
      "단어 사전: {'자연어': 2, '처리는': 4, '재미있다': 3, 'python으로': 0, '가능하다': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 문장 데이터\n",
    "corpus = [\"자연어 처리는 재미있다.\", \"Python으로 가능하다.\"]\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)                        # IDF 학습(fit) + TF-IDF 벡터 변환 -> 희소행렬\n",
    "print(\"TF-IDF 결과:\\\\n\", X.toarray())                       # 희소행렬을 밀집 배열 형태로 변환\n",
    "print(\"단어 사전:\", vectorizer.vocabulary_)                 # {단어 : 인덱스} 형태의 단어사전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbd12ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 '자연어'의 벡터: [-0.00713902  0.00124103 -0.00717672 -0.00224462  0.0037193   0.00583312\n",
      "  0.00119818  0.00210273 -0.00411039  0.00722533 -0.00630704  0.00464722\n",
      " -0.00821997  0.00203647 -0.00497705 -0.00424769 -0.00310898  0.00565521\n",
      "  0.0057984  -0.00497465  0.00077333 -0.00849578  0.00780981  0.00925729\n",
      " -0.00274233  0.00080022  0.00074665  0.00547788 -0.00860608  0.00058446\n",
      "  0.00686942  0.00223159  0.00112468 -0.00932216  0.00848237 -0.00626413\n",
      " -0.00299237  0.00349379 -0.00077263  0.00141129  0.00178199 -0.0068289\n",
      " -0.00972481  0.00904058  0.00619805 -0.00691293  0.00340348  0.00020606\n",
      "  0.00475375 -0.00711994  0.00402695  0.00434743  0.00995737 -0.00447374\n",
      " -0.00138926 -0.00731732 -0.00969783 -0.00908026 -0.00102275 -0.00650329\n",
      "  0.00484973 -0.00616403  0.00251919  0.00073944 -0.00339215 -0.00097922\n",
      "  0.00997913  0.00914589 -0.00446183  0.00908303 -0.00564176  0.00593092\n",
      " -0.00309722  0.00343175  0.00301723  0.00690046 -0.00237388  0.00877504\n",
      "  0.00758943 -0.00954765 -0.00800821 -0.0076379   0.00292326 -0.00279472\n",
      " -0.00692952 -0.00812826  0.00830918  0.00199049 -0.00932802 -0.00479272\n",
      "  0.00313674 -0.00471321  0.00528084 -0.00423344  0.0026418  -0.00804569\n",
      "  0.00620989  0.00481889  0.00078719  0.00301345]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec # 단어 임베딩 학습 모델 \n",
    "\n",
    "# 샘플 문장\n",
    "sentences = [[\"자연어\", \"처리\", \"재미있다\"], [\"Python\", \"가능하다\"]]\n",
    "\n",
    "# Word2Vec 모델 학습\n",
    "model = Word2Vec(\n",
    "    sentences,          # 학습에 사용할 토큰화 문장들\n",
    "    vector_size=100,    # 단어 벡터 차원 수\n",
    "    window=3,           # 주변 문맥으로 볼 단어 범위 (좌우 3개)\n",
    "    min_count=1,        # 최소 등장 횟수 1개 \n",
    "    sg=0                # 0 = CBOW (주변 -> 중심 단어 예측), 1 = Skip-gram(중심 -> 주변 단어 예측)\n",
    ")\n",
    "\n",
    "# 단어 벡터 확인\n",
    "print(\"단어 '자연어'의 벡터:\", model.wv[\"자연어\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d363f70c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.0045030187)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(\"자연어\",\"재미있다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe1186cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('자연어', 0.17018885910511017),\n",
       " ('처리', -0.013514922931790352),\n",
       " ('Python', -0.023671654984354973),\n",
       " ('가능하다', -0.05234673619270325)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"재미있다\",topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a69a4d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  __________________________________\n",
      "| 방어원정대 조기 조퇴멤버는 김다빈님 입니다. 얼른 집에가세요. |\n",
      "  ==================================\n",
      "                                  \\\n",
      "                                   \\                  &************************&\n",
      "                                    \\             &******************************&\n",
      "                                     \\          &**********************************&\n",
      "                                              &**************************************&\n",
      "                                            &*****************************************&\n",
      "                                           &*******************************************&\n",
      "                                          &*********************************************&\n",
      "                                         &***********************************************&\n",
      "                                        &************************************************&\n",
      "                                        &***#########********#########*******************&\n",
      "                                        &*##       ##########          ##################&\n",
      "                                        &*##   O   ##@**####   O       ##***************&\n",
      "                                        &***#########@*******#########*****************&\n",
      "                                        &***********@*********************************&\n",
      "                                        &**********@*********************************&\n",
      "                                        &*********@*********************************&\n",
      "                                        &********@@*********************************&\n",
      "                                         &*******@@@@@@****************************&\n",
      "                                          &**************************************&\n",
      "                                            &**************************************&\n",
      "                                             &******@@@@@@@@@@@@*********************&\n",
      "                                               &*************************************&\n",
      "                                                 &************************************&\n",
      "                                                       &*******************************&\n",
      "                                                         &*****************************&\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import cowsay\n",
    "def go_home_one_person(member):\n",
    "    goodbye_member = random.choice(member)\n",
    "    cowsay.miki(f\"방어원정대 조기 조퇴멤버는 {goodbye_member}님 입니다. 얼른 집에가세요.\")\n",
    "\n",
    "defence_expedition = \"\"\"강승원 정석원 신승훈 김다빈 김도영 양창일 송주엽 유헌상\"\"\".split()\n",
    "\n",
    "go_home_one_person(defence_expedition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fa77dd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
      "king 벡터 차원: (50,)\n",
      "similarity(king, queen): 0.7839043\n",
      "similarity(king, banana): 0.2207408\n",
      "most_similar('king'): [('prince', 0.8236179351806641), ('queen', 0.7839043140411377), ('ii', 0.7746230363845825), ('emperor', 0.7736247777938843), ('son', 0.766719400882721)]\n",
      "[('queen', 0.8523604273796082), ('throne', 0.7664334177970886), ('prince', 0.7592144012451172), ('daughter', 0.7473883628845215), ('elizabeth', 0.7460219860076904)]\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# 1) 미리 학습된 GloVe 로드 (처음 1번은 다운로드 시간이 걸림)\n",
    "# 50차원 버전: 가볍고 실습용으로 좋음\n",
    "glove = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "# 2) 단어 벡터 확인 (길이=50)\n",
    "print(\"king 벡터 차원:\", glove[\"king\"].shape)\n",
    "\n",
    "# 3) 코사인 유사도\n",
    "print(\"similarity(king, queen):\", glove.similarity(\"king\", \"queen\"))\n",
    "print(\"similarity(king, banana):\", glove.similarity(\"king\", \"banana\"))\n",
    "\n",
    "# 4) 가장 비슷한 단어 top-n\n",
    "print(\"most_similar('king'):\", glove.most_similar(\"king\", topn=5))\n",
    "\n",
    "# 5) 벡터 연산(관계) 예시: king - man + woman ≈ queen\n",
    "print(glove.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e679d70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOC_0 벡터 일부: [-0.01058207 -0.01189603 -0.01985634  0.01694183  0.00701691  0.00072557\n",
      " -0.01981407 -0.01017613 -0.01967002  0.00412226]\n",
      "\n",
      "DOC_0와 유사한 문서:\n",
      "[('DOC_1', 0.2878270447254181), ('DOC_2', 0.1307842880487442), ('DOC_3', -0.07168306410312653)]\n",
      "\n",
      "새 문장과 유사한 문서:\n",
      "[('DOC_2', 0.19597965478897095), ('DOC_0', -0.018320461735129356), ('DOC_3', -0.06657172739505768)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument           # 문서 임베딩(Doc2Vec) 모델과 태그 문서 클래스\n",
    "\n",
    "# 1) 예시 문서(문장) 준비\n",
    "docs = [\n",
    "    \"자연어 처리는 재미있다\",\n",
    "    \"파이썬으로 자연어 처리를 할 수 있다\",\n",
    "    \"축구 경기는 정말 재미있다\",\n",
    "    \"오늘 날씨가 좋다\"\n",
    "]\n",
    "\n",
    "# 2) Doc2Vec은 문서마다 태그(아이디)가 필요함\n",
    "tagged_docs = [\n",
    "    TaggedDocument(words=d.split(), tags=[f\"DOC_{i}\"])\n",
    "    for i, d in enumerate(docs)\n",
    "]   # 각 문서를 (토큰리스트, 문서 ID)로 변환\n",
    "\n",
    "# 3) Doc2Vec 모델 학습\n",
    "model = Doc2Vec(\n",
    "    documents=tagged_docs,  # 태그가 붙은 문서들로 학습\n",
    "    vector_size=50,   # 문서 벡터 차원\n",
    "    window=3,         # 문맥 범위\n",
    "    min_count=1,      # 최소 등장 횟수\n",
    "    workers=2,        # CPU 쓰레드\n",
    "    epochs=50         # 학습 반복\n",
    ")\n",
    "\n",
    "# 4) 학습된 문서 벡터 확인\n",
    "print(\"DOC_0 벡터 일부:\", model.dv[\"DOC_0\"][:10])\n",
    "\n",
    "# 5) 특정 문서와 가장 유사한 문서 찾기\n",
    "print(\"\\nDOC_0와 유사한 문서:\")\n",
    "print(model.dv.most_similar(\"DOC_0\", topn=3))\n",
    "\n",
    "# 6) 새 문장(학습에 없던 문장)도 벡터로 추정 가능(infer_vector)\n",
    "new_doc = \"자연어 처리는 파이썬으로 가능하다\"\n",
    "new_vec = model.infer_vector(new_doc.split())\n",
    "\n",
    "print(\"\\n새 문장과 유사한 문서:\")\n",
    "print(model.dv.most_similar([new_vec], topn=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e6839a",
   "metadata": {},
   "source": [
    "## **LDA**\n",
    "\n",
    "문서 ⇒ 단어 묶음. 그 안에 주제가 섞여있다. ⇒ (주제 찾기)토픽 모델링\n",
    "\n",
    "문서 - 주제 확률분포(Topic 비율 벡터) ⇒ 기사 (스포츠 0.5, 경제 0.3, IT 0.2)\n",
    "\n",
    "주제 요약에도 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "001248ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "주제별 단어 분포:\\n [[1.4913572  1.4913572  0.50945773 0.50945773 0.50945773]\n",
      " [0.5086428  0.5086428  1.49054227 1.49054227 1.49054227]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation     # LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer     # 문서의 단어 등장 횟수 (BoW)로 변환하는 벡터라이저\n",
    "\n",
    "# 문장 데이터\n",
    "corpus = [\"자연어 처리는 재미있다.\", \"Python으로 가능하다.\"]\n",
    "\n",
    "# BoW 벡터화\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)    # 단어 사전 학습 + 문서-단어 희소행렬 변환\n",
    "\n",
    "# LDA 모델 학습\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=2,     # 찾을 주제 갯수\n",
    "    random_state=0      # 재현성 (시드 고정)\n",
    ")\n",
    "lda.fit(X)      # BoW 행렬로 LDA 모델 학습(주제 - 단어 분포/문서-주제 분포 추정)\n",
    "\n",
    "# 주제별 단어 분포\n",
    "print(\"주제별 단어 분포:\\\\n\", lda.components_)  # 각 주제에서 단어가 얼마나 중요한지(가중치)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a008bfde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['python으로', '가능하다', '자연어', '재미있다', '처리는'], dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f9a96058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  [('python으로', np.float64(1.4913571964571597)), ('가능하다', np.float64(1.491357196456652)), ('자연어', np.float64(0.5094577321201083)), ('처리는', np.float64(0.5094577321189349)), ('재미있다', np.float64(0.5094577321184829))]\n",
      "Topic 1:  [('재미있다', np.float64(1.4905422678815157)), ('처리는', np.float64(1.4905422678810638)), ('자연어', np.float64(1.4905422678798905)), ('가능하다', np.float64(0.5086428035433468)), ('python으로', np.float64(0.5086428035428391))]\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_idx = topic.argsort()[::-1][:5]\n",
    "    print(f\"Topic {topic_idx}: \", [(feature_names[i], topic[i]) for i in top_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2fc64a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "자연어 벡터 일부 : [ 0.00364446 -0.00144183 -0.00430582  0.00206623 -0.00079104 -0.00219508\n",
      " -0.00202816  0.00041452  0.00483354 -0.00515762]\n",
      "자연어와 비슷한 단어 : [('파이썬', 0.04029620811343193), ('처리는', 0.0400528721511364), ('처리', 0.02612016163766384)]\n",
      "similarity(자연어, 처리) : 0.02612016\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText # 서브워드 (n-그램) 기반 단어 임베딩 모델\n",
    "\n",
    "# 토큰화된 문장 준비\n",
    "sentences =[\n",
    "    [\"자연어\", \"처리는\", \"재미있다\"],\n",
    "    [\"파이썬\", \"으로\", \"자연어\", \"처리\",\"가능하다\"],\n",
    "    [\"자연어\", \"처리\", \"공부\",\"하자\"]\n",
    "]\n",
    "# FastText : 단어를 '문자 n-그램(서브워드)' 조합으로도 표현하여 OOV(미등장 단어) 벡터 생성에 강함\n",
    "model = FastText(\n",
    "    sentences,          # 학습 데이터\n",
    "    vector_size = 50,   # 단어 벡터 차원 수\n",
    "    window = 3,         # 주변 문맥 범위 (좌우 3개씩)\n",
    "    min_count = 1 ,     # 최소 등장 횟수(1번 이상일때 사용)\n",
    "    sg = 1,             # 0 = CBOW (주변 -> 중심 단어 예측), 1= Skip-gram(중심 -> 주변 단어 예측)\n",
    "    min_n = 2,          # 서브워드 (문자 n-gram) 최소 길이\n",
    "    max_n = 5,          # 서브워드 (문자 n-gram) 최대 길이\n",
    "    epochs = 50         # 학습 반복 횟수\n",
    ")\n",
    "\n",
    "# 단어 벡터 확인 ('자연어') 임베딩 벡터 중 앞 10개 차원\n",
    "print('자연어 벡터 일부 :',model.wv['자연어'][:10])\n",
    "\n",
    "# 유사 단어 : 코사인 유사도가 높은 단어 TOP3\n",
    "print('자연어와 비슷한 단어 :', model.wv.most_similar(\"자연어\",topn = 3 ))\n",
    "\n",
    "# 단어 간 코사인 유사도 확인 \n",
    "print(\"similarity(자연어, 처리) :\", model.wv.similarity(\"자연어\", \"처리\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
