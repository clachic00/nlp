{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c354673",
   "metadata": {},
   "source": [
    "# 원핫인코딩 (One-hot Encoding)\n",
    "\n",
    "- 의미: 단어(또는 토큰)를 어휘 크기(vocab size)만큼의 길이를 가진 벡터로 바꾸는 방법\n",
    "- 표현 방식: 해당 단어의 인덱스 위치만 1, 나머지는 전부 0 (즉, “이 단어가 맞다/아니다”만 표시)\n",
    "- 예시: vocab size가 5이고 단어 인덱스가 3이면 → [0, 0, 1, 0, 0]\n",
    "- 특징/주의: 단어 간 의미/유사도 정보는 없고, vocab이 커질수록 벡터가 매우 커져 메모리/연산 비용이 증가함\n",
    "    - 그래서 실무에선 보통 임베딩(Embedding)으로 저차원 밀집(dense) 벡터로 바꿔 사용한다\n",
    "- 사용 시기: 단어 수가 아주 작거나(카테고리/키워드 몇십~몇백) 간단한 모델/해석이 중요한 경우엔 원-핫/BoW를 사용할 때도 있다.\n",
    "\n",
    "- 임베딩\n",
    "    - 학습 가능한 변환(레이어/행렬): 단어 인덱스 k를 길이 d(예: 64, 128)인 밀집(dense) 벡터로 매핑\n",
    "    - 예: k=3 → [0.12, -0.03, ...] (d차원)\n",
    "    - 특징: 저차원, 밀집, 학습을 통해 의미/유사도가 반영될 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2c76bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\"The Little Prince, written by Antoine de Saint-Exupéry, is a poetic tale about a young prince who travels from his home planet to Earth. The story begins with a pilot stranded in the Sahara Desert after his plane crashes. While trying to fix his plane, he meets a mysterious young boy, the Little Prince.\n",
    "\n",
    "The Little Prince comes from a small asteroid called B-612, where he lives alone with a rose that he loves deeply. He recounts his journey to the pilot, describing his visits to several other planets. Each planet is inhabited by a different character, such as a king, a vain man, a drunkard, a businessman, a geographer, and a fox. Through these encounters, the Prince learns valuable lessons about love, responsibility, and the nature of adult behavior.\n",
    "\n",
    "On Earth, the Little Prince meets various creatures, including a fox, who teaches him about relationships and the importance of taming, which means building ties with others. The fox's famous line, \"You become responsible, forever, for what you have tamed,\" resonates with the Prince's feelings for his rose.\n",
    "\n",
    "Ultimately, the Little Prince realizes that the essence of life is often invisible and can only be seen with the heart. After sharing his wisdom with the pilot, he prepares to return to his asteroid and his beloved rose. The story concludes with the pilot reflecting on the lessons learned from the Little Prince and the enduring impact of their friendship.\n",
    "\n",
    "The narrative is a beautifully simple yet profound exploration of love, loss, and the importance of seeing beyond the surface of things.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd7425e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['little', 'prince', 'written', 'antoine', 'saint-exupéry', 'poetic', 'tale', 'young', 'prince', 'travels', 'home', 'planet', 'earth'], ['story', 'begins', 'pilot', 'stranded', 'sahara', 'desert', 'plane', 'crashes'], ['trying', 'fix', 'plane', 'meets', 'mysterious', 'young', 'boy', 'little', 'prince'], ['little', 'prince', 'comes', 'small', 'asteroid', 'called', 'b-612', 'lives', 'alone', 'rose', 'loves', 'deeply'], ['recounts', 'journey', 'pilot', 'describing', 'visits', 'several', 'planets'], ['planet', 'inhabited', 'different', 'character', 'king', 'vain', 'man', 'drunkard', 'businessman', 'geographer', 'fox'], ['encounters', 'prince', 'learns', 'valuable', 'lessons', 'love', 'responsibility', 'nature', 'adult', 'behavior'], ['earth', 'little', 'prince', 'meets', 'various', 'creatures', 'including', 'fox', 'teaches', 'relationships', 'importance', 'taming', 'means', 'building', 'ties', 'others'], ['fox', 'famous', 'line', 'become', 'responsible', 'forever', 'tamed', 'resonates', 'prince', 'feelings', 'rose'], ['ultimately', 'little', 'prince', 'realizes', 'essence', 'life', 'often', 'invisible', 'seen', 'heart'], ['sharing', 'wisdom', 'pilot', 'prepares', 'return', 'asteroid', 'beloved', 'rose'], ['story', 'concludes', 'pilot', 'reflecting', 'lessons', 'learned', 'little', 'prince', 'enduring', 'impact', 'friendship'], ['narrative', 'beautifully', 'simple', 'yet', 'profound', 'exploration', 'love', 'loss', 'importance', 'seeing', 'beyond', 'surface', 'things']]\n",
      "{'little': 6, 'prince': 9, 'written': 1, 'antoine': 1, 'saint-exupéry': 1, 'poetic': 1, 'tale': 1, 'young': 2, 'travels': 1, 'home': 1, 'planet': 2, 'earth': 2, 'story': 2, 'begins': 1, 'pilot': 4, 'stranded': 1, 'sahara': 1, 'desert': 1, 'plane': 2, 'crashes': 1, 'trying': 1, 'fix': 1, 'meets': 2, 'mysterious': 1, 'boy': 1, 'comes': 1, 'small': 1, 'asteroid': 2, 'called': 1, 'b-612': 1, 'lives': 1, 'alone': 1, 'rose': 3, 'loves': 1, 'deeply': 1, 'recounts': 1, 'journey': 1, 'describing': 1, 'visits': 1, 'several': 1, 'planets': 1, 'inhabited': 1, 'different': 1, 'character': 1, 'king': 1, 'vain': 1, 'man': 1, 'drunkard': 1, 'businessman': 1, 'geographer': 1, 'fox': 3, 'encounters': 1, 'learns': 1, 'valuable': 1, 'lessons': 2, 'love': 2, 'responsibility': 1, 'nature': 1, 'adult': 1, 'behavior': 1, 'various': 1, 'creatures': 1, 'including': 1, 'teaches': 1, 'relationships': 1, 'importance': 2, 'taming': 1, 'means': 1, 'building': 1, 'ties': 1, 'others': 1, 'famous': 1, 'line': 1, 'become': 1, 'responsible': 1, 'forever': 1, 'tamed': 1, 'resonates': 1, 'feelings': 1, 'ultimately': 1, 'realizes': 1, 'essence': 1, 'life': 1, 'often': 1, 'invisible': 1, 'seen': 1, 'heart': 1, 'sharing': 1, 'wisdom': 1, 'prepares': 1, 'return': 1, 'beloved': 1, 'concludes': 1, 'reflecting': 1, 'learned': 1, 'enduring': 1, 'impact': 1, 'friendship': 1, 'narrative': 1, 'beautifully': 1, 'simple': 1, 'yet': 1, 'profound': 1, 'exploration': 1, 'loss': 1, 'seeing': 1, 'beyond': 1, 'surface': 1, 'things': 1}\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sentences = sent_tokenize(raw_text)     # 원문을 문장 리스트로 분리\n",
    "en_stopwords = stopwords.words('english')\n",
    "\n",
    "vocab = {}\n",
    "preprocessed_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence = sentence.lower()                                             # 소문자 변환\n",
    "    tokens = word_tokenize(sentence)                                        # 문장을 단어(토큰) 리스트로 변환\n",
    "    tokens = [token for token in tokens if token not in en_stopwords]       # 불용어 토큰 제거\n",
    "    tokens = [token for token in tokens if len(token)>2]                    # 길이 2 이하 토큰 제거 (노이즈 감소)\n",
    "    \n",
    "    # 전처리된 토큰들로 빈도 집계\n",
    "    for token in tokens:\n",
    "        # vocab에 없으면 빈도 1로 초기화, vocab에 이미 존재하면 +1\n",
    "        if token in vocab:\n",
    "            vocab[token] += 1 \n",
    "        else :\n",
    "            vocab[token] = 1\n",
    "\n",
    "    preprocessed_sentences.append(tokens)\n",
    "\n",
    "print(preprocessed_sentences)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1193c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  1,  1,  7,  2,  1,  1,  8,  9],\n",
       "       [ 0,  0, 10,  1,  4,  1,  1,  1, 11,  1],\n",
       "       [ 0,  1,  1, 11, 12,  1,  7,  1,  3,  2],\n",
       "       [ 1,  1, 13,  1,  1,  1,  1,  5,  1,  1],\n",
       "       [ 0,  0,  0,  1,  1,  4,  1,  1,  1,  1],\n",
       "       [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  6],\n",
       "       [ 1,  2,  1,  1, 14,  1,  1,  1,  1,  1],\n",
       "       [ 1,  6,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "       [ 1,  1,  1,  1,  1,  1,  1,  2,  1,  5],\n",
       "       [ 1,  3,  2,  1,  1,  1,  1,  1,  1,  1],\n",
       "       [ 0,  0,  1,  1,  4,  1,  1, 13,  1,  5],\n",
       "       [ 1,  4,  1, 14,  1,  3,  2,  1,  1,  1],\n",
       "       [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1]], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keras 토큰 인덱싱 + 정수 시퀀스 변환 + padding/truncation\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=15, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "\n",
    "# 길이 10으로 통일. 길면 앞을 자르고, 잛으면 앞을 0으로 채우기\n",
    "padded_seqs = pad_sequences(sequences, padding='pre' ,maxlen=10, value=0)\n",
    "padded_seqs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6515530d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 10)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_seqs.shape       # (문장 수, 길이)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a3e8f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.]]], shape=(13, 10, 15))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정수 시퀀스를 원-핫 벡터로 변환 (One-hot Encoding)\n",
    "from tensorflow.keras.utils import to_categorical       # 정수 라벨/인덱스를 원-핫 벡터로 바꾸는 함수\n",
    "\n",
    "one_hot_encoded = to_categorical(padded_seqs)           # (문장, 길이) 정수 인덱스를 (문장, 길이, 클래스 수) 원-핫으로 변환\n",
    "one_hot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "997f1d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 10, 15)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f46fbe",
   "metadata": {},
   "source": [
    "shape 이 (문장 개수, 토큰길이, vocab_size)  \n",
    "마지막 차원인 vocab_size는 padded_seqs에 등장한 최대 인덱스 + 1 크기이다. (0=PAD 포함)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec93574",
   "metadata": {},
   "source": [
    "한국어 전처리\n",
    "\n",
    "1. 토큰화(형태소 분석)\n",
    "2. 시퀀스 처리 Tokenizer\n",
    "3. 패딩처리 pad_sequences\n",
    "4. one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c8bfd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    '나는 오늘 학원에 간다.',\n",
    "    '친구들과 맛있는 점심 식사를 해서 기분이 좋다.',\n",
    "    '오늘은 또 어떤 재미난 수업을 할지 너무 기대가 된다.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "85b09230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['오늘', '학원', '간다'],\n",
       " ['친구', '맛있다', '점심', '식사', '하다', '기분', '좋다'],\n",
       " ['오늘', '또', '어떻다', '재미', '난', '수업', '하다', '너무', '기대', '되다']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KoNLPy(Okt)   형태소 분석 기반 한국어 전처리 + 불용어/기호 제거\n",
    "from konlpy.tag import Okt      # KoNLPy의 Okt 형태소 분석기\n",
    "import re                       # 정규표현식\n",
    "\n",
    "okt = Okt()                     # Okt 형태소 분석기 객체 생성\n",
    "\n",
    "# 한국어 불용어 리스트\n",
    "ko_stopwords = ['은', '는', '이', '가', '을', '를', '와', '과', '에', '의', '으로', '나', '내', '우리', '들']\n",
    "\n",
    "preprocessed_texts = []         # 전처리 토큰 결과 저장용\n",
    "\n",
    "for text in texts:              # 원문 텍스트 리스트를 한 문장씩 순회\n",
    "    tokens = okt.morphs(text, stem=True)\n",
    "    tokens = [token for token in tokens if token not in ko_stopwords]               # 불용어 제거\n",
    "    tokens = [token for token in tokens if not re.search(r'[\\s.,:;?!]', token)]     # 공백/구두점 기호가 포함된 토큰 제거\n",
    "    preprocessed_texts.append(tokens)                                               # 전처리된 결과를 저장\n",
    "    \n",
    "preprocessed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8fce1558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 4, 5], [6, 7, 8, 9, 3, 10, 11], [2, 12, 13, 14, 15, 16, 3, 17, 18, 19]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keras Tokenizer : 토큰을 인덱스로 매핑하고 시퀀스를 정수 인덱스로 변환 (OOV 표함)\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(preprocessed_texts)\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_texts)\n",
    "sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "82f0b629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " '오늘': 2,\n",
       " '하다': 3,\n",
       " '학원': 4,\n",
       " '간다': 5,\n",
       " '친구': 6,\n",
       " '맛있다': 7,\n",
       " '점심': 8,\n",
       " '식사': 9,\n",
       " '기분': 10,\n",
       " '좋다': 11,\n",
       " '또': 12,\n",
       " '어떻다': 13,\n",
       " '재미': 14,\n",
       " '난': 15,\n",
       " '수업': 16,\n",
       " '너무': 17,\n",
       " '기대': 18,\n",
       " '되다': 19}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95042fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 6)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keras pad_sequences로 시퀀스 패딩()\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded_seqs = pad_sequences(sequences, maxlen=6)                    # 길이 6으로 통일 (기본값 padding = 'pre')\n",
    "padded_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "905831be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 6, 20)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정수 시퀀스를 원-핫 벡터로 변환 (One-hot Encoding)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "one_hot_encoded = to_categorical(padded_seqs)\n",
    "one_hot_encoded.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3cced3",
   "metadata": {},
   "source": [
    "shape : (문장 개수, 패딩포함 길이, 최대인덱스 + 1)\n",
    "마지막 차원은 0(PAD)가 포함되어 +1 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7a8816aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">232</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m20\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_3 (\u001b[38;5;33mSimpleRNN\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m232\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m9\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">241</span> (964.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m241\u001b[0m (964.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">241</span> (964.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m241\u001b[0m (964.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 원-핫 시퀀스 입력(길이 6, vocab 20)을 SimpleRnn 모델로 이진분류 구성\n",
    "from tensorflow.keras import models, layers                 # keras 모델/레이어 구성 요소\n",
    "\n",
    "input = layers.Input(shape=(6,20))                          # 입력 텐서 정의 : (timestamps=6, features=20) 원-핫 벡터 시퀀스\n",
    "x = layers.SimpleRNN(8)(input)                              # RNN 은닉유닛 8개\n",
    "output = layers.Dense(1, activation = 'sigmoid')(x)         # 이진 분류용 출력 : 확률(0~1) 1개로 변환\n",
    "\n",
    "model = models.Model(inputs = input, outputs = output)      # Functional API 로 입력 -> 출력 연결해 모델 생성\n",
    "model.summary()                                             # 모델 레이어 구성/파라미터 수 요약 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a9f30eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 690ms/step - accuracy: 0.3333 - loss: 0.9055\n",
      "Epoch 2/3\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.3333 - loss: 0.8942\n",
      "Epoch 3/3\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.3333 - loss: 0.8830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x24deb222e70>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 컴파일 + 라벨 준비 + 원-핫 입력으로 학습 수행\n",
    "import numpy as np \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "labels = np.array([1, 0, 1])\n",
    "\n",
    "model.fit(one_hot_encoded, labels, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c87258",
   "metadata": {},
   "source": [
    "- **임베딩을 쓰기 전 단계 데모**로 가장 단순한 방법으로 학습까지 진행해봤다.\n",
    "- 일반적으로 원- 핫 인코딩을\n",
    "    - vocab이 커지면 원-핫 차원이 너무 커져서 **메모리/연산이 비효율적**\n",
    "    - 단어 의미/유사도 정보를 못 담음\n",
    "- 이러한 이유로 잘 사용하지 않기 때문에 정수 인덱스를 그대로 넣고 Embedding을 학습\n",
    "    - 입력: `(batch, timesteps)` 정수 인덱스\n",
    "    - `Embedding(vocab_size, embed_dim)` → `(batch, timesteps, embed_dim)`\n",
    "    - 그 다음 RNN/Transformer 모델로 학습한다.\n",
    "\n",
    "\n",
    "\n",
    "| 표현 방식                     | 입력 단위        | 벡터 형태(Shape)                               | 값이 의미하는 것                      | 순서(문맥) 보존    | 장점                                  | 단점                                      | 주 사용처                                     |\n",
    "| ------------------------- | ------------ | ------------------------------------------ | ------------------------------ | ------------ | ----------------------------------- | --------------------------------------- | ----------------------------------------- |\n",
    "| **원-핫(시퀀스)**              | 토큰 시퀀스(문장)   | `(seq_len, V)` 또는 배치 `(batch, seq_len, V)` | 각 토큰을 길이 `V` 벡터로 표시(해당 인덱스만 1) | ✅(시퀀스 구조 유지) | 구현/이해가 쉬움, RNN 입력으로 바로 가능           | `V`가 커지면 메모리/연산 폭발(매우 희소), 의미/유사도 정보 없음 | 교육용 데모, 아주 작은 vocab 실험                    |\n",
    "| **BoW (CountVectorizer)** | 문서(문장/리뷰) 1개 | `(V)` 또는 `(batch, V)`                      | 단어 등장 **횟수**(count)            | ❌            | 빠르고 간단, 전통 ML에서 강력                  | 단어 순서/문맥 완전 손실, 고차원 희소                  | 스팸 분류/감성분석의 베이스라인, 빠른 EDA                 |\n",
    "| **TF-IDF**                | 문서(문장/리뷰) 1개 | `(V)` 또는 `(batch, V)`                      | 단어 중요도 = 빈도(TF) × 희귀성(IDF)     | ❌            | BoW보다 “중요 단어”가 잘 드러남, 전통 ML 성능 좋음   | 문맥/순서 손실, 여전히 고차원 희소                    | 검색/유사도, 분류 베이스라인(Linear SVM, LR)          |\n",
    "| **Embedding**             | 토큰 시퀀스(문장)   | `(seq_len, d)` 또는 `(batch, seq_len, d)`    | 토큰 ID → **d차원 실수 벡터**(학습/사전학습) | ✅            | 저차원 밀집(dense), 의미/유사도 학습 가능, 딥러닝 표준 | 학습 데이터/자원 필요, 해석이 상대적으로 어려움             | RNN/LSTM/GRU/Transformer 입력, 거의 모든 현대 NLP |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8763fbe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
